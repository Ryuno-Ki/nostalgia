# Add and/or Contribute a New Data Source

The first thing to consider is that you should produce a Pandas Dataframe. It should have one or two datetime columns (1 for an event, 2 for a period with a start and end date).
It is also useful to have a column called `title`.

Following the pattern below will ensure that your data will play nicely with all the apps and query language.

## Example

Let's look at the code for collecting your self-written Reddit posts:

First install dependencies:

    pip install psaw nostalgia

It extends the basic [Nostalgia DataFrame](https://github.com/nostalgia-dev/nostalgia/blob/master/nostalgia/ndf.py#L111).

It has 2 methods:

- ingest
- load

In this case `ingest` takes an author as argument (e.g. pvkooten) and will gather all the posts using the free (unauthenticated) PushShift API.
In it, you can see it makes use of the provided time helper function `datetime_from_timestamp` that will ensure it will be converted your current timezone from UTC. It calls nostalgia's `save_df` function which will compress and cache the data.

Then there is the `load` function, that will allow post-processing the cached data, e.g. to add period itervals that cannot be represented by parquet files.

```python
class RedditPosts(NDF):
    vendor = "reddit"

    @classmethod
    def ingest(cls, author):
        from psaw import PushshiftAPI

        api = PushshiftAPI()

        posts = [
            {
                "title": x.title,
                "time": datetime_from_timestamp(x.created_utc),
                "url": x.full_link,
                "text": x.selftext,
            }
            for x in api.search_submissions(author=author)
        ]
        posts = pd.DataFrame(posts)
        posts["author"] = author
        save_df(posts, "reddit_posts")

    @classmethod
    def load(cls, nrows=None):
        df = load_df("reddit_posts", nrows)
        return cls(df)
```

Here is the result after ingest and loading using the timeline:

![Reddit post on Timeline](https://raw.githubusercontent.com/nostalgia-dev/nostalgia/master/docs/reddit_example.png)

## Details

### Loading data

To do data ingestion, there are a few helper functions:

    read_array_of_dict_from_json
    load_data_file_modified_time
    load_json_file_modified_time
    load_image_texts
    load_dataframe_per_json_file
    load_object_per_newline
    latest_file_is_historic

Basically the helper functions help with common file extensions such as CSV and JSON, and consider only having to process as little as possible.

For example, it will use modification time of a file to determine whether to do any processing.

See their [docs](https://github.com/nostalgia-dev/nostalgia/tree/master/nostalgia/data_loading.py) for more info.

### Time

There are a couple of [time helper functions](https://github.com/nostalgia-dev/nostalgia/tree/master/nostalgia/times.py) that can be used to get dates in the right timezone format:

Most notably for parsing:

```python
def datetime_tz(*args):
    return tz.localize(datetime(*args))

def datetime_from_timestamp(x, tzone=tz, divide_by_1000=True):
    if divide_by_1000:
        x = x // 1000
    if isinstance(tzone, str):
        tzone = timezone(tzone)
    x = datetime.fromtimestamp(x, tz=tzone)
    if tzone != tz:
        x = x.astimezone(tz)
    return x


def datetime_from_format(s, fmt, in_utc=False):
    base = datetime.strptime(s, fmt)
    if in_utc:
        return utc.localize(base).astimezone(tz)
    else:
        return tz.localize(base)

def parse_date_tz(text):
    ...
    returns a_metadate
```

### Caching

It is recommended to use caching (to disk) for expensive calls, for e.g. long calculations, or API calls.

```python
from nostalgia.cache import get_cache

CACHE = get_cache("screenshots")
```

It depends on `diskcache` and is very useful. Data will be stored in `~/nostalgia_data/cache` and can safely be removed, though at the obvious cost of having to recompute.
