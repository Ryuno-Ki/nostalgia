{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Ecosystem for combining personal data Nostalgia will help with gathering data from a variety of sources and enable you to combine them easily. It's much like Home Assistant , providing a platform, but then for connecting data instead of IoT devices. Afterwards, it will help you filter and visualize the data. The architecture is as follows. You're looking at the core which contains the code for ingesting sources, installing the backend system and allows you to write scripts using Nostalgia Query Language. If you want to add your own data that is not supported, please for now contact us directly in either discord or slack and we'll help you get started. You can also look at the open issues to see suggestions for new sources. Available Data Bindings Full list of current sources . Heartrate ( Fitbit , Samsung Watch ) Sleep ( Fitbit , Samsung Watch , SleepCycle) Places (Google Timeline) Bank Payments (ING) Pictures (Google) Emails (Gmail) App Usage (Google/Android) Chat Conversations (WhatsApp, Facebook Messenger) Music listening (Google) Music identification ( Shazam ) Posts ( Reddit , Facebook) File Visits ( Emacs ) Annotated Screenshots (Tesseract) Web ( Nostalgia Chrome Plugin ) Products Events Videos Google Search People Indoor Positioning ( whereami ) Public Transport (MijnOV) Getting started Follow the instructions for a source of interest to ensure it is loaded Use the data in an interactive session (run Python) Nostalgia Query Language - extending pandas payments . expenses . by_card \\ . last_week \\ . office_days \\ . at_night () \\ . heartrate_above ( 80 ) \\ . when_at ( \"amsterdam\" ) . sum () This will give the total expenses by card in the last week, but only on work days, at night, when my heart rate is above 80 and I'm in Amsterdam. It combined the generic class functionality, together with data from: A Payments provider A Location provider A Heartrate provider Contributing Please contribute the data sources that others could use as well!","title":"Home"},{"location":"#ecosystem-for-combining-personal-data","text":"Nostalgia will help with gathering data from a variety of sources and enable you to combine them easily. It's much like Home Assistant , providing a platform, but then for connecting data instead of IoT devices. Afterwards, it will help you filter and visualize the data. The architecture is as follows. You're looking at the core which contains the code for ingesting sources, installing the backend system and allows you to write scripts using Nostalgia Query Language. If you want to add your own data that is not supported, please for now contact us directly in either discord or slack and we'll help you get started. You can also look at the open issues to see suggestions for new sources.","title":"Ecosystem for combining personal data"},{"location":"#available-data-bindings","text":"Full list of current sources . Heartrate ( Fitbit , Samsung Watch ) Sleep ( Fitbit , Samsung Watch , SleepCycle) Places (Google Timeline) Bank Payments (ING) Pictures (Google) Emails (Gmail) App Usage (Google/Android) Chat Conversations (WhatsApp, Facebook Messenger) Music listening (Google) Music identification ( Shazam ) Posts ( Reddit , Facebook) File Visits ( Emacs ) Annotated Screenshots (Tesseract) Web ( Nostalgia Chrome Plugin ) Products Events Videos Google Search People Indoor Positioning ( whereami ) Public Transport (MijnOV)","title":"Available Data Bindings"},{"location":"#getting-started","text":"Follow the instructions for a source of interest to ensure it is loaded Use the data in an interactive session (run Python)","title":"Getting started"},{"location":"#nostalgia-query-language-extending-pandas","text":"payments . expenses . by_card \\ . last_week \\ . office_days \\ . at_night () \\ . heartrate_above ( 80 ) \\ . when_at ( \"amsterdam\" ) . sum () This will give the total expenses by card in the last week, but only on work days, at night, when my heart rate is above 80 and I'm in Amsterdam. It combined the generic class functionality, together with data from: A Payments provider A Location provider A Heartrate provider","title":"Nostalgia Query Language - extending pandas"},{"location":"#contributing","text":"Please contribute the data sources that others could use as well!","title":"Contributing"},{"location":"docs/1.-connect_your_data/","text":"Connect Your Data Each source has specific instructions on how to get the data into the ecosystem. The benefit of the system starts to show the more data is connected and can be searched/utilized at once. There are 3 ways to find data to connect: searching, data type overview and vendor overview. The latter 2 can also help you discover/inspire you to connect other data. Searching In case you are looking for a particular source, you can search on either github , or goto the documentation and search. Data type overview Heartrate ( Fitbit , Samsung Watch ) Sleep ( Fitbit , Samsung Watch , SleepCycle) Places (Google Timeline) Bank Payments (ING) Pictures ( Google ) Emails ( Gmail ) App Usage ( Google/Android ) Chat Conversations ( WhatsApp , Facebook Messenger ) Music listening ( Google ) Music identification ( Shazam ) Posts ( Reddit , Facebook) File Visits ( Emacs ) Annotated Screenshots (Tesseract) Web ( Nostalgia Chrome Plugin ) Products Events Videos Google Search People Indoor Positioning ( whereami ) Public Transport (MijnOV) Vendor overview If you're looking to load in a lot of data at once, consider adding data from these vendors: Facebook Google Samsung Health (watch) Chrome History Fitbit Data missing? In case the data you want to connect is not in here, you can add it yourself (and please consider contributing it to the ecosystem!).","title":"1. Connect Your Data"},{"location":"docs/1.-connect_your_data/#connect-your-data","text":"Each source has specific instructions on how to get the data into the ecosystem. The benefit of the system starts to show the more data is connected and can be searched/utilized at once. There are 3 ways to find data to connect: searching, data type overview and vendor overview. The latter 2 can also help you discover/inspire you to connect other data.","title":"Connect Your Data"},{"location":"docs/1.-connect_your_data/#searching","text":"In case you are looking for a particular source, you can search on either github , or goto the documentation and search.","title":"Searching"},{"location":"docs/1.-connect_your_data/#data-type-overview","text":"Heartrate ( Fitbit , Samsung Watch ) Sleep ( Fitbit , Samsung Watch , SleepCycle) Places (Google Timeline) Bank Payments (ING) Pictures ( Google ) Emails ( Gmail ) App Usage ( Google/Android ) Chat Conversations ( WhatsApp , Facebook Messenger ) Music listening ( Google ) Music identification ( Shazam ) Posts ( Reddit , Facebook) File Visits ( Emacs ) Annotated Screenshots (Tesseract) Web ( Nostalgia Chrome Plugin ) Products Events Videos Google Search People Indoor Positioning ( whereami ) Public Transport (MijnOV)","title":"Data type overview"},{"location":"docs/1.-connect_your_data/#vendor-overview","text":"If you're looking to load in a lot of data at once, consider adding data from these vendors: Facebook Google Samsung Health (watch) Chrome History Fitbit","title":"Vendor overview"},{"location":"docs/1.-connect_your_data/#data-missing","text":"In case the data you want to connect is not in here, you can add it yourself (and please consider contributing it to the ecosystem!).","title":"Data missing?"},{"location":"docs/2.-add_or_contribute_a_new_data_source/","text":"Add and/or Contribute a New Data Source The first thing to consider is that you should produce a Pandas Dataframe. It should have one or two datetime columns (1 for an event, 2 for a period with a start and end date). It is also useful to have a column called title . Following the pattern below will ensure that your data will play nicely with all the apps and query language. Example First install dependencies: pip install psaw nostalgia Now let's look at the code for collecting your self-written Reddit posts. class RedditPosts ( NDF ): vendor = \"reddit\" @classmethod def ingest ( cls , author ): from psaw import PushshiftAPI api = PushshiftAPI () posts = [ { \"title\" : x . title , \"time\" : datetime_from_timestamp ( x . created_utc ), \"url\" : x . full_link , \"text\" : x . selftext , } for x in api . search_submissions ( author = author ) ] posts = pd . DataFrame ( posts ) posts [ \"author\" ] = author save_df ( posts , \"reddit_posts\" ) @classmethod def load ( cls , nrows = None ): df = load_df ( \"reddit_posts\" , nrows ) return cls ( df ) It extends the basic Nostalgia DataFrame . It has 2 methods: ingest load In this case ingest takes an author as argument (e.g. \"pvkooten\") and will gather all the posts using the free (unauthenticated) PushShift API. In it, you can see it makes use of the provided time helper function datetime_from_timestamp that will ensure it will be converted your current timezone from UTC. It calls nostalgia's save_df function which will compress and cache the data. Then there is the load function, that will allow post-processing the cached data, e.g. to add period itervals that cannot be represented by parquet files. Here is the result after ingest and loading using the timeline: Details Loading data To do data ingestion, there are a few helper functions: read_array_of_dict_from_json load_data_file_modified_time load_json_file_modified_time load_image_texts load_dataframe_per_json_file load_object_per_newline latest_file_is_historic Basically the helper functions help with common file extensions such as CSV and JSON, and consider only having to process as little as possible. For example, it will use modification time of a file to determine whether to do any processing. See their docs for more info. Time There are a couple of time helper functions that can be used to get dates in the right timezone format: Most notably for parsing: def datetime_tz ( * args ): return tz . localize ( datetime ( * args )) def datetime_from_timestamp ( x , tzone = tz , divide_by_1000 = True ): if divide_by_1000 : x = x // 1000 if isinstance ( tzone , str ): tzone = timezone ( tzone ) x = datetime . fromtimestamp ( x , tz = tzone ) if tzone != tz : x = x . astimezone ( tz ) return x def datetime_from_format ( s , fmt , in_utc = False ): base = datetime . strptime ( s , fmt ) if in_utc : return utc . localize ( base ) . astimezone ( tz ) else : return tz . localize ( base ) def parse_date_tz ( text ): ... returns a_metadate Caching It is recommended to use caching (to disk) for expensive calls, for e.g. long calculations, or API calls. from nostalgia.cache import get_cache CACHE = get_cache ( \"screenshots\" ) It depends on diskcache and is very useful. Data will be stored in ~/nostalgia_data/cache and can safely be removed, though at the obvious cost of having to recompute.","title":"2. Add Or Contribute A New Data Source"},{"location":"docs/2.-add_or_contribute_a_new_data_source/#add-andor-contribute-a-new-data-source","text":"The first thing to consider is that you should produce a Pandas Dataframe. It should have one or two datetime columns (1 for an event, 2 for a period with a start and end date). It is also useful to have a column called title . Following the pattern below will ensure that your data will play nicely with all the apps and query language.","title":"Add and/or Contribute a New Data Source"},{"location":"docs/2.-add_or_contribute_a_new_data_source/#example","text":"First install dependencies: pip install psaw nostalgia Now let's look at the code for collecting your self-written Reddit posts. class RedditPosts ( NDF ): vendor = \"reddit\" @classmethod def ingest ( cls , author ): from psaw import PushshiftAPI api = PushshiftAPI () posts = [ { \"title\" : x . title , \"time\" : datetime_from_timestamp ( x . created_utc ), \"url\" : x . full_link , \"text\" : x . selftext , } for x in api . search_submissions ( author = author ) ] posts = pd . DataFrame ( posts ) posts [ \"author\" ] = author save_df ( posts , \"reddit_posts\" ) @classmethod def load ( cls , nrows = None ): df = load_df ( \"reddit_posts\" , nrows ) return cls ( df ) It extends the basic Nostalgia DataFrame . It has 2 methods: ingest load In this case ingest takes an author as argument (e.g. \"pvkooten\") and will gather all the posts using the free (unauthenticated) PushShift API. In it, you can see it makes use of the provided time helper function datetime_from_timestamp that will ensure it will be converted your current timezone from UTC. It calls nostalgia's save_df function which will compress and cache the data. Then there is the load function, that will allow post-processing the cached data, e.g. to add period itervals that cannot be represented by parquet files. Here is the result after ingest and loading using the timeline:","title":"Example"},{"location":"docs/2.-add_or_contribute_a_new_data_source/#details","text":"","title":"Details"},{"location":"docs/2.-add_or_contribute_a_new_data_source/#loading-data","text":"To do data ingestion, there are a few helper functions: read_array_of_dict_from_json load_data_file_modified_time load_json_file_modified_time load_image_texts load_dataframe_per_json_file load_object_per_newline latest_file_is_historic Basically the helper functions help with common file extensions such as CSV and JSON, and consider only having to process as little as possible. For example, it will use modification time of a file to determine whether to do any processing. See their docs for more info.","title":"Loading data"},{"location":"docs/2.-add_or_contribute_a_new_data_source/#time","text":"There are a couple of time helper functions that can be used to get dates in the right timezone format: Most notably for parsing: def datetime_tz ( * args ): return tz . localize ( datetime ( * args )) def datetime_from_timestamp ( x , tzone = tz , divide_by_1000 = True ): if divide_by_1000 : x = x // 1000 if isinstance ( tzone , str ): tzone = timezone ( tzone ) x = datetime . fromtimestamp ( x , tz = tzone ) if tzone != tz : x = x . astimezone ( tz ) return x def datetime_from_format ( s , fmt , in_utc = False ): base = datetime . strptime ( s , fmt ) if in_utc : return utc . localize ( base ) . astimezone ( tz ) else : return tz . localize ( base ) def parse_date_tz ( text ): ... returns a_metadate","title":"Time"},{"location":"docs/2.-add_or_contribute_a_new_data_source/#caching","text":"It is recommended to use caching (to disk) for expensive calls, for e.g. long calculations, or API calls. from nostalgia.cache import get_cache CACHE = get_cache ( \"screenshots\" ) It depends on diskcache and is very useful. Data will be stored in ~/nostalgia_data/cache and can safely be removed, though at the obvious cost of having to recompute.","title":"Caching"},{"location":"docs/explore_your_data/1.-using_nostalgia_ql/","text":"NostalgiaQL","title":"1. Using Nostalgia Ql"},{"location":"docs/explore_your_data/1.-using_nostalgia_ql/#nostalgiaql","text":"","title":"NostalgiaQL"},{"location":"docs/explore_your_data/2.-using_timeline/","text":"Timeline","title":"2. Using Timeline"},{"location":"docs/explore_your_data/2.-using_timeline/#timeline","text":"","title":"Timeline"},{"location":"docs/explore_your_data/3.-using_chatbot/","text":"Chatbot","title":"3. Using Chatbot"},{"location":"docs/explore_your_data/3.-using_chatbot/#chatbot","text":"","title":"Chatbot"},{"location":"docs/searching_your_data/by_location/","text":"test","title":"By Location"},{"location":"docs/searching_your_data/by_text/","text":"test","title":"By Text"},{"location":"docs/searching_your_data/by_time/","text":"test","title":"By Time"},{"location":"reference/nostalgia/","text":"Module nostalgia View Source import just ENTRY = \"~/nostalgia_data/nostalgia_entry.py\" if not just . exists ( \"~/nostalgia_data/nostalgia_entry.py\" ): just . write ( \"\" , ENTRY ) Sub-modules nostalgia.cache nostalgia.data_loading nostalgia.extracter nostalgia.file_caching nostalgia.interfaces nostalgia.ndf nostalgia.nlp nostalgia.selenium nostalgia.sources nostalgia.times nostalgia.unzip nostalgia.utils Variables ENTRY","title":"Index"},{"location":"reference/nostalgia/#module-nostalgia","text":"View Source import just ENTRY = \"~/nostalgia_data/nostalgia_entry.py\" if not just . exists ( \"~/nostalgia_data/nostalgia_entry.py\" ): just . write ( \"\" , ENTRY )","title":"Module nostalgia"},{"location":"reference/nostalgia/#sub-modules","text":"nostalgia.cache nostalgia.data_loading nostalgia.extracter nostalgia.file_caching nostalgia.interfaces nostalgia.ndf nostalgia.nlp nostalgia.selenium nostalgia.sources nostalgia.times nostalgia.unzip nostalgia.utils","title":"Sub-modules"},{"location":"reference/nostalgia/#variables","text":"ENTRY","title":"Variables"},{"location":"reference/nostalgia/cache/","text":"Module nostalgia.cache View Source import os import diskcache def get_cache ( name ): return diskcache . Cache ( os . path . expanduser ( \"~/nostalgia_data/cache/\" + name )) Functions get_cache def get_cache ( name ) View Source def get_cache(name): return diskcache.Cache(os.path.expanduser(\"~/nostalgia_data/cache/\" + name))","title":"Cache"},{"location":"reference/nostalgia/cache/#module-nostalgiacache","text":"View Source import os import diskcache def get_cache ( name ): return diskcache . Cache ( os . path . expanduser ( \"~/nostalgia_data/cache/\" + name ))","title":"Module nostalgia.cache"},{"location":"reference/nostalgia/cache/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/cache/#get_cache","text":"def get_cache ( name ) View Source def get_cache(name): return diskcache.Cache(os.path.expanduser(\"~/nostalgia_data/cache/\" + name))","title":"get_cache"},{"location":"reference/nostalgia/data_loading/","text":"Module nostalgia.data_loading View Source import os import just import pandas as pd from nostalgia.utils import normalize_name from nostalgia.file_caching import save_df , load_df from nostalgia.file_caching import get_newline_count , save_newline_count from nostalgia.file_caching import get_processed_files , save_processed_files from nostalgia.file_caching import get_last_latest_file , save_last_latest_file from nostalgia.file_caching import get_last_mod_time , save_last_mod_time def read_array_of_dict_from_json ( fname , key_name = None , nrows = None ): \"\"\" This is an iterative way to read a json file without having to construct Python elements for everything. It can be a lot faster. Example data: {\"participants\": {\"name\": \"a\", \"name\": \"b\", \"messages\": [{\"sender\": \"a\", \"time\": 123}, {\"sender\": \"b\", \"time\": 124}]}} Function call: read_array_of_dict_from_json(fname, \"messages\", nrows=1) Returns: pd.DataFrame([{\"sender\": \"a\", \"time\": 123}]) \"\"\" if nrows is None : if not key_name : return pd . read_json ( fname , lines = fname . endswith ( \".jsonl\" )) else : return pd . DataFrame ( just . read ( fname )[ key_name ]) import ijson with open ( just . make_path ( fname )) as f : parser = ijson . parse ( f ) capture = False rows = [] row = {} map_key = \"\" num = 0 for prefix , event , value in parser : if num & gt ; nrows : break if prefix == key_name and event == \"start_array\" : capture = True if not capture : continue if event == \"start_map\" : continue elif event == \"map_key\" : map_key = value elif event == \"end_map\" : rows . append ( row ) row = {} num += 1 elif map_key : row [ map_key ] = value return pd . DataFrame ( rows ) class Loader : @classmethod def load_data_file_modified_time ( cls , fname , key_name = \"\" , nrows = None , from_cache = True , ** kwargs ): \"\"\" It will load from cache if filename is not changed since last run (and there is a cache). If it has changed, it will reprocess and save it in cache (including the modified_time). Handles csv, mbox and json currently. key_name is only for json. nrows is for enabling quickly loading a sample. from_cache=False allows ignoring the cache and reprocessing the file. Loading the csv, json or mbox file will yield you a DF IMPORTANT: assumes you implement `handle_dataframe_per_file` This is the post-processing required after the file is loaded, for e.g. converting time dropping and adding columns. \"\"\" name = fname + \"_\" + normalize_name ( cls . __name__ ) modified_time = os . path . getmtime ( os . path . expanduser ( fname )) last_modified = get_last_mod_time ( name ) if modified_time != last_modified or not from_cache : if fname . endswith ( \".csv\" ): data = pd . read_csv ( fname , error_bad_lines = False , nrows = nrows , ** kwargs ) elif fname . endswith ( \".mbox\" ): import mailbox m = mailbox . mbox ( fname ) data = pd . DataFrame ( [{ l : x [ l ] for l in [ \"from\" , \"to\" , \"date\" , \"subject\" ]} for x in m ] ) else : data = read_array_of_dict_from_json ( fname , key_name , nrows , ** kwargs ) data = cls . handle_dataframe_per_file ( data , fname ) if nrows is None : save_df ( data , name ) save_last_mod_time ( modified_time , name ) else : data = load_df ( name , nrows ) if nrows is not None : data = data . iloc [ - nrows :] return data @classmethod def load_json_file_modified_time ( cls , fname , nrows = None , from_cache = True , ** kwargs ): name = fname + \"_\" + normalize_name ( cls . __name__ ) modified_time = os . path . getmtime ( os . path . expanduser ( fname )) last_modified = get_last_mod_time ( name ) if modified_time != last_modified or not from_cache : data = just . read ( fname ) data = cls . handle_json ( data , ** kwargs ) data = pd . DataFrame ( data ) if nrows is None : save_df ( data , name ) save_last_mod_time ( modified_time , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows :] return data @classmethod def load_image_texts ( cls , glob_pattern_s , nrows = None ): import pytesseract from PIL import Image if isinstance ( glob_pattern_s , list ): fnames = set () for glob_pattern in glob_pattern_s : fnames . update ( set ( just . glob ( glob_pattern ))) glob_pattern = \"_\" . join ( glob_pattern_s ) else : fnames = set ( just . glob ( glob_pattern )) name = glob_pattern + \"_\" + normalize_name ( cls . __name__ ) processed_files = get_processed_files ( name ) to_process = fnames . difference ( processed_files ) objects = [] cache = get_cache ( \"tesseract\" ) if nrows is not None : if not to_process : return load_df ( name ) . iloc [ - nrows :] else : to_process = list ( to_process )[ - nrows :] if to_process : for fname in to_process : if fname in cache : text = cache [ fname ] else : try : text = pytesseract . image_to_string ( Image . open ( just . make_path ( fname ))) except OSError as e : print ( \"ERR\" , fname , e ) continue cache [ fname ] = text time = datetime_from_timestamp ( os . path . getmtime ( fname ), \"utc\" ) data = { \"text\" : text , \"path\" : fname , \"title\" : fname . split ( \"/\" )[ - 1 ], \"time\" : time } objects . append ( data ) data = pd . DataFrame ( objects ) if processed_files and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ]: if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) save_processed_files ( fnames | processed_files , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows :] return data @classmethod def load_dataframe_per_json_file ( cls , glob_pattern , key = \"\" , nrows = None ): fnames = set ( just . glob ( glob_pattern )) name = glob_pattern + \"_\" + normalize_name ( cls . __name__ ) processed_files = get_processed_files ( name ) to_process = fnames . difference ( processed_files ) objects = [] if nrows is not None : if not to_process : to_process = list ( processed_files )[ - nrows :] else : to_process = list ( to_process )[ - nrows :] if to_process : print ( \"processing {} files\" . format ( len ( to_process ))) for fname in to_process : data = read_array_of_dict_from_json ( fname , key , nrows ) data = cls . handle_dataframe_per_file ( data , fname ) if data is None : continue objects . append ( data ) data = pd . concat ( objects ) if processed_files and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ]: if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) save_processed_files ( fnames | processed_files , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows :] return data @classmethod def load_object_per_newline ( cls , fname , nrows = None ): \"\"\" Iterates over a file containing an object per line (e.g. .jsonl or .txt). Will only handle new lines not seen earlier; it detects this by storing the number-of-objects seen. You should implement `object_to_row(cls, row)` on your class that returns a dictionary. \"\"\" data = [] name = fname + \"_\" + normalize_name ( cls . __name__ ) newline_count = get_newline_count ( name ) for i , x in enumerate ( just . iread ( fname )): if nrows is None : if i & lt ; newline_count : continue row = cls . object_to_row ( x ) if row is None : continue data . append ( row ) # breaking at approx 5 rows if nrows is not None and i & gt ; nrows : break if data : data = pd . DataFrame ( data ) if newline_count and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ]: if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) n = i + 1 save_newline_count ( n , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows :] return data @classmethod def latest_file_is_historic ( cls , glob , key_name = \"\" , nrows = None , from_cache = True ): \"\"\" Glob is for using a wildcard pattern, and the last created file will be loaded. See `load_data_file_modified_time` for further reference. Returns a pd.DataFrame \"\"\" recent = max ([ x for x in just . glob ( glob ) if \"(\" not in x ], key = os . path . getctime ) return cls . load_data_file_modified_time ( recent , key_name , nrows , from_cache ) Functions read_array_of_dict_from_json def read_array_of_dict_from_json ( fname , key_name = None , nrows = None ) This is an iterative way to read a json file without having to construct Python elements for everything. It can be a lot faster. Example data: {\"participants\": {\"name\": \"a\", \"name\": \"b\", \"messages\": [{\"sender\": \"a\", \"time\": 123}, {\"sender\": \"b\", \"time\": 124}]}} Function call: read_array_of_dict_from_json(fname, \"messages\", nrows=1) Returns: pd.DataFrame([{\"sender\": \"a\", \"time\": 123}]) View Source def read_array_of_dict_from_json ( fname , key_name = None , nrows = None ): \"\"\" This is an iterative way to read a json file without having to construct Python elements for everything. It can be a lot faster. Example data: {\"participants\": {\"name\": \"a\", \"name\": \"b\", \"messages\": [{\"sender\": \"a\", \"time\": 123}, {\"sender\": \"b\", \"time\": 124}]}} Function call: read_array_of_dict_from_json(fname, \"messages\", nrows=1) Returns: pd.DataFrame([{\"sender\": \"a\", \"time\": 123}]) \"\"\" if nrows is None : if not key_name : return pd . read_json ( fname , lines = fname . endswith ( \".jsonl\" )) else : return pd . DataFrame ( just . read ( fname )[ key_name ]) import ijson with open ( just . make_path ( fname )) as f : parser = ijson . parse ( f ) capture = False rows = [] row = {} map_key = \"\" num = 0 for prefix , event , value in parser : if num & gt ; nrows : break if prefix == key_name and event == \"start_array\" : capture = True if not capture : continue if event == \"start_map\" : continue elif event == \"map_key\" : map_key = value elif event == \"end_map\" : rows . append ( row ) row = {} num += 1 elif map_key : row [ map_key ] = value return pd . DataFrame ( rows ) Classes Loader class Loader ( / , * args , ** kwargs ) View Source class Loader : @classmethod def load_data_file_modified_time ( cls , fname , key_name = \"\" , nrows = None , from_cache = True , ** kwargs ) : \"\"\" It will load from cache if filename is not changed since last run (and there is a cache). If it has changed, it will reprocess and save it in cache (including the modified_time). Handles csv, mbox and json currently. key_name is only for json. nrows is for enabling quickly loading a sample. from_cache=False allows ignoring the cache and reprocessing the file. Loading the csv, json or mbox file will yield you a DF IMPORTANT: assumes you implement `handle_dataframe_per_file` This is the post-processing required after the file is loaded, for e.g. converting time dropping and adding columns. \"\"\" name = fname + \"_\" + normalize_name ( cls . __name__ ) modified_time = os . path . getmtime ( os . path . expanduser ( fname )) last_modified = get_last_mod_time ( name ) if modified_time != last_modified or not from_cache : if fname . endswith ( \".csv\" ) : data = pd . read_csv ( fname , error_bad_lines = False , nrows = nrows , ** kwargs ) elif fname . endswith ( \".mbox\" ) : import mailbox m = mailbox . mbox ( fname ) data = pd . DataFrame ( [{ l : x [ l ] for l in [ \"from\" , \"to\" , \"date\" , \"subject\" ]} for x in m ] ) else : data = read_array_of_dict_from_json ( fname , key_name , nrows , ** kwargs ) data = cls . handle_dataframe_per_file ( data , fname ) if nrows is None : save_df ( data , name ) save_last_mod_time ( modified_time , name ) else : data = load_df ( name , nrows ) if nrows is not None : data = data . iloc [ - nrows : ] return data @classmethod def load_json_file_modified_time ( cls , fname , nrows = None , from_cache = True , ** kwargs ) : name = fname + \"_\" + normalize_name ( cls . __name__ ) modified_time = os . path . getmtime ( os . path . expanduser ( fname )) last_modified = get_last_mod_time ( name ) if modified_time != last_modified or not from_cache : data = just . read ( fname ) data = cls . handle_json ( data , ** kwargs ) data = pd . DataFrame ( data ) if nrows is None : save_df ( data , name ) save_last_mod_time ( modified_time , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows : ] return data @classmethod def load_image_texts ( cls , glob_pattern_s , nrows = None ) : import pytesseract from PIL import Image if isinstance ( glob_pattern_s , list ) : fnames = set () for glob_pattern in glob_pattern_s : fnames.update ( set ( just . glob ( glob_pattern ))) glob_pattern = \"_\" . join ( glob_pattern_s ) else : fnames = set ( just . glob ( glob_pattern )) name = glob_pattern + \"_\" + normalize_name ( cls . __name__ ) processed_files = get_processed_files ( name ) to_process = fnames . difference ( processed_files ) objects = [] cache = get_cache ( \"tesseract\" ) if nrows is not None : if not to_process : return load_df ( name ). iloc [ - nrows : ] else : to_process = list ( to_process )[ - nrows : ] if to_process : for fname in to_process : if fname in cache : text = cache [ fname ] else : try : text = pytesseract . image_to_string ( Image . open ( just . make_path ( fname ))) except OSError as e : print ( \"ERR\" , fname , e ) continue cache [ fname ] = text time = datetime_from_timestamp ( os . path . getmtime ( fname ), \"utc\" ) data = { \"text\" : text , \"path\" : fname , \"title\" : fname . split ( \"/\" )[ - 1 ], \"time\" : time } objects . append ( data ) data = pd . DataFrame ( objects ) if processed_files and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ] : if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) save_processed_files ( fnames | processed_files , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows : ] return data @classmethod def load_dataframe_per_json_file ( cls , glob_pattern , key = \"\" , nrows = None ) : fnames = set ( just . glob ( glob_pattern )) name = glob_pattern + \"_\" + normalize_name ( cls . __name__ ) processed_files = get_processed_files ( name ) to_process = fnames . difference ( processed_files ) objects = [] if nrows is not None : if not to_process : to_process = list ( processed_files )[ - nrows : ] else : to_process = list ( to_process )[ - nrows : ] if to_process : print ( \"processing {} files\" . format ( len ( to_process ))) for fname in to_process : data = read_array_of_dict_from_json ( fname , key , nrows ) data = cls . handle_dataframe_per_file ( data , fname ) if data is None : continue objects . append ( data ) data = pd . concat ( objects ) if processed_files and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ] : if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) save_processed_files ( fnames | processed_files , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows : ] return data @classmethod def load_object_per_newline ( cls , fname , nrows = None ) : \"\"\" Iterates over a file containing an object per line (e.g. .jsonl or .txt). Will only handle new lines not seen earlier; it detects this by storing the number-of-objects seen. You should implement `object_to_row(cls, row)` on your class that returns a dictionary. \"\"\" data = [] name = fname + \"_\" + normalize_name ( cls . __name__ ) newline_count = get_newline_count ( name ) for i , x in enumerate ( just . iread ( fname )) : if nrows is None : if i & lt ; newline_count : continue row = cls . object_to_row ( x ) if row is None : continue data . append ( row ) # breaking at approx 5 rows if nrows is not None and i & gt ; nrows : break if data : data = pd . DataFrame ( data ) if newline_count and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ] : if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) n = i + 1 save_newline_count ( n , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows : ] return data @classmethod def latest_file_is_historic ( cls , glob , key_name = \"\" , nrows = None , from_cache = True ) : \"\"\" Glob is for using a wildcard pattern, and the last created file will be loaded. See `load_data_file_modified_time` for further reference. Returns a pd.DataFrame \"\"\" recent = max ([ x for x in just . glob ( glob ) if \"(\" not in x ], key = os . path . getctime ) return cls . load_data_file_modified_time ( recent , key_name , nrows , from_cache ) Static methods latest_file_is_historic def latest_file_is_historic ( glob , key_name = '' , nrows = None , from_cache = True ) Glob is for using a wildcard pattern, and the last created file will be loaded. See load_data_file_modified_time for further reference. Returns a pd.DataFrame View Source @classmethod def latest_file_is_historic(cls, glob, key_name=\"\", nrows=None, from_cache=True): \"\"\" Glob is for using a wildcard pattern, and the last created file will be loaded. See `load_data_file_modified_time` for further reference. Returns a pd.DataFrame \"\"\" recent = max([x for x in just.glob(glob) if \"(\" not in x], key=os.path.getctime) return cls.load_data_file_modified_time(recent, key_name, nrows, from_cache) load_data_file_modified_time def load_data_file_modified_time ( fname , key_name = '' , nrows = None , from_cache = True , ** kwargs ) It will load from cache if filename is not changed since last run (and there is a cache). If it has changed, it will reprocess and save it in cache (including the modified_time). Handles csv, mbox and json currently. key_name is only for json. nrows is for enabling quickly loading a sample. from_cache=False allows ignoring the cache and reprocessing the file. Loading the csv, json or mbox file will yield you a DF IMPORTANT: assumes you implement handle_dataframe_per_file This is the post-processing required after the file is loaded, for e.g. converting time dropping and adding columns. View Source @classmethod def load_data_file_modified_time( cls, fname, key_name=\"\", nrows=None, from_cache=True, **kwargs ): \"\"\" It will load from cache if filename is not changed since last run (and there is a cache). If it has changed, it will reprocess and save it in cache (including the modified_time). Handles csv, mbox and json currently. key_name is only for json. nrows is for enabling quickly loading a sample. from_cache=False allows ignoring the cache and reprocessing the file. Loading the csv, json or mbox file will yield you a DF IMPORTANT: assumes you implement `handle_dataframe_per_file` This is the post-processing required after the file is loaded, for e.g. converting time dropping and adding columns. \"\"\" name = fname + \"_\" + normalize_name(cls.__name__) modified_time = os.path.getmtime(os.path.expanduser(fname)) last_modified = get_last_mod_time(name) if modified_time != last_modified or not from_cache: if fname.endswith(\".csv\"): data = pd.read_csv(fname, error_bad_lines=False, nrows=nrows, **kwargs) elif fname.endswith(\".mbox\"): import mailbox m = mailbox.mbox(fname) data = pd.DataFrame( [{l: x[l] for l in [\"from\", \"to\", \"date\", \"subject\"]} for x in m] ) else: data = read_array_of_dict_from_json(fname, key_name, nrows, **kwargs) data = cls.handle_dataframe_per_file(data, fname) if nrows is None: save_df(data, name) save_last_mod_time(modified_time, name) else: data = load_df(name, nrows) if nrows is not None: data = data.iloc[-nrows:] return data load_dataframe_per_json_file def load_dataframe_per_json_file ( glob_pattern , key = '' , nrows = None ) View Source @classmethod def load_dataframe_per_json_file(cls, glob_pattern, key=\"\", nrows=None): fnames = set(just.glob(glob_pattern)) name = glob_pattern + \"_\" + normalize_name(cls.__name__) processed_files = get_processed_files(name) to_process = fnames.difference(processed_files) objects = [] if nrows is not None: if not to_process: to_process = list(processed_files)[-nrows:] else: to_process = list(to_process)[-nrows:] if to_process: print(\"processing {} files\".format(len(to_process))) for fname in to_process: data = read_array_of_dict_from_json(fname, key, nrows) data = cls.handle_dataframe_per_file(data, fname) if data is None: continue objects.append(data) data = pd.concat(objects) if processed_files and nrows is None: data = pd.concat((data, load_df(name))) for x in [\"time\", \"start\", \"end\"]: if x in data: data = data.sort_values(x) break if nrows is None: save_df(data, name) save_processed_files(fnames | processed_files, name) else: data = load_df(name) if nrows is not None: data = data.iloc[-nrows:] return data load_image_texts def load_image_texts ( glob_pattern_s , nrows = None ) View Source @classmethod def load_image_texts ( cls , glob_pattern_s , nrows = None ): import pytesseract from PIL import Image if isinstance ( glob_pattern_s , list ): fnames = set () for glob_pattern in glob_pattern_s : fnames . update ( set ( just . glob ( glob_pattern ))) glob_pattern = \"_\" . join ( glob_pattern_s ) else : fnames = set ( just . glob ( glob_pattern )) name = glob_pattern + \"_\" + normalize_name ( cls . __name__ ) processed_files = get_processed_files ( name ) to_process = fnames . difference ( processed_files ) objects = [] cache = get_cache ( \"tesseract\" ) if nrows is not None : if not to_process : return load_df ( name ) . iloc [ - nrows :] else : to_process = list ( to_process )[ - nrows :] if to_process : for fname in to_process : if fname in cache : text = cache [ fname ] else : try : text = pytesseract . image_to_string ( Image . open ( just . make_path ( fname ))) except OSError as e : print ( \"ERR\" , fname , e ) continue cache [ fname ] = text time = datetime_from_timestamp ( os . path . getmtime ( fname ), \"utc\" ) data = { \"text\" : text , \"path\" : fname , \"title\" : fname . split ( \"/\" )[ - 1 ], \"time\" : time } objects . append ( data ) data = pd . DataFrame ( objects ) if processed_files and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ]: if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) save_processed_files ( fnames | processed_files , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows :] return data load_json_file_modified_time def load_json_file_modified_time ( fname , nrows = None , from_cache = True , ** kwargs ) View Source @classmethod def load_json_file_modified_time(cls, fname, nrows=None, from_cache=True, **kwargs): name = fname + \"_\" + normalize_name(cls.__name__) modified_time = os.path.getmtime(os.path.expanduser(fname)) last_modified = get_last_mod_time(name) if modified_time != last_modified or not from_cache: data = just.read(fname) data = cls.handle_json(data, **kwargs) data = pd.DataFrame(data) if nrows is None: save_df(data, name) save_last_mod_time(modified_time, name) else: data = load_df(name) if nrows is not None: data = data.iloc[-nrows:] return data load_object_per_newline def load_object_per_newline ( fname , nrows = None ) Iterates over a file containing an object per line (e.g. .jsonl or .txt). Will only handle new lines not seen earlier; it detects this by storing the number-of-objects seen. You should implement object_to_row(cls, row) on your class that returns a dictionary. View Source @classmethod def load_object_per_newline(cls, fname, nrows=None): \"\"\" Iterates over a file containing an object per line (e.g. .jsonl or .txt). Will only handle new lines not seen earlier; it detects this by storing the number-of-objects seen. You should implement `object_to_row(cls, row)` on your class that returns a dictionary. \"\"\" data = [] name = fname + \"_\" + normalize_name(cls.__name__) newline_count = get_newline_count(name) for i, x in enumerate(just.iread(fname)): if nrows is None: if i &lt; newline_count: continue row = cls.object_to_row(x) if row is None: continue data.append(row) # breaking at approx 5 rows if nrows is not None and i &gt; nrows: break if data: data = pd.DataFrame(data) if newline_count and nrows is None: data = pd.concat((data, load_df(name))) for x in [\"time\", \"start\", \"end\"]: if x in data: data = data.sort_values(x) break if nrows is None: save_df(data, name) n = i + 1 save_newline_count(n, name) else: data = load_df(name) if nrows is not None: data = data.iloc[-nrows:] return data","title":"Data Loading"},{"location":"reference/nostalgia/data_loading/#module-nostalgiadata_loading","text":"View Source import os import just import pandas as pd from nostalgia.utils import normalize_name from nostalgia.file_caching import save_df , load_df from nostalgia.file_caching import get_newline_count , save_newline_count from nostalgia.file_caching import get_processed_files , save_processed_files from nostalgia.file_caching import get_last_latest_file , save_last_latest_file from nostalgia.file_caching import get_last_mod_time , save_last_mod_time def read_array_of_dict_from_json ( fname , key_name = None , nrows = None ): \"\"\" This is an iterative way to read a json file without having to construct Python elements for everything. It can be a lot faster. Example data: {\"participants\": {\"name\": \"a\", \"name\": \"b\", \"messages\": [{\"sender\": \"a\", \"time\": 123}, {\"sender\": \"b\", \"time\": 124}]}} Function call: read_array_of_dict_from_json(fname, \"messages\", nrows=1) Returns: pd.DataFrame([{\"sender\": \"a\", \"time\": 123}]) \"\"\" if nrows is None : if not key_name : return pd . read_json ( fname , lines = fname . endswith ( \".jsonl\" )) else : return pd . DataFrame ( just . read ( fname )[ key_name ]) import ijson with open ( just . make_path ( fname )) as f : parser = ijson . parse ( f ) capture = False rows = [] row = {} map_key = \"\" num = 0 for prefix , event , value in parser : if num & gt ; nrows : break if prefix == key_name and event == \"start_array\" : capture = True if not capture : continue if event == \"start_map\" : continue elif event == \"map_key\" : map_key = value elif event == \"end_map\" : rows . append ( row ) row = {} num += 1 elif map_key : row [ map_key ] = value return pd . DataFrame ( rows ) class Loader : @classmethod def load_data_file_modified_time ( cls , fname , key_name = \"\" , nrows = None , from_cache = True , ** kwargs ): \"\"\" It will load from cache if filename is not changed since last run (and there is a cache). If it has changed, it will reprocess and save it in cache (including the modified_time). Handles csv, mbox and json currently. key_name is only for json. nrows is for enabling quickly loading a sample. from_cache=False allows ignoring the cache and reprocessing the file. Loading the csv, json or mbox file will yield you a DF IMPORTANT: assumes you implement `handle_dataframe_per_file` This is the post-processing required after the file is loaded, for e.g. converting time dropping and adding columns. \"\"\" name = fname + \"_\" + normalize_name ( cls . __name__ ) modified_time = os . path . getmtime ( os . path . expanduser ( fname )) last_modified = get_last_mod_time ( name ) if modified_time != last_modified or not from_cache : if fname . endswith ( \".csv\" ): data = pd . read_csv ( fname , error_bad_lines = False , nrows = nrows , ** kwargs ) elif fname . endswith ( \".mbox\" ): import mailbox m = mailbox . mbox ( fname ) data = pd . DataFrame ( [{ l : x [ l ] for l in [ \"from\" , \"to\" , \"date\" , \"subject\" ]} for x in m ] ) else : data = read_array_of_dict_from_json ( fname , key_name , nrows , ** kwargs ) data = cls . handle_dataframe_per_file ( data , fname ) if nrows is None : save_df ( data , name ) save_last_mod_time ( modified_time , name ) else : data = load_df ( name , nrows ) if nrows is not None : data = data . iloc [ - nrows :] return data @classmethod def load_json_file_modified_time ( cls , fname , nrows = None , from_cache = True , ** kwargs ): name = fname + \"_\" + normalize_name ( cls . __name__ ) modified_time = os . path . getmtime ( os . path . expanduser ( fname )) last_modified = get_last_mod_time ( name ) if modified_time != last_modified or not from_cache : data = just . read ( fname ) data = cls . handle_json ( data , ** kwargs ) data = pd . DataFrame ( data ) if nrows is None : save_df ( data , name ) save_last_mod_time ( modified_time , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows :] return data @classmethod def load_image_texts ( cls , glob_pattern_s , nrows = None ): import pytesseract from PIL import Image if isinstance ( glob_pattern_s , list ): fnames = set () for glob_pattern in glob_pattern_s : fnames . update ( set ( just . glob ( glob_pattern ))) glob_pattern = \"_\" . join ( glob_pattern_s ) else : fnames = set ( just . glob ( glob_pattern )) name = glob_pattern + \"_\" + normalize_name ( cls . __name__ ) processed_files = get_processed_files ( name ) to_process = fnames . difference ( processed_files ) objects = [] cache = get_cache ( \"tesseract\" ) if nrows is not None : if not to_process : return load_df ( name ) . iloc [ - nrows :] else : to_process = list ( to_process )[ - nrows :] if to_process : for fname in to_process : if fname in cache : text = cache [ fname ] else : try : text = pytesseract . image_to_string ( Image . open ( just . make_path ( fname ))) except OSError as e : print ( \"ERR\" , fname , e ) continue cache [ fname ] = text time = datetime_from_timestamp ( os . path . getmtime ( fname ), \"utc\" ) data = { \"text\" : text , \"path\" : fname , \"title\" : fname . split ( \"/\" )[ - 1 ], \"time\" : time } objects . append ( data ) data = pd . DataFrame ( objects ) if processed_files and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ]: if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) save_processed_files ( fnames | processed_files , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows :] return data @classmethod def load_dataframe_per_json_file ( cls , glob_pattern , key = \"\" , nrows = None ): fnames = set ( just . glob ( glob_pattern )) name = glob_pattern + \"_\" + normalize_name ( cls . __name__ ) processed_files = get_processed_files ( name ) to_process = fnames . difference ( processed_files ) objects = [] if nrows is not None : if not to_process : to_process = list ( processed_files )[ - nrows :] else : to_process = list ( to_process )[ - nrows :] if to_process : print ( \"processing {} files\" . format ( len ( to_process ))) for fname in to_process : data = read_array_of_dict_from_json ( fname , key , nrows ) data = cls . handle_dataframe_per_file ( data , fname ) if data is None : continue objects . append ( data ) data = pd . concat ( objects ) if processed_files and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ]: if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) save_processed_files ( fnames | processed_files , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows :] return data @classmethod def load_object_per_newline ( cls , fname , nrows = None ): \"\"\" Iterates over a file containing an object per line (e.g. .jsonl or .txt). Will only handle new lines not seen earlier; it detects this by storing the number-of-objects seen. You should implement `object_to_row(cls, row)` on your class that returns a dictionary. \"\"\" data = [] name = fname + \"_\" + normalize_name ( cls . __name__ ) newline_count = get_newline_count ( name ) for i , x in enumerate ( just . iread ( fname )): if nrows is None : if i & lt ; newline_count : continue row = cls . object_to_row ( x ) if row is None : continue data . append ( row ) # breaking at approx 5 rows if nrows is not None and i & gt ; nrows : break if data : data = pd . DataFrame ( data ) if newline_count and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ]: if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) n = i + 1 save_newline_count ( n , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows :] return data @classmethod def latest_file_is_historic ( cls , glob , key_name = \"\" , nrows = None , from_cache = True ): \"\"\" Glob is for using a wildcard pattern, and the last created file will be loaded. See `load_data_file_modified_time` for further reference. Returns a pd.DataFrame \"\"\" recent = max ([ x for x in just . glob ( glob ) if \"(\" not in x ], key = os . path . getctime ) return cls . load_data_file_modified_time ( recent , key_name , nrows , from_cache )","title":"Module nostalgia.data_loading"},{"location":"reference/nostalgia/data_loading/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/data_loading/#read_array_of_dict_from_json","text":"def read_array_of_dict_from_json ( fname , key_name = None , nrows = None ) This is an iterative way to read a json file without having to construct Python elements for everything. It can be a lot faster. Example data: {\"participants\": {\"name\": \"a\", \"name\": \"b\", \"messages\": [{\"sender\": \"a\", \"time\": 123}, {\"sender\": \"b\", \"time\": 124}]}} Function call: read_array_of_dict_from_json(fname, \"messages\", nrows=1) Returns: pd.DataFrame([{\"sender\": \"a\", \"time\": 123}]) View Source def read_array_of_dict_from_json ( fname , key_name = None , nrows = None ): \"\"\" This is an iterative way to read a json file without having to construct Python elements for everything. It can be a lot faster. Example data: {\"participants\": {\"name\": \"a\", \"name\": \"b\", \"messages\": [{\"sender\": \"a\", \"time\": 123}, {\"sender\": \"b\", \"time\": 124}]}} Function call: read_array_of_dict_from_json(fname, \"messages\", nrows=1) Returns: pd.DataFrame([{\"sender\": \"a\", \"time\": 123}]) \"\"\" if nrows is None : if not key_name : return pd . read_json ( fname , lines = fname . endswith ( \".jsonl\" )) else : return pd . DataFrame ( just . read ( fname )[ key_name ]) import ijson with open ( just . make_path ( fname )) as f : parser = ijson . parse ( f ) capture = False rows = [] row = {} map_key = \"\" num = 0 for prefix , event , value in parser : if num & gt ; nrows : break if prefix == key_name and event == \"start_array\" : capture = True if not capture : continue if event == \"start_map\" : continue elif event == \"map_key\" : map_key = value elif event == \"end_map\" : rows . append ( row ) row = {} num += 1 elif map_key : row [ map_key ] = value return pd . DataFrame ( rows )","title":"read_array_of_dict_from_json"},{"location":"reference/nostalgia/data_loading/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/data_loading/#loader","text":"class Loader ( / , * args , ** kwargs ) View Source class Loader : @classmethod def load_data_file_modified_time ( cls , fname , key_name = \"\" , nrows = None , from_cache = True , ** kwargs ) : \"\"\" It will load from cache if filename is not changed since last run (and there is a cache). If it has changed, it will reprocess and save it in cache (including the modified_time). Handles csv, mbox and json currently. key_name is only for json. nrows is for enabling quickly loading a sample. from_cache=False allows ignoring the cache and reprocessing the file. Loading the csv, json or mbox file will yield you a DF IMPORTANT: assumes you implement `handle_dataframe_per_file` This is the post-processing required after the file is loaded, for e.g. converting time dropping and adding columns. \"\"\" name = fname + \"_\" + normalize_name ( cls . __name__ ) modified_time = os . path . getmtime ( os . path . expanduser ( fname )) last_modified = get_last_mod_time ( name ) if modified_time != last_modified or not from_cache : if fname . endswith ( \".csv\" ) : data = pd . read_csv ( fname , error_bad_lines = False , nrows = nrows , ** kwargs ) elif fname . endswith ( \".mbox\" ) : import mailbox m = mailbox . mbox ( fname ) data = pd . DataFrame ( [{ l : x [ l ] for l in [ \"from\" , \"to\" , \"date\" , \"subject\" ]} for x in m ] ) else : data = read_array_of_dict_from_json ( fname , key_name , nrows , ** kwargs ) data = cls . handle_dataframe_per_file ( data , fname ) if nrows is None : save_df ( data , name ) save_last_mod_time ( modified_time , name ) else : data = load_df ( name , nrows ) if nrows is not None : data = data . iloc [ - nrows : ] return data @classmethod def load_json_file_modified_time ( cls , fname , nrows = None , from_cache = True , ** kwargs ) : name = fname + \"_\" + normalize_name ( cls . __name__ ) modified_time = os . path . getmtime ( os . path . expanduser ( fname )) last_modified = get_last_mod_time ( name ) if modified_time != last_modified or not from_cache : data = just . read ( fname ) data = cls . handle_json ( data , ** kwargs ) data = pd . DataFrame ( data ) if nrows is None : save_df ( data , name ) save_last_mod_time ( modified_time , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows : ] return data @classmethod def load_image_texts ( cls , glob_pattern_s , nrows = None ) : import pytesseract from PIL import Image if isinstance ( glob_pattern_s , list ) : fnames = set () for glob_pattern in glob_pattern_s : fnames.update ( set ( just . glob ( glob_pattern ))) glob_pattern = \"_\" . join ( glob_pattern_s ) else : fnames = set ( just . glob ( glob_pattern )) name = glob_pattern + \"_\" + normalize_name ( cls . __name__ ) processed_files = get_processed_files ( name ) to_process = fnames . difference ( processed_files ) objects = [] cache = get_cache ( \"tesseract\" ) if nrows is not None : if not to_process : return load_df ( name ). iloc [ - nrows : ] else : to_process = list ( to_process )[ - nrows : ] if to_process : for fname in to_process : if fname in cache : text = cache [ fname ] else : try : text = pytesseract . image_to_string ( Image . open ( just . make_path ( fname ))) except OSError as e : print ( \"ERR\" , fname , e ) continue cache [ fname ] = text time = datetime_from_timestamp ( os . path . getmtime ( fname ), \"utc\" ) data = { \"text\" : text , \"path\" : fname , \"title\" : fname . split ( \"/\" )[ - 1 ], \"time\" : time } objects . append ( data ) data = pd . DataFrame ( objects ) if processed_files and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ] : if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) save_processed_files ( fnames | processed_files , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows : ] return data @classmethod def load_dataframe_per_json_file ( cls , glob_pattern , key = \"\" , nrows = None ) : fnames = set ( just . glob ( glob_pattern )) name = glob_pattern + \"_\" + normalize_name ( cls . __name__ ) processed_files = get_processed_files ( name ) to_process = fnames . difference ( processed_files ) objects = [] if nrows is not None : if not to_process : to_process = list ( processed_files )[ - nrows : ] else : to_process = list ( to_process )[ - nrows : ] if to_process : print ( \"processing {} files\" . format ( len ( to_process ))) for fname in to_process : data = read_array_of_dict_from_json ( fname , key , nrows ) data = cls . handle_dataframe_per_file ( data , fname ) if data is None : continue objects . append ( data ) data = pd . concat ( objects ) if processed_files and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ] : if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) save_processed_files ( fnames | processed_files , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows : ] return data @classmethod def load_object_per_newline ( cls , fname , nrows = None ) : \"\"\" Iterates over a file containing an object per line (e.g. .jsonl or .txt). Will only handle new lines not seen earlier; it detects this by storing the number-of-objects seen. You should implement `object_to_row(cls, row)` on your class that returns a dictionary. \"\"\" data = [] name = fname + \"_\" + normalize_name ( cls . __name__ ) newline_count = get_newline_count ( name ) for i , x in enumerate ( just . iread ( fname )) : if nrows is None : if i & lt ; newline_count : continue row = cls . object_to_row ( x ) if row is None : continue data . append ( row ) # breaking at approx 5 rows if nrows is not None and i & gt ; nrows : break if data : data = pd . DataFrame ( data ) if newline_count and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ] : if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) n = i + 1 save_newline_count ( n , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows : ] return data @classmethod def latest_file_is_historic ( cls , glob , key_name = \"\" , nrows = None , from_cache = True ) : \"\"\" Glob is for using a wildcard pattern, and the last created file will be loaded. See `load_data_file_modified_time` for further reference. Returns a pd.DataFrame \"\"\" recent = max ([ x for x in just . glob ( glob ) if \"(\" not in x ], key = os . path . getctime ) return cls . load_data_file_modified_time ( recent , key_name , nrows , from_cache )","title":"Loader"},{"location":"reference/nostalgia/data_loading/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/data_loading/#latest_file_is_historic","text":"def latest_file_is_historic ( glob , key_name = '' , nrows = None , from_cache = True ) Glob is for using a wildcard pattern, and the last created file will be loaded. See load_data_file_modified_time for further reference. Returns a pd.DataFrame View Source @classmethod def latest_file_is_historic(cls, glob, key_name=\"\", nrows=None, from_cache=True): \"\"\" Glob is for using a wildcard pattern, and the last created file will be loaded. See `load_data_file_modified_time` for further reference. Returns a pd.DataFrame \"\"\" recent = max([x for x in just.glob(glob) if \"(\" not in x], key=os.path.getctime) return cls.load_data_file_modified_time(recent, key_name, nrows, from_cache)","title":"latest_file_is_historic"},{"location":"reference/nostalgia/data_loading/#load_data_file_modified_time","text":"def load_data_file_modified_time ( fname , key_name = '' , nrows = None , from_cache = True , ** kwargs ) It will load from cache if filename is not changed since last run (and there is a cache). If it has changed, it will reprocess and save it in cache (including the modified_time). Handles csv, mbox and json currently. key_name is only for json. nrows is for enabling quickly loading a sample. from_cache=False allows ignoring the cache and reprocessing the file. Loading the csv, json or mbox file will yield you a DF IMPORTANT: assumes you implement handle_dataframe_per_file This is the post-processing required after the file is loaded, for e.g. converting time dropping and adding columns. View Source @classmethod def load_data_file_modified_time( cls, fname, key_name=\"\", nrows=None, from_cache=True, **kwargs ): \"\"\" It will load from cache if filename is not changed since last run (and there is a cache). If it has changed, it will reprocess and save it in cache (including the modified_time). Handles csv, mbox and json currently. key_name is only for json. nrows is for enabling quickly loading a sample. from_cache=False allows ignoring the cache and reprocessing the file. Loading the csv, json or mbox file will yield you a DF IMPORTANT: assumes you implement `handle_dataframe_per_file` This is the post-processing required after the file is loaded, for e.g. converting time dropping and adding columns. \"\"\" name = fname + \"_\" + normalize_name(cls.__name__) modified_time = os.path.getmtime(os.path.expanduser(fname)) last_modified = get_last_mod_time(name) if modified_time != last_modified or not from_cache: if fname.endswith(\".csv\"): data = pd.read_csv(fname, error_bad_lines=False, nrows=nrows, **kwargs) elif fname.endswith(\".mbox\"): import mailbox m = mailbox.mbox(fname) data = pd.DataFrame( [{l: x[l] for l in [\"from\", \"to\", \"date\", \"subject\"]} for x in m] ) else: data = read_array_of_dict_from_json(fname, key_name, nrows, **kwargs) data = cls.handle_dataframe_per_file(data, fname) if nrows is None: save_df(data, name) save_last_mod_time(modified_time, name) else: data = load_df(name, nrows) if nrows is not None: data = data.iloc[-nrows:] return data","title":"load_data_file_modified_time"},{"location":"reference/nostalgia/data_loading/#load_dataframe_per_json_file","text":"def load_dataframe_per_json_file ( glob_pattern , key = '' , nrows = None ) View Source @classmethod def load_dataframe_per_json_file(cls, glob_pattern, key=\"\", nrows=None): fnames = set(just.glob(glob_pattern)) name = glob_pattern + \"_\" + normalize_name(cls.__name__) processed_files = get_processed_files(name) to_process = fnames.difference(processed_files) objects = [] if nrows is not None: if not to_process: to_process = list(processed_files)[-nrows:] else: to_process = list(to_process)[-nrows:] if to_process: print(\"processing {} files\".format(len(to_process))) for fname in to_process: data = read_array_of_dict_from_json(fname, key, nrows) data = cls.handle_dataframe_per_file(data, fname) if data is None: continue objects.append(data) data = pd.concat(objects) if processed_files and nrows is None: data = pd.concat((data, load_df(name))) for x in [\"time\", \"start\", \"end\"]: if x in data: data = data.sort_values(x) break if nrows is None: save_df(data, name) save_processed_files(fnames | processed_files, name) else: data = load_df(name) if nrows is not None: data = data.iloc[-nrows:] return data","title":"load_dataframe_per_json_file"},{"location":"reference/nostalgia/data_loading/#load_image_texts","text":"def load_image_texts ( glob_pattern_s , nrows = None ) View Source @classmethod def load_image_texts ( cls , glob_pattern_s , nrows = None ): import pytesseract from PIL import Image if isinstance ( glob_pattern_s , list ): fnames = set () for glob_pattern in glob_pattern_s : fnames . update ( set ( just . glob ( glob_pattern ))) glob_pattern = \"_\" . join ( glob_pattern_s ) else : fnames = set ( just . glob ( glob_pattern )) name = glob_pattern + \"_\" + normalize_name ( cls . __name__ ) processed_files = get_processed_files ( name ) to_process = fnames . difference ( processed_files ) objects = [] cache = get_cache ( \"tesseract\" ) if nrows is not None : if not to_process : return load_df ( name ) . iloc [ - nrows :] else : to_process = list ( to_process )[ - nrows :] if to_process : for fname in to_process : if fname in cache : text = cache [ fname ] else : try : text = pytesseract . image_to_string ( Image . open ( just . make_path ( fname ))) except OSError as e : print ( \"ERR\" , fname , e ) continue cache [ fname ] = text time = datetime_from_timestamp ( os . path . getmtime ( fname ), \"utc\" ) data = { \"text\" : text , \"path\" : fname , \"title\" : fname . split ( \"/\" )[ - 1 ], \"time\" : time } objects . append ( data ) data = pd . DataFrame ( objects ) if processed_files and nrows is None : data = pd . concat (( data , load_df ( name ))) for x in [ \"time\" , \"start\" , \"end\" ]: if x in data : data = data . sort_values ( x ) break if nrows is None : save_df ( data , name ) save_processed_files ( fnames | processed_files , name ) else : data = load_df ( name ) if nrows is not None : data = data . iloc [ - nrows :] return data","title":"load_image_texts"},{"location":"reference/nostalgia/data_loading/#load_json_file_modified_time","text":"def load_json_file_modified_time ( fname , nrows = None , from_cache = True , ** kwargs ) View Source @classmethod def load_json_file_modified_time(cls, fname, nrows=None, from_cache=True, **kwargs): name = fname + \"_\" + normalize_name(cls.__name__) modified_time = os.path.getmtime(os.path.expanduser(fname)) last_modified = get_last_mod_time(name) if modified_time != last_modified or not from_cache: data = just.read(fname) data = cls.handle_json(data, **kwargs) data = pd.DataFrame(data) if nrows is None: save_df(data, name) save_last_mod_time(modified_time, name) else: data = load_df(name) if nrows is not None: data = data.iloc[-nrows:] return data","title":"load_json_file_modified_time"},{"location":"reference/nostalgia/data_loading/#load_object_per_newline","text":"def load_object_per_newline ( fname , nrows = None ) Iterates over a file containing an object per line (e.g. .jsonl or .txt). Will only handle new lines not seen earlier; it detects this by storing the number-of-objects seen. You should implement object_to_row(cls, row) on your class that returns a dictionary. View Source @classmethod def load_object_per_newline(cls, fname, nrows=None): \"\"\" Iterates over a file containing an object per line (e.g. .jsonl or .txt). Will only handle new lines not seen earlier; it detects this by storing the number-of-objects seen. You should implement `object_to_row(cls, row)` on your class that returns a dictionary. \"\"\" data = [] name = fname + \"_\" + normalize_name(cls.__name__) newline_count = get_newline_count(name) for i, x in enumerate(just.iread(fname)): if nrows is None: if i &lt; newline_count: continue row = cls.object_to_row(x) if row is None: continue data.append(row) # breaking at approx 5 rows if nrows is not None and i &gt; nrows: break if data: data = pd.DataFrame(data) if newline_count and nrows is None: data = pd.concat((data, load_df(name))) for x in [\"time\", \"start\", \"end\"]: if x in data: data = data.sort_values(x) break if nrows is None: save_df(data, name) n = i + 1 save_newline_count(n, name) else: data = load_df(name) if nrows is not None: data = data.iloc[-nrows:] return data","title":"load_object_per_newline"},{"location":"reference/nostalgia/extracter/","text":"Module nostalgia.extracter View Source import os import just import zipfile def load_from_download ( ingest_glob , vendor , recent_only = True , delete_existing = True ): ingest_files = just . glob ( ingest_glob ) if not ingest_files : raise ValueError ( f \"Nothing to extract using {ingest_glob} - Aborting\" ) nostalgia_input = \"~/nostalgia_data/input/{}\" . format ( vendor ) if delete_existing : just . remove ( nostalgia_input , allow_recursive = True ) elif just . exists ( nostalgia_input ): raise ValueError ( f \"Cannot overwrite path {nostalgia_input}, pass delete_existing=True\" ) fnames = sorted ( ingest_files , key = os . path . getctime ) if recent_only : fnames = fnames [ - 1 :] for fname in fnames : with zipfile . ZipFile ( fname , 'r' ) as zip_ref : out = os . path . expanduser ( nostalgia_input ) print ( \"unpacking from\" , fname , \"to\" , out ) zip_ref . extractall ( out ) Functions load_from_download def load_from_download ( ingest_glob , vendor , recent_only = True , delete_existing = True ) View Source def load_from_download(ingest_glob, vendor, recent_only=True, delete_existing=True): ingest_files = just.glob(ingest_glob) if not ingest_files: raise ValueError(f\"Nothing to extract using {ingest_glob} - Aborting\") nostalgia_input = \"~/nostalgia_data/input/{}\".format(vendor) if delete_existing: just.remove(nostalgia_input, allow_recursive=True) elif just.exists(nostalgia_input): raise ValueError(f\"Cannot overwrite path {nostalgia_input}, pass delete_existing=True\") fnames = sorted(ingest_files, key=os.path.getctime) if recent_only: fnames = fnames[-1:] for fname in fnames: with zipfile.ZipFile(fname, 'r') as zip_ref: out = os.path.expanduser(nostalgia_input) print(\"unpacking from\", fname, \"to\", out) zip_ref.extractall(out)","title":"Extracter"},{"location":"reference/nostalgia/extracter/#module-nostalgiaextracter","text":"View Source import os import just import zipfile def load_from_download ( ingest_glob , vendor , recent_only = True , delete_existing = True ): ingest_files = just . glob ( ingest_glob ) if not ingest_files : raise ValueError ( f \"Nothing to extract using {ingest_glob} - Aborting\" ) nostalgia_input = \"~/nostalgia_data/input/{}\" . format ( vendor ) if delete_existing : just . remove ( nostalgia_input , allow_recursive = True ) elif just . exists ( nostalgia_input ): raise ValueError ( f \"Cannot overwrite path {nostalgia_input}, pass delete_existing=True\" ) fnames = sorted ( ingest_files , key = os . path . getctime ) if recent_only : fnames = fnames [ - 1 :] for fname in fnames : with zipfile . ZipFile ( fname , 'r' ) as zip_ref : out = os . path . expanduser ( nostalgia_input ) print ( \"unpacking from\" , fname , \"to\" , out ) zip_ref . extractall ( out )","title":"Module nostalgia.extracter"},{"location":"reference/nostalgia/extracter/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/extracter/#load_from_download","text":"def load_from_download ( ingest_glob , vendor , recent_only = True , delete_existing = True ) View Source def load_from_download(ingest_glob, vendor, recent_only=True, delete_existing=True): ingest_files = just.glob(ingest_glob) if not ingest_files: raise ValueError(f\"Nothing to extract using {ingest_glob} - Aborting\") nostalgia_input = \"~/nostalgia_data/input/{}\".format(vendor) if delete_existing: just.remove(nostalgia_input, allow_recursive=True) elif just.exists(nostalgia_input): raise ValueError(f\"Cannot overwrite path {nostalgia_input}, pass delete_existing=True\") fnames = sorted(ingest_files, key=os.path.getctime) if recent_only: fnames = fnames[-1:] for fname in fnames: with zipfile.ZipFile(fname, 'r') as zip_ref: out = os.path.expanduser(nostalgia_input) print(\"unpacking from\", fname, \"to\", out) zip_ref.extractall(out)","title":"load_from_download"},{"location":"reference/nostalgia/file_caching/","text":"Module nostalgia.file_caching View Source import re import just import os import pandas as pd def slugify ( name ): return re . sub ( r '[\\W_]+' , '-' , name ) def file_modified_since_last ( fname , name ): path = just . make_path ( \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" ) last_run_mt = float ( just . read ( path , no_exist = 0 )) modified_time = os . path . getmtime ( fname ) if last_run_mt != modified_time : return modified_time else : return None def get_last_mod_time ( name ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return just . read ( path , no_exist = 0.0 ) def save_last_mod_time ( last_mod_time , name ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return just . write ( last_mod_time , path ) def get_last_latest_file ( name ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return just . read ( path , no_exist = 0 ) def save_last_latest_file ( latest_file , name ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return just . write ( latest_file , path ) def get_newline_count ( name ): \"\"\" counts by row numbers in a file \"\"\" path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return just . read ( path , no_exist = 0 ) def save_newline_count ( n , name ): \"\"\" counts by row numbers in a file \"\"\" path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return just . write ( n , path ) def get_processed_files ( name ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return set ( just . read ( path , no_exist = [])) def save_processed_files ( fnames , name ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" just . write ( fnames , path ) def check_seen ( name , value ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" is_new = True res = just . read ( path , no_exist = False ) if res : if isinstance ( value , tuple ): value = list ( value ) is_new = res != value if is_new : just . write ( value , path ) return is_new def make_path ( name ): dir_name = os . path . expanduser ( \"~/nostalgia_data/dfs/\" ) just . mkdir ( dir_name , 0x700 ) return dir_name + slugify ( name ) def save_df ( df , name ): path = make_path ( name ) + \".parquet\" df = df . reset_index ( drop = True ) try : df . to_parquet ( path , compression = \"zstd\" , allow_truncated_timestamps = True ) except Exception as e : print ( \"ERROR with\" , name ) for x in df . columns : print ( x , sum ([ str ( y )[: 1 ] == \"{\" for y in df [ x ]])) raise e def load_df ( name , nrows = None ): path = make_path ( name ) + \".parquet\" data = pd . read_parquet ( path ) if nrows is not None : data = data . iloc [ - nrows :] return data Functions check_seen def check_seen ( name , value ) View Source def check_seen(name, value): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" is_new = True res = just.read(path, no_exist=False) if res: if isinstance(value, tuple): value = list(value) is_new = res != value if is_new: just.write(value, path) return is_new file_modified_since_last def file_modified_since_last ( fname , name ) View Source def file_modified_since_last(fname, name): path = just.make_path(\"~/nostalgia_data/seen/\" + slugify(name) + \".json\") last_run_mt = float(just.read(path, no_exist=0)) modified_time = os.path.getmtime(fname) if last_run_mt != modified_time: return modified_time else: return None get_last_latest_file def get_last_latest_file ( name ) View Source def get_last_latest_file(name): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return just.read(path, no_exist=0) get_last_mod_time def get_last_mod_time ( name ) View Source def get_last_mod_time(name): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return just.read(path, no_exist=0.0) get_newline_count def get_newline_count ( name ) counts by row numbers in a file View Source def get_newline_count(name): \"\"\" counts by row numbers in a file \"\"\" path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return just.read(path, no_exist=0) get_processed_files def get_processed_files ( name ) View Source def get_processed_files(name): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return set(just.read(path, no_exist=[])) load_df def load_df ( name , nrows = None ) View Source def load_df(name, nrows=None): path = make_path(name) + \".parquet\" data = pd.read_parquet(path) if nrows is not None: data = data.iloc[-nrows:] return data make_path def make_path ( name ) View Source def make_path(name): dir_name = os.path.expanduser(\"~/nostalgia_data/dfs/\") just.mkdir(dir_name, 0x700) return dir_name + slugify(name) save_df def save_df ( df , name ) View Source def save_df(df, name): path = make_path(name) + \".parquet\" df = df.reset_index(drop=True) try: df.to_parquet(path, compression=\"zstd\", allow_truncated_timestamps=True) except Exception as e: print(\"ERROR with\", name) for x in df.columns: print(x, sum([str(y)[:1] == \"{\" for y in df[x]])) raise e save_last_latest_file def save_last_latest_file ( latest_file , name ) View Source def save_last_latest_file(latest_file, name): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return just.write(latest_file, path) save_last_mod_time def save_last_mod_time ( last_mod_time , name ) View Source def save_last_mod_time(last_mod_time, name): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return just.write(last_mod_time, path) save_newline_count def save_newline_count ( n , name ) counts by row numbers in a file View Source def save_newline_count(n, name): \"\"\" counts by row numbers in a file \"\"\" path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return just.write(n, path) save_processed_files def save_processed_files ( fnames , name ) View Source def save_processed_files(fnames, name): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" just.write(fnames, path) slugify def slugify ( name ) View Source def slugify(name): return re.sub(r'[\\W_]+', '-', name)","title":"File Caching"},{"location":"reference/nostalgia/file_caching/#module-nostalgiafile_caching","text":"View Source import re import just import os import pandas as pd def slugify ( name ): return re . sub ( r '[\\W_]+' , '-' , name ) def file_modified_since_last ( fname , name ): path = just . make_path ( \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" ) last_run_mt = float ( just . read ( path , no_exist = 0 )) modified_time = os . path . getmtime ( fname ) if last_run_mt != modified_time : return modified_time else : return None def get_last_mod_time ( name ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return just . read ( path , no_exist = 0.0 ) def save_last_mod_time ( last_mod_time , name ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return just . write ( last_mod_time , path ) def get_last_latest_file ( name ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return just . read ( path , no_exist = 0 ) def save_last_latest_file ( latest_file , name ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return just . write ( latest_file , path ) def get_newline_count ( name ): \"\"\" counts by row numbers in a file \"\"\" path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return just . read ( path , no_exist = 0 ) def save_newline_count ( n , name ): \"\"\" counts by row numbers in a file \"\"\" path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return just . write ( n , path ) def get_processed_files ( name ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" return set ( just . read ( path , no_exist = [])) def save_processed_files ( fnames , name ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" just . write ( fnames , path ) def check_seen ( name , value ): path = \"~/nostalgia_data/seen/\" + slugify ( name ) + \".json\" is_new = True res = just . read ( path , no_exist = False ) if res : if isinstance ( value , tuple ): value = list ( value ) is_new = res != value if is_new : just . write ( value , path ) return is_new def make_path ( name ): dir_name = os . path . expanduser ( \"~/nostalgia_data/dfs/\" ) just . mkdir ( dir_name , 0x700 ) return dir_name + slugify ( name ) def save_df ( df , name ): path = make_path ( name ) + \".parquet\" df = df . reset_index ( drop = True ) try : df . to_parquet ( path , compression = \"zstd\" , allow_truncated_timestamps = True ) except Exception as e : print ( \"ERROR with\" , name ) for x in df . columns : print ( x , sum ([ str ( y )[: 1 ] == \"{\" for y in df [ x ]])) raise e def load_df ( name , nrows = None ): path = make_path ( name ) + \".parquet\" data = pd . read_parquet ( path ) if nrows is not None : data = data . iloc [ - nrows :] return data","title":"Module nostalgia.file_caching"},{"location":"reference/nostalgia/file_caching/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/file_caching/#check_seen","text":"def check_seen ( name , value ) View Source def check_seen(name, value): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" is_new = True res = just.read(path, no_exist=False) if res: if isinstance(value, tuple): value = list(value) is_new = res != value if is_new: just.write(value, path) return is_new","title":"check_seen"},{"location":"reference/nostalgia/file_caching/#file_modified_since_last","text":"def file_modified_since_last ( fname , name ) View Source def file_modified_since_last(fname, name): path = just.make_path(\"~/nostalgia_data/seen/\" + slugify(name) + \".json\") last_run_mt = float(just.read(path, no_exist=0)) modified_time = os.path.getmtime(fname) if last_run_mt != modified_time: return modified_time else: return None","title":"file_modified_since_last"},{"location":"reference/nostalgia/file_caching/#get_last_latest_file","text":"def get_last_latest_file ( name ) View Source def get_last_latest_file(name): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return just.read(path, no_exist=0)","title":"get_last_latest_file"},{"location":"reference/nostalgia/file_caching/#get_last_mod_time","text":"def get_last_mod_time ( name ) View Source def get_last_mod_time(name): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return just.read(path, no_exist=0.0)","title":"get_last_mod_time"},{"location":"reference/nostalgia/file_caching/#get_newline_count","text":"def get_newline_count ( name ) counts by row numbers in a file View Source def get_newline_count(name): \"\"\" counts by row numbers in a file \"\"\" path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return just.read(path, no_exist=0)","title":"get_newline_count"},{"location":"reference/nostalgia/file_caching/#get_processed_files","text":"def get_processed_files ( name ) View Source def get_processed_files(name): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return set(just.read(path, no_exist=[]))","title":"get_processed_files"},{"location":"reference/nostalgia/file_caching/#load_df","text":"def load_df ( name , nrows = None ) View Source def load_df(name, nrows=None): path = make_path(name) + \".parquet\" data = pd.read_parquet(path) if nrows is not None: data = data.iloc[-nrows:] return data","title":"load_df"},{"location":"reference/nostalgia/file_caching/#make_path","text":"def make_path ( name ) View Source def make_path(name): dir_name = os.path.expanduser(\"~/nostalgia_data/dfs/\") just.mkdir(dir_name, 0x700) return dir_name + slugify(name)","title":"make_path"},{"location":"reference/nostalgia/file_caching/#save_df","text":"def save_df ( df , name ) View Source def save_df(df, name): path = make_path(name) + \".parquet\" df = df.reset_index(drop=True) try: df.to_parquet(path, compression=\"zstd\", allow_truncated_timestamps=True) except Exception as e: print(\"ERROR with\", name) for x in df.columns: print(x, sum([str(y)[:1] == \"{\" for y in df[x]])) raise e","title":"save_df"},{"location":"reference/nostalgia/file_caching/#save_last_latest_file","text":"def save_last_latest_file ( latest_file , name ) View Source def save_last_latest_file(latest_file, name): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return just.write(latest_file, path)","title":"save_last_latest_file"},{"location":"reference/nostalgia/file_caching/#save_last_mod_time","text":"def save_last_mod_time ( last_mod_time , name ) View Source def save_last_mod_time(last_mod_time, name): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return just.write(last_mod_time, path)","title":"save_last_mod_time"},{"location":"reference/nostalgia/file_caching/#save_newline_count","text":"def save_newline_count ( n , name ) counts by row numbers in a file View Source def save_newline_count(n, name): \"\"\" counts by row numbers in a file \"\"\" path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" return just.write(n, path)","title":"save_newline_count"},{"location":"reference/nostalgia/file_caching/#save_processed_files","text":"def save_processed_files ( fnames , name ) View Source def save_processed_files(fnames, name): path = \"~/nostalgia_data/seen/\" + slugify(name) + \".json\" just.write(fnames, path)","title":"save_processed_files"},{"location":"reference/nostalgia/file_caching/#slugify","text":"def slugify ( name ) View Source def slugify(name): return re.sub(r'[\\W_]+', '-', name)","title":"slugify"},{"location":"reference/nostalgia/ndf/","text":"Module nostalgia.ndf View Source import os import re import pandas as pd import numpy as np from nostalgia.times import now , yesterday , last_week , last_month , last_year , parse_date_tz from metadate import is_mp from datetime import timedelta import just from nostalgia.nlp import nlp_registry , nlp , n , COLUMN_BLACKLIST , ResultInfo from nostalgia.utils import get_token_set , normalize_name , view from nostalgia.extracter import load_from_download from nostalgia.times import datetime_from_timestamp from nostalgia.cache import get_cache from nostalgia.data_loading import Loader def ab_overlap_cd ( a , b , c , d ): return (( a & lt ; d ) & amp ; ( b & gt ; c )) | (( b & gt ; c ) & amp ; ( d & gt ; a )) def ab_overlap_c ( a , b , c ): return ( a & lt ; = c ) & amp ; ( c & lt ; = b ) def join_time_naive ( locs , df , ** window_kwargs ): tmp = [] if not locs . inferred_time : locs . infer_time () for _ , row in locs . iterrows (): if df . start is not None : if locs . _start_col is None : start = row [ locs . _time_col ] - pd . Timedelta ( ** window_kwargs ) end = row [ locs . _time_col ] + pd . Timedelta ( ** window_kwargs ) else : start = row [ locs . _start_col ] end = row [ locs . _end_col ] res = df [ ab_overlap_cd ( df . start , df . end , start , end )] else : if locs . _start_col is None : start = row [ locs . _time_col ] - pd . Timedelta ( ** window_kwargs ) end = row [ locs . _time_col ] + pd . Timedelta ( ** window_kwargs ) else : start , end = row . start , row . end res = df [ df . time . between ( start , end )] if not res . empty : # import pdb # pdb.set_trace() tmp . append ( res ) if not tmp : return df [: 0 ] tmp = pd . concat ([ pd . DataFrame ( x ) for x in tmp ]) try : tmp = tmp . drop_duplicates () except TypeError : tmp = tmp [ ~ tmp . astype ( str ) . duplicated ()] return tmp def join_time_index ( locs , df , ** window_kwargs ): try : return df [ df . time . apply ( lambda x : locs . index . contains ( x ))] except TypeError : print ( \"warning\" ) return join_time_naive ( locs , df , ** window_kwargs ) def join_time ( locs , df , ** window_kwargs ): if df . start is not None : return join_time_naive ( locs , df , ** window_kwargs ) if not str ( df . index . dtype ) . startswith ( \"interval\" ): return join_time_naive ( locs , df , ** window_kwargs ) W = locs . shape [ 0 ] L = df . shape [ 0 ] y = 0.001 + W * - 0.318 + L * 0.013 if y & gt ; 300 : p = 1 else : exponent = np . exp ( y ) p = exponent / ( 1 + exponent ) if p & gt ; 0.5 : fn = join_time_naive else : fn = join_time_index return fn ( locs , df , ** window_kwargs ) registry = {} def get_type_from_registry ( tp ): for key , value in registry . items (): if key . endswith ( tp ): return value def time ( x ): return x . time def col_contains_wrapper ( word , col ): def col_contains ( x ): return x . col_contains ( word , col ) return col_contains class NDF : keywords = [] nlp_columns = [] nlp_when = True selected_columns = [] vendor = None def __init__ ( self , data ): super () . __init__ ( data ) C = self . __class__ self . num_times = None self . _start_col , self . _time_col , self . _end_col = None , None , None self . inferred_time = False # IF BIG BREAK, THEN THIS IS HERE # if self.df_name not in registry: # registry[self.df_name] = self if self . df_name != \"results\" : if self . df_name not in registry : registry [ self . df_name ] = self # if the new data is smaller than what is in the registry, do not overwrite the registry elif data . shape [ 0 ] & gt ; registry [ self . df_name ] . shape [ 0 ]: registry [ self . df_name ] = self if n is None : return seen_keyword_keywords = set () for kw in self . keywords : for w in n . simple_extend ( kw ): if w and isinstance ( w , str ): nlp_registry [ w ] . add ( ResultInfo ( C , \"start\" , orig_word = kw )) seen_keyword_keywords . add ( w ) nlp_registry [ kw ] . add ( ResultInfo ( C , \"start\" , orig_word = kw )) seen_keyword_keywords . add ( kw ) # nlp_keywords_column = [] # if \"keywords\" in data: # nlp_keywords_column = [\"keywords\"] BLACKLIST = seen_keyword_keywords . union ( COLUMN_BLACKLIST ) for col in self . nlp_columns : words = get_token_set ( self [ col ] . unique ()) for word in words : if word in BLACKLIST : continue for w in n . simple_extend ( word ): if w in BLACKLIST : continue if w and isinstance ( w , str ): nlp_registry [ w ] . add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) if n . is_verb ( word ): for w in n . get_verbs ( word ) . values (): if w in BLACKLIST : continue if w in words or not isinstance ( w , str ): continue nlp_registry [ w ] . add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) if n . is_verb ( word ): for w in n . get_verbs ( word ) . values (): if w in BLACKLIST : continue if w in words or not isinstance ( w , str ): continue nlp_registry [ w ] . add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) nlp_registry [ word ] . add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) if self . nlp_when : nlp_registry [ \"when\" ] . add ( ResultInfo ( C , \"end\" , time , orig_word = \"when\" )) @classmethod def ingest ( cls ): load_from_download ( vendor = cls . vendor , ** cls . ingest_settings ) @property def df_name ( self ): name = normalize_name ( self . __class__ . __name__ ) if self . vendor is not None and not name . startswith ( self . vendor ): name = self . vendor + \"_\" + name return name @classmethod def df_label ( cls ): return normalize_name ( cls . __name__ ) . replace ( \"_\" , \" \" ) . title () @property def time ( self ): if not self . inferred_time : self . infer_time () self . inferred_time = True if self . _time_col == \"time\" : return self . _get_item_cache ( self . _time_col ) if self . _time_col not in self : return None return getattr ( self , self . _time_col ) # @property # def _constructor(self): # return self.__class__ def duration_longer_than ( self , ** timedelta_kwargs ): return self [( self . end - self . time ) & gt ; = timedelta ( ** timedelta_kwargs )] def duration_shorter_than ( self , ** timedelta_kwargs ): return self [( self . end - self . time ) & lt ; = timedelta ( ** timedelta_kwargs )] def take_from ( self , registry_ending , col_name ): for registry_type in registry : if not registry_type . endswith ( registry_ending ): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self . columns : return self [ new_name ] tp = get_type_from_registry ( registry_type ) results = [] if not self . inferred_time : self . infer_time () for x in self [ self . _time_col ]: try : res = tp . loc [ x ] if not isinstance ( res , pd . Series ): res = res . iloc [ 0 ] res = res [ col_name ] except ( KeyError , TypeError ): res = np . nan results . append ( res ) self [ new_name ] = results return self [ new_name ] def add_heartrate ( self ): return self . take_from ( \"heartrate\" , \"value\" ) def heartrate_range ( self , low , high = None ): if \"heartrate_value\" not in self . columns : self . add_heartrate () if high is not None and low is not None : return self [( self [ \"heartrate_value\" ] & gt ; = low ) & amp ; self [ \"heartrate_value\" ] & lt ; high ] if low is not None : return self [ self [ \"heartrate_value\" ] & gt ; = low ] if high is not None : return self [ self [ \"heartrate_value\" ] & lt ; high ] def heartrate_above ( self , value ): return self . heartrate_range ( value ) def heartrate_below ( self , value ): return self . heartrate_range ( None , value ) @classmethod def get_schema ( cls , * args , ** kwargs ): sample = cls . load ( * args , nrows = 5 , ** kwargs ) return { k : v for k , v in zip ( sample . columns , sample . dtypes )} @property def start ( self ): if not self . inferred_time : self . infer_time () self . inferred_time = True if self . _start_col == \"start\" : return self . _get_item_cache ( self . _start_col ) if self . _start_col not in self : return None return getattr ( self , self . _start_col ) @property def _office_days ( self ): return self . time . dt . weekday & lt ; 5 @property def _office_hours ( self ): if ( self . time . dt . hour == 0 ) . all (): raise ValueError ( \"Hours are not set, thus unreliable. Use `office_days` instead?\" ) if self . start is not None : return np . array ( [ ( x & gt ; = 8 and x & lt ; = 17 and y & gt ; = 8 and y & lt ; = 17 ) for x , y in zip ( self . start . dt . hour , self . end . dt . hour ) ] ) return np . array ([( x & gt ; = 8 and x & lt ; = 17 ) for x in self . time . dt . hour ]) @property def in_office_days ( self ): return self [ self . _office_days ] @property def in_office_hours ( self ): return self [ self . _office_hours ] @property def during_office_hours ( self ): return self [ self . _office_hours & amp ; self . _office_days ] @property def outside_office_hours ( self ): return self [ ~ ( self . _office_hours & amp ; self . _office_days )] @property def end ( self ): if not self . inferred_time : self . infer_time () self . inferred_time = True if self . _end_col == \"end\" : return self . _get_item_cache ( self . _end_col ) if self . _end_col not in self : return None return getattr ( self , self . _end_col ) def time_level ( self , col ): if ( col . dt . microsecond != 0 ) . any (): return 4 if ( col . dt . second != 0 ) . any (): return 3 if ( col . dt . minute != 0 ) . any (): return 2 if ( col . dt . hour != 0 ) . any (): return 1 return 0 def infer_time ( self ): if self . __class__ . __name__ == \"Results\" : self . _start_col , self . _time_col , self . _end_col = \"start\" , \"start\" , \"end\" return times = [ x for x , y in zip ( self . columns , self . dtypes ) if \"datetime\" in str ( y )] levels = [ self . time_level ( self [ x ]) for x in times ] if not levels : raise ValueError ( f \"No datetime found in {self.__class__.__name__}\" ) max_level = max ( levels ) # workaround # start: 10:00:00 # end: 10:00:59 times = [ t for t , l in zip ( times , levels ) if l == max_level or ( l == 2 and max_level == 3 )] num_times = len ( times ) self . num_times = num_times if num_times == 0 : self . _start_col , self . _time_col , self . _end_col = None , None , None elif num_times == 1 : self . _start_col , self . _time_col , self . _end_col = None , times [ 0 ], None elif num_times == 2 : col1 , col2 = times sub = self [ self [ col1 ] . notnull () & amp ; self [ col2 ] . notnull ()] a , b = sub [ col1 ], sub [ col2 ] if ( a & gt ; b ) . all (): col1 , col2 = col2 , col1 elif not ( a & lt ; = b ) . all (): raise ValueError ( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\" : col2 , col1 = col1 , col2 self . _start_col , self . _time_col , self . _end_col = col1 , col1 , col2 else : msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception ( msg + \" Found: \" + str ( times )) def when_at ( self , other , ** window_kwargs ): if isinstance ( other , str ): other = get_type_from_registry ( \"places\" ) . containing ( other ) return self . __class__ ( join_time ( other , self , ** window_kwargs )) def browsing ( self , other , ** window_kwargs ): if isinstance ( other , str ): other = get_type_from_registry ( \"browser\" ) . containing ( other ) return self . __class__ ( join_time ( other , self , ** window_kwargs )) def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None : return self [( self . start . dt . hour & gt ; start ) | ( self . end . dt . hour & lt ; end )] return self [( self . time . dt . hour & gt ; start ) | ( self . time . dt . hour & lt ; end )] @property def when_asleep ( self ): return self . __class__ ( join_time ( registry [ \"sleep\" ] . asleep , self )) # browser[\"session\"] = browser.time.diff().apply(lambda x: x.seconds &gt; 20 * 60).cumsum()+10 # session, start, end = zip(*[(name, group.iloc[0].time - pd.Timedelta(minutes=10), group.iloc[-1].time + pd.Timedelta(minutes=10)) for name, group in browser.groupby(\"session\")]) # df = pd.DataFrame({\"session\": session}, index=pd.IntervalIndex.from_arrays(start, end)) # df.index.get_loc(pd.Timestamp(parse_date_tz(\"2019-09-18 18:40:56.729633\").start_date)) def col_contains ( self , string , col_name , case = False , regex = False , na = False ): return self [ self [ col_name ] . str . contains ( string , case = case , regex = regex , na = na )] def __getitem__ ( self , key ): new = super () . __getitem__ ( key ) if isinstance ( new , pd . DataFrame ): new = self . __class__ ( new ) return new @property def text_cols ( self ): return [ x for x , t in zip ( self . columns , self . dtypes ) if t == np . dtype ( 'O' )] @nlp ( \"filter\" , \"by me\" , \"i\" , \"my\" ) def by_me ( self ): return self def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound : string = r \"\\b\" + string + r \"\\b\" if col_name is not None : return self . col_contains ( string , col_name , case , regex , na ) bool_cols = [ self [ x ] . str . contains ( string , case = case , regex = regex , na = na ) for x in self . text_cols ] bool_array = bool_cols [ 0 ] for b in bool_cols [ 1 :]: bool_array = np . logical_or ( bool_array , b ) return self . __class__ ( self [ bool_array ]) def query ( self , expr ): return self . __class__ ( super () . query ( expr )) def as_simple ( self , max_n = None ): data = { \"title\" : self . df_name , # default, to be overwritten \"url\" : None , \"start\" : None , \"end\" : None , # \"body\": None, \"type\" : self . df_name , \"interval\" : True , \"sender\" : None , \"value\" : getattr ( self , \"value\" , None ), \"index_loc\" : self . index , } for x in [ \"title\" , \"name\" , \"naam\" , \"subject\" , \"url\" , \"content\" , \"text\" , \"value\" ]: res = getattr ( self , x , None ) if res is not None : data [ \"title\" ] = res break res = getattr ( self , \"sender\" , None ) if res is not None : data [ \"sender\" ] = res for x in [ \"url\" , \"path\" , \"file\" ]: res = getattr ( self , x , None ) if res is not None : data [ \"url\" ] = res break for x in [ \"start\" , \"time\" , \"timestamp\" ]: res = getattr ( self , x , None ) if res is not None : data [ \"start\" ] = res break for x in [ \"end\" ]: res = getattr ( self , x , None ) if res is not None : data [ \"end\" ] = res - pd . Timedelta ( microseconds = 1 ) break if data [ \"end\" ] is None : data [ \"end\" ] = data [ \"start\" ] + pd . Timedelta ( minutes = 5 ) data [ \"interval\" ] = False try : data = pd . DataFrame ( data ) . sort_values ( \"start\" ) if max_n is not None : data = data . iloc [ - max_n :] return data except ValueError : raise ValueError ( f \"No fields are mapped for {self.__class__.__name__}\" ) def _select_at_day ( self , day_or_class ): if isinstance ( day_or_class , pd . DataFrame ): days = day_or_class . time . dt . date . unique () return self . time . dt . date . isin ( days ) elif isinstance ( day_or_class , ( list , tuple , set , pd . Series )): return self . time . dt . date . isin ( set ( day_or_class )) else : mp = parse_date_tz ( day_or_class ) return ( self . time . dt . date & gt ; = mp . start_date . date ()) & amp ; ( self . time . dt . date & lt ; mp . end_date . date () ) def at_day ( self , day_or_class ): return self [ self . _select_at_day ( day_or_class )] def not_at_day ( self , day_or_class ): return self [ ~ self . _select_at_day ( day_or_class )] @property def last_day ( self ): return self [( self . time & gt ; yesterday ()) & amp ; ( self . time & lt ; now ())] @property def yesterday ( self ): return self [( self . time & gt ; yesterday ()) & amp ; ( self . time & lt ; now ())] @property def last_week ( self ): return self [( self . time & gt ; last_week ()) & amp ; ( self . time & lt ; now ())] @property def last_month ( self ): return self [( self . time & gt ; last_month ()) & amp ; ( self . time & lt ; now ())] @property def last_year ( self ): return self [( self . time & gt ; last_year ()) & amp ; ( self . time & lt ; now ())] def near ( self , s ): if isinstance ( s , NDF ) and s . df_name . endswith ( \"places\" ): selection = s else : selection = get_type_from_registry ( \"places\" ) . containing ( s ) return self . when_at ( selection ) def to_place ( self ): results = [] places = get_type_from_registry ( \"places\" ) for time in self . time : try : results . append ( places . iloc [ places . index . get_loc ( time )] . iloc [ 0 ]) except ( TypeError , KeyError ): pass return places . __class__ ( results ) def in_a ( self , s ): return self . near ( s ) def read ( self , index ): return just . read ( self . path [ index ]) def view ( self , index ): view ( self . path [ index ]) def head ( self , * args , ** kwargs ): return self . __class__ ( super () . head ( * args , ** kwargs )) def tail ( self , * args , ** kwargs ): return self . __class__ ( super () . tail ( * args , ** kwargs )) @nlp ( \"end\" , \"how many\" , \"how many times\" , \"how often\" ) def count ( self ): return self . shape [ 0 ] @property def at_home ( self ): self . take_from ( \"places\" , \"category\" ) return self [ self [ \"places_category\" ] == \"Home\" ] @property def at_work ( self ): self . take_from ( \"places\" , \"category\" ) return self [ self [ \"places_category\" ] == \"Work\" ] def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ): if is_mp ( start ): start = start . start_date end = start . end_date elif isinstance ( start , str ) and end is None : mp = parse_date_tz ( start ) start = mp . start_date end = mp . end_date elif isinstance ( start , str ) and isinstance ( end , str ): mp = parse_date_tz ( start ) start = mp . start_date mp = parse_date_tz ( end ) end = mp . start_date elif end is None and window_kwargs : end = start elif end is None : raise ValueError ( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self . infer_time () if window_kwargs : start = start - pd . Timedelta ( ** window_kwargs ) end = end + pd . Timedelta ( ** window_kwargs ) if self . _start_col is None : res = self [ ab_overlap_c ( start , end , self [ self . _time_col ])] else : res = self [ ab_overlap_cd ( self [ self . _start_col ], self [ self . _end_col ], start , end )] if not res . empty and sort_diff : # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res [ \"sort_score\" ] = res [ self . _time_col ] res = res . sort_values ( 'sort_score' ) . drop ( 'sort_score' , axis = 1 ) return self . __class__ ( res ) when = when_at @property def duration ( self ): return self . end - self . start def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ): return self . __class__ ( pd . DataFrame . sort_values ( self , by , axis , ascending , inplace , kind , na_position ) ) @nlp ( \"filter\" , \"last\" , \"last time\" , \"most recently\" ) def last ( self ): _ = self . time # to get inferred time if not set col = self . _time_col or self . _start_col return self . __class__ ( self . sort_values ( col , na_position = \"last\" , ascending = False ) . iloc [: 1 ]) # maybe which/what could be showing only unique? @nlp ( \"end\" , \"show\" , \"show me\" , \"show me the\" , \"show the\" , \"what\" ) def show_me ( self ): _ = self . time # to get inferred time if not set col = self . _time_col or self . _start_col return self . __class__ ( self . sort_values ( col , na_position = \"last\" , ascending = False )) def at ( self , time_or_place ): if isinstance ( time_or_place , NDF ) and time_or_place . df_name . endswith ( \"places\" ): return self . when_at ( time_or_place ) if isinstance ( time_or_place , str ): mp = parse_date_tz ( time_or_place ) if mp : start = mp . start_date end = mp . end_date return self . at_time ( start , end ) else : return self . when_at ( get_type_from_registry ( \"places\" ) . containing ( time_or_place )) raise ValueError ( \"neither time nor place was passed\" ) def to_html ( self ): if self . selected_columns : data = pd . DataFrame ({ x : getattr ( self , x ) for x in self . selected_columns }) return data . to_html () return super () . to_html () def get_type_from_registry ( self , tp ): for key , value in rergistry . items (): if key . endswith ( tp ): return value def __call_ ( self ): return self # ov = ab_overlap_cd( # places.start, # places.end, # parse(\"2015-02-05 21:53:09.581001+00:00\"), # parse(\"2015-02-05 21:53:13.881000+00:00\"), # ) class Results ( NDF ): @classmethod def merge ( cls , * dfs , max_n = None ): data = pd . concat ([ x . as_simple ( max_n ) for x in dfs ]) data = data . set_index ( \"start\" ) data = data . sort_values ( \"start\" , na_position = \"first\" ) data [ \"start\" ] = data . index return Results ( data ) def get_original ( self , index_loc ): row = self . iloc [ index_loc ] rec = registry [ row . type ] . loc [ row [ \"index_loc\" ]] # rec.add_heartrate() # not here, but at the results level.. but then just to display # if a certain value if not isinstance ( rec , pd . Series ): rec = rec . iloc [ 0 ] return rec Variables COLUMN_BLACKLIST n nlp_registry registry Functions ab_overlap_c def ab_overlap_c ( a , b , c ) View Source def ab_overlap_c(a, b, c): return (a &lt;= c) &amp; (c &lt;= b) ab_overlap_cd def ab_overlap_cd ( a , b , c , d ) View Source def ab_overlap_cd(a, b, c, d): return ((a &lt; d) &amp; (b &gt; c)) | ((b &gt; c) &amp; (d &gt; a)) col_contains_wrapper def col_contains_wrapper ( word , col ) View Source def col_contains_wrapper(word, col): def col_contains(x): return x.col_contains(word, col) return col_contains get_type_from_registry def get_type_from_registry ( tp ) View Source def get_type_from_registry(tp): for key, value in registry.items(): if key.endswith(tp): return value join_time def join_time ( locs , df , ** window_kwargs ) View Source def join_time(locs, df, **window_kwargs): if df.start is not None: return join_time_naive(locs, df, **window_kwargs) if not str(df.index.dtype).startswith(\"interval\"): return join_time_naive(locs, df, **window_kwargs) W = locs.shape[0] L = df.shape[0] y = 0.001 + W * -0.318 + L * 0.013 if y &gt; 300: p = 1 else: exponent = np.exp(y) p = exponent / (1 + exponent) if p &gt; 0.5: fn = join_time_naive else: fn = join_time_index return fn(locs, df, **window_kwargs) join_time_index def join_time_index ( locs , df , ** window_kwargs ) View Source def join_time_index(locs, df, **window_kwargs): try: return df[df.time.apply(lambda x: locs.index.contains(x))] except TypeError: print(\"warning\") return join_time_naive(locs, df, **window_kwargs) join_time_naive def join_time_naive ( locs , df , ** window_kwargs ) View Source def join_time_naive ( locs , df , ** window_kwargs ): tmp = [] if not locs . inferred_time : locs . infer_time () for _ , row in locs . iterrows (): if df . start is not None : if locs . _start_col is None : start = row [ locs . _time_col ] - pd . Timedelta ( ** window_kwargs ) end = row [ locs . _time_col ] + pd . Timedelta ( ** window_kwargs ) else : start = row [ locs . _start_col ] end = row [ locs . _end_col ] res = df [ ab_overlap_cd ( df . start , df . end , start , end )] else : if locs . _start_col is None : start = row [ locs . _time_col ] - pd . Timedelta ( ** window_kwargs ) end = row [ locs . _time_col ] + pd . Timedelta ( ** window_kwargs ) else : start , end = row . start , row . end res = df [ df . time . between ( start , end )] if not res . empty : # import pdb # pdb.set_trace() tmp . append ( res ) if not tmp : return df [: 0 ] tmp = pd . concat ([ pd . DataFrame ( x ) for x in tmp ]) try : tmp = tmp . drop_duplicates () except TypeError : tmp = tmp [ ~ tmp . astype ( str ) . duplicated ()] return tmp time def time ( x ) View Source def time(x): return x.time Classes NDF class NDF ( data ) View Source class NDF: keywords = [] nlp_columns = [] nlp_when = True selected_columns = [] vendor = None def __init__ ( self , data ): super (). __init__ ( data ) C = self . __class__ self . num_times = None self . _start_col , self . _time_col , self . _end_col = None , None , None self . inferred_time = False # IF BIG BREAK, THEN THIS IS HERE # if self.df_name not in registry: # registry[self.df_name] = self if self . df_name != \"results\" : if self . df_name not in registry: registry [ self . df_name ] = self # if the new data is smaller than what is in the registry, do not overwrite the registry elif data . shape [ 0 ] &gt ; registry [ self . df_name ]. shape [ 0 ]: registry [ self . df_name ] = self if n is None: return seen_keyword_keywords = set () for kw in self . keywords: for w in n . simple_extend ( kw ): if w and isinstance ( w , str ): nlp_registry [ w ]. add ( ResultInfo ( C , \"start\" , orig_word = kw )) seen_keyword_keywords . add ( w ) nlp_registry [ kw ]. add ( ResultInfo ( C , \"start\" , orig_word = kw )) seen_keyword_keywords . add ( kw ) # nlp_keywords_column = [] # if \"keywords\" in data: # nlp_keywords_column = [\"keywords\"] BLACKLIST = seen_keyword_keywords . union ( COLUMN_BLACKLIST ) for col in self . nlp_columns: words = get_token_set ( self [ col ]. unique ()) for word in words: if word in BLACKLIST: continue for w in n . simple_extend ( word ): if w in BLACKLIST: continue if w and isinstance ( w , str ): nlp_registry [ w ]. add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) if n . is_verb ( word ): for w in n . get_verbs ( word ). values (): if w in BLACKLIST: continue if w in words or not isinstance ( w , str ): continue nlp_registry [ w ]. add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) if n . is_verb ( word ): for w in n . get_verbs ( word ). values (): if w in BLACKLIST: continue if w in words or not isinstance ( w , str ): continue nlp_registry [ w ]. add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) nlp_registry [ word ]. add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) if self . nlp_when: nlp_registry [ \"when\" ]. add ( ResultInfo ( C , \"end\" , time , orig_word = \"when\" )) @classmethod def ingest ( cls ): load_from_download ( vendor = cls . vendor , ** cls . ingest_settings ) @property def df_name ( self ): name = normalize_name ( self . __class__ . __name__ ) if self . vendor is not None and not name . startswith ( self . vendor ): name = self . vendor + \"_\" + name return name @classmethod def df_label ( cls ): return normalize_name ( cls . __name__ ). replace ( \"_\" , \" \" ). title () @property def time ( self ): if not self . inferred_time: self . infer_time () self . inferred_time = True if self . _time_col == \"time\" : return self . _get_item_cache ( self . _time_col ) if self . _time_col not in self: return None return getattr ( self , self . _time_col ) # @property # def _constructor(self): # return self.__class__ def duration_longer_than ( self , ** timedelta_kwargs ): return self [( self . end - self . time ) &gt ;= timedelta (** timedelta_kwargs )] def duration_shorter_than ( self , ** timedelta_kwargs ): return self [( self . end - self . time ) &lt ;= timedelta (** timedelta_kwargs )] def take_from ( self , registry_ending , col_name ): for registry_type in registry: if not registry_type . endswith ( registry_ending ): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self . columns: return self [ new_name ] tp = get_type_from_registry ( registry_type ) results = [] if not self . inferred_time: self . infer_time () for x in self [ self . _time_col ]: try: res = tp . loc [ x ] if not isinstance ( res , pd . Series ): res = res . iloc [ 0 ] res = res [ col_name ] except ( KeyError , TypeError ): res = np . nan results . append ( res ) self [ new_name ] = results return self [ new_name ] def add_heartrate ( self ): return self . take_from ( \"heartrate\" , \"value\" ) def heartrate_range ( self , low , high = None ): if \"heartrate_value\" not in self . columns: self . add_heartrate () if high is not None and low is not None: return self [( self [ \"heartrate_value\" ] &gt ;= low ) &amp ; self [ \"heartrate_value\" ] &lt ; high ] if low is not None: return self [ self [ \"heartrate_value\" ] &gt ;= low ] if high is not None: return self [ self [ \"heartrate_value\" ] &lt ; high ] def heartrate_above ( self , value ): return self . heartrate_range ( value ) def heartrate_below ( self , value ): return self . heartrate_range ( None , value ) @classmethod def get_schema ( cls , * args , ** kwargs ): sample = cls . load (* args , nrows = 5 , ** kwargs ) return { k: v for k , v in zip ( sample . columns , sample . dtypes )} @property def start ( self ): if not self . inferred_time: self . infer_time () self . inferred_time = True if self . _start_col == \"start\" : return self . _get_item_cache ( self . _start_col ) if self . _start_col not in self: return None return getattr ( self , self . _start_col ) @property def _office_days ( self ): return self . time . dt . weekday &lt ; 5 @property def _office_hours ( self ): if ( self . time . dt . hour == 0 ). all (): raise ValueError ( \"Hours are not set, thus unreliable. Use `office_days` instead?\" ) if self . start is not None: return np . array ( [ ( x &gt ;= 8 and x &lt ;= 17 and y &gt ;= 8 and y &lt ;= 17 ) for x , y in zip ( self . start . dt . hour , self . end . dt . hour ) ] ) return np . array ([( x &gt ;= 8 and x &lt ;= 17 ) for x in self . time . dt . hour ]) @property def in_office_days ( self ): return self [ self . _office_days ] @property def in_office_hours ( self ): return self [ self . _office_hours ] @property def during_office_hours ( self ): return self [ self . _office_hours &amp ; self . _office_days ] @property def outside_office_hours ( self ): return self [~( self . _office_hours &amp ; self . _office_days )] @property def end ( self ): if not self . inferred_time: self . infer_time () self . inferred_time = True if self . _end_col == \"end\" : return self . _get_item_cache ( self . _end_col ) if self . _end_col not in self: return None return getattr ( self , self . _end_col ) def time_level ( self , col ): if ( col . dt . microsecond != 0 ). any (): return 4 if ( col . dt . second != 0 ). any (): return 3 if ( col . dt . minute != 0 ). any (): return 2 if ( col . dt . hour != 0 ). any (): return 1 return 0 def infer_time ( self ): if self . __class__ . __name__ == \"Results\" : self . _start_col , self . _time_col , self . _end_col = \"start\" , \"start\" , \"end\" return times = [ x for x , y in zip ( self . columns , self . dtypes ) if \"datetime\" in str ( y )] levels = [ self . time_level ( self [ x ]) for x in times ] if not levels: raise ValueError ( f \"No datetime found in {self.__class__.__name__}\" ) max_level = max ( levels ) # workaround # start: 10:00:00 # end: 10:00:59 times = [ t for t , l in zip ( times , levels ) if l == max_level or ( l == 2 and max_level == 3 )] num_times = len ( times ) self . num_times = num_times if num_times == 0 : self . _start_col , self . _time_col , self . _end_col = None , None , None elif num_times == 1 : self . _start_col , self . _time_col , self . _end_col = None , times [ 0 ], None elif num_times == 2 : col1 , col2 = times sub = self [ self [ col1 ]. notnull () &amp ; self [ col2 ]. notnull ()] a , b = sub [ col1 ], sub [ col2 ] if ( a &gt ; b ). all (): col1 , col2 = col2 , col1 elif not ( a &lt ;= b ). all (): raise ValueError ( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\" : col2 , col1 = col1 , col2 self . _start_col , self . _time_col , self . _end_col = col1 , col1 , col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception ( msg + \" Found: \" + str ( times )) def when_at ( self , other , ** window_kwargs ): if isinstance ( other , str ): other = get_type_from_registry ( \"places\" ). containing ( other ) return self . __class__ ( join_time ( other , self , ** window_kwargs )) def browsing ( self , other , ** window_kwargs ): if isinstance ( other , str ): other = get_type_from_registry ( \"browser\" ). containing ( other ) return self . __class__ ( join_time ( other , self , ** window_kwargs )) def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] @property def when_asleep ( self ): return self . __class__ ( join_time ( registry [ \"sleep\" ]. asleep , self )) # browser[\"session\"] = browser.time.diff().apply(lambda x: x.seconds &gt; 20 * 60).cumsum()+10 # session, start, end = zip(*[(name, group.iloc[0].time - pd.Timedelta(minutes=10), group.iloc[-1].time + pd.Timedelta(minutes=10)) for name, group in browser.groupby(\"session\")]) # df = pd.DataFrame({\"session\": session}, index=pd.IntervalIndex.from_arrays(start, end)) # df.index.get_loc(pd.Timestamp(parse_date_tz(\"2019-09-18 18:40:56.729633\").start_date)) def col_contains ( self , string , col_name , case = False , regex = False , na = False ): return self [ self [ col_name ]. str . contains ( string , case = case , regex = regex , na = na )] def __getitem__ ( self , key ): new = super (). __getitem__ ( key ) if isinstance ( new , pd . DataFrame ): new = self . __class__ ( new ) return new @property def text_cols ( self ): return [ x for x , t in zip ( self . columns , self . dtypes ) if t == np . dtype ( 'O' )] @nlp ( \"filter\" , \"by me\" , \"i\" , \"my\" ) def by_me ( self ): return self def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r \"\\b\" + string + r \"\\b\" if col_name is not None: return self . col_contains ( string , col_name , case , regex , na ) bool_cols = [ self [ x ]. str . contains ( string , case = case , regex = regex , na = na ) for x in self . text_cols ] bool_array = bool_cols [ 0 ] for b in bool_cols [ 1 :]: bool_array = np . logical_or ( bool_array , b ) return self . __class__ ( self [ bool_array ]) def query ( self , expr ): return self . __class__ ( super (). query ( expr )) def as_simple ( self , max_n = None ): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [ \"title\" , \"name\" , \"naam\" , \"subject\" , \"url\" , \"content\" , \"text\" , \"value\" ]: res = getattr ( self , x , None ) if res is not None: data [ \"title\" ] = res break res = getattr ( self , \"sender\" , None ) if res is not None: data [ \"sender\" ] = res for x in [ \"url\" , \"path\" , \"file\" ]: res = getattr ( self , x , None ) if res is not None: data [ \"url\" ] = res break for x in [ \"start\" , \"time\" , \"timestamp\" ]: res = getattr ( self , x , None ) if res is not None: data [ \"start\" ] = res break for x in [ \"end\" ]: res = getattr ( self , x , None ) if res is not None: data [ \"end\" ] = res - pd . Timedelta ( microseconds = 1 ) break if data [ \"end\" ] is None: data [ \"end\" ] = data [ \"start\" ] + pd . Timedelta ( minutes = 5 ) data [ \"interval\" ] = False try: data = pd . DataFrame ( data ). sort_values ( \"start\" ) if max_n is not None: data = data . iloc [- max_n: ] return data except ValueError: raise ValueError ( f \"No fields are mapped for {self.__class__.__name__}\" ) def _select_at_day ( self , day_or_class ): if isinstance ( day_or_class , pd . DataFrame ): days = day_or_class . time . dt . date . unique () return self . time . dt . date . isin ( days ) elif isinstance ( day_or_class , ( list , tuple , set , pd . Series )): return self . time . dt . date . isin ( set ( day_or_class )) else: mp = parse_date_tz ( day_or_class ) return ( self . time . dt . date &gt ;= mp . start_date . date ()) &amp ; ( self . time . dt . date &lt ; mp . end_date . date () ) def at_day ( self , day_or_class ): return self [ self . _select_at_day ( day_or_class )] def not_at_day ( self , day_or_class ): return self [~ self . _select_at_day ( day_or_class )] @property def last_day ( self ): return self [( self . time &gt ; yesterday ()) &amp ; ( self . time &lt ; now ())] @property def yesterday ( self ): return self [( self . time &gt ; yesterday ()) &amp ; ( self . time &lt ; now ())] @property def last_week ( self ): return self [( self . time &gt ; last_week ()) &amp ; ( self . time &lt ; now ())] @property def last_month ( self ): return self [( self . time &gt ; last_month ()) &amp ; ( self . time &lt ; now ())] @property def last_year ( self ): return self [( self . time &gt ; last_year ()) &amp ; ( self . time &lt ; now ())] def near ( self , s ): if isinstance ( s , NDF ) and s . df_name . endswith ( \"places\" ): selection = s else: selection = get_type_from_registry ( \"places\" ). containing ( s ) return self . when_at ( selection ) def to_place ( self ): results = [] places = get_type_from_registry ( \"places\" ) for time in self . time: try: results . append ( places . iloc [ places . index . get_loc ( time )]. iloc [ 0 ]) except ( TypeError , KeyError ): pass return places . __class__ ( results ) def in_a ( self , s ): return self . near ( s ) def read ( self , index ): return just . read ( self . path [ index ]) def view ( self , index ): view ( self . path [ index ]) def head ( self , * args , ** kwargs ): return self . __class__ ( super (). head (* args , ** kwargs )) def tail ( self , * args , ** kwargs ): return self . __class__ ( super (). tail (* args , ** kwargs )) @nlp ( \"end\" , \"how many\" , \"how many times\" , \"how often\" ) def count ( self ): return self . shape [ 0 ] @property def at_home ( self ): self . take_from ( \"places\" , \"category\" ) return self [ self [ \"places_category\" ] == \"Home\" ] @property def at_work ( self ): self . take_from ( \"places\" , \"category\" ) return self [ self [ \"places_category\" ] == \"Work\" ] def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ): if is_mp ( start ): start = start . start_date end = start . end_date elif isinstance ( start , str ) and end is None: mp = parse_date_tz ( start ) start = mp . start_date end = mp . end_date elif isinstance ( start , str ) and isinstance ( end , str ): mp = parse_date_tz ( start ) start = mp . start_date mp = parse_date_tz ( end ) end = mp . start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError ( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self . infer_time () if window_kwargs: start = start - pd . Timedelta (** window_kwargs ) end = end + pd . Timedelta (** window_kwargs ) if self . _start_col is None: res = self [ ab_overlap_c ( start , end , self [ self . _time_col ])] else: res = self [ ab_overlap_cd ( self [ self . _start_col ], self [ self . _end_col ], start , end )] if not res . empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res [ \"sort_score\" ] = res [ self . _time_col ] res = res . sort_values ( 'sort_score' ). drop ( 'sort_score' , axis = 1 ) return self . __class__ ( res ) when = when_at @property def duration ( self ): return self . end - self . start def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ): return self . __class__ ( pd . DataFrame . sort_values ( self , by , axis , ascending , inplace , kind , na_position ) ) @nlp ( \"filter\" , \"last\" , \"last time\" , \"most recently\" ) def last ( self ): _ = self . time # to get inferred time if not set col = self . _time_col or self . _start_col return self . __class__ ( self . sort_values ( col , na_position = \"last\" , ascending = False ). iloc [: 1 ]) # maybe which/what could be showing only unique? @nlp ( \"end\" , \"show\" , \"show me\" , \"show me the\" , \"show the\" , \"what\" ) def show_me ( self ): _ = self . time # to get inferred time if not set col = self . _time_col or self . _start_col return self . __class__ ( self . sort_values ( col , na_position = \"last\" , ascending = False )) def at ( self , time_or_place ): if isinstance ( time_or_place , NDF ) and time_or_place . df_name . endswith ( \"places\" ): return self . when_at ( time_or_place ) if isinstance ( time_or_place , str ): mp = parse_date_tz ( time_or_place ) if mp: start = mp . start_date end = mp . end_date return self . at_time ( start , end ) else: return self . when_at ( get_type_from_registry ( \"places\" ). containing ( time_or_place )) raise ValueError ( \"neither time nor place was passed\" ) def to_html ( self ): if self . selected_columns: data = pd . DataFrame ({ x: getattr(self, x) for x in self.selected_columns }) return data . to_html () return super (). to_html () def get_type_from_registry ( self , tp ): for key , value in rergistry . items (): if key . endswith ( tp ): return value def __call_ ( self ): return self Descendants nostalgia.ndf.Results nostalgia.interfaces.chat.ChatInterface nostalgia.interfaces.places.Places nostalgia.interfaces.post.PostInterface nostalgia.sources.chrome_history.WebHistory nostalgia.sources.emacs_file_visits.FileVisits nostalgia.sources.facebook.Facebook nostalgia.sources.fitbit.heartrate.FitbitHeartrate nostalgia.sources.fitbit.sleep.FitbitSleep nostalgia.sources.google.Google nostalgia.sources.ing_banking.mijn_ing.Payments nostalgia.sources.mijn_chipkaart.MijnChipkaart nostalgia.sources.samsung.Samsung nostalgia.sources.samsung.heartrate.SamsungHeartrate nostalgia.sources.samsung.stress.SamsungStress nostalgia.sources.screenshots.Screenshots nostalgia.sources.shazam.Shazam nostalgia.sources.sleepcycle.SleepCycle nostalgia.sources.web.linked_events.Events nostalgia.sources.web.linked_google_search.GoogleSearch nostalgia.sources.web.linked_offers.Offers nostalgia.sources.web.linked_person.Person nostalgia.sources.web.linked_videos.Videos nostalgia.sources.web.videos_watched.VideosWatched nostalgia.sources.whereami.Whereami Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) Results class Results ( data ) View Source class Results ( NDF ) : @classmethod def merge ( cls , * dfs , max_n = None ) : data = pd . concat ([ x . as_simple ( max_n ) for x in dfs ]) data = data . set_index ( \"start\" ) data = data . sort_values ( \"start\" , na_position = \"first\" ) data [ \"start\" ] = data . index return Results ( data ) def get_original ( self , index_loc ) : row = self . iloc [ index_loc ] rec = registry [ row . type ]. loc [ row [ \"index_loc\" ]] # rec . add_heartrate () # not here , but at the results level .. but then just to display # if a certain value if not isinstance ( rec , pd . Series ) : rec = rec . iloc [ 0 ] return rec Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) merge def merge ( * dfs , max_n = None ) View Source @classmethod def merge(cls, *dfs, max_n=None): data = pd.concat([x.as_simple(max_n) for x in dfs]) data = data.set_index(\"start\") data = data.sort_values(\"start\", na_position=\"first\") data[\"start\"] = data.index return Results(data) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_original def get_original ( self , index_loc ) View Source def get_original(self, index_loc): row = self.iloc[index_loc] rec = registry[row.type].loc[row[\"index_loc\"]] # rec.add_heartrate() # not here, but at the results level.. but then just to display # if a certain value if not isinstance(rec, pd.Series): rec = rec.iloc[0] return rec get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Ndf"},{"location":"reference/nostalgia/ndf/#module-nostalgiandf","text":"View Source import os import re import pandas as pd import numpy as np from nostalgia.times import now , yesterday , last_week , last_month , last_year , parse_date_tz from metadate import is_mp from datetime import timedelta import just from nostalgia.nlp import nlp_registry , nlp , n , COLUMN_BLACKLIST , ResultInfo from nostalgia.utils import get_token_set , normalize_name , view from nostalgia.extracter import load_from_download from nostalgia.times import datetime_from_timestamp from nostalgia.cache import get_cache from nostalgia.data_loading import Loader def ab_overlap_cd ( a , b , c , d ): return (( a & lt ; d ) & amp ; ( b & gt ; c )) | (( b & gt ; c ) & amp ; ( d & gt ; a )) def ab_overlap_c ( a , b , c ): return ( a & lt ; = c ) & amp ; ( c & lt ; = b ) def join_time_naive ( locs , df , ** window_kwargs ): tmp = [] if not locs . inferred_time : locs . infer_time () for _ , row in locs . iterrows (): if df . start is not None : if locs . _start_col is None : start = row [ locs . _time_col ] - pd . Timedelta ( ** window_kwargs ) end = row [ locs . _time_col ] + pd . Timedelta ( ** window_kwargs ) else : start = row [ locs . _start_col ] end = row [ locs . _end_col ] res = df [ ab_overlap_cd ( df . start , df . end , start , end )] else : if locs . _start_col is None : start = row [ locs . _time_col ] - pd . Timedelta ( ** window_kwargs ) end = row [ locs . _time_col ] + pd . Timedelta ( ** window_kwargs ) else : start , end = row . start , row . end res = df [ df . time . between ( start , end )] if not res . empty : # import pdb # pdb.set_trace() tmp . append ( res ) if not tmp : return df [: 0 ] tmp = pd . concat ([ pd . DataFrame ( x ) for x in tmp ]) try : tmp = tmp . drop_duplicates () except TypeError : tmp = tmp [ ~ tmp . astype ( str ) . duplicated ()] return tmp def join_time_index ( locs , df , ** window_kwargs ): try : return df [ df . time . apply ( lambda x : locs . index . contains ( x ))] except TypeError : print ( \"warning\" ) return join_time_naive ( locs , df , ** window_kwargs ) def join_time ( locs , df , ** window_kwargs ): if df . start is not None : return join_time_naive ( locs , df , ** window_kwargs ) if not str ( df . index . dtype ) . startswith ( \"interval\" ): return join_time_naive ( locs , df , ** window_kwargs ) W = locs . shape [ 0 ] L = df . shape [ 0 ] y = 0.001 + W * - 0.318 + L * 0.013 if y & gt ; 300 : p = 1 else : exponent = np . exp ( y ) p = exponent / ( 1 + exponent ) if p & gt ; 0.5 : fn = join_time_naive else : fn = join_time_index return fn ( locs , df , ** window_kwargs ) registry = {} def get_type_from_registry ( tp ): for key , value in registry . items (): if key . endswith ( tp ): return value def time ( x ): return x . time def col_contains_wrapper ( word , col ): def col_contains ( x ): return x . col_contains ( word , col ) return col_contains class NDF : keywords = [] nlp_columns = [] nlp_when = True selected_columns = [] vendor = None def __init__ ( self , data ): super () . __init__ ( data ) C = self . __class__ self . num_times = None self . _start_col , self . _time_col , self . _end_col = None , None , None self . inferred_time = False # IF BIG BREAK, THEN THIS IS HERE # if self.df_name not in registry: # registry[self.df_name] = self if self . df_name != \"results\" : if self . df_name not in registry : registry [ self . df_name ] = self # if the new data is smaller than what is in the registry, do not overwrite the registry elif data . shape [ 0 ] & gt ; registry [ self . df_name ] . shape [ 0 ]: registry [ self . df_name ] = self if n is None : return seen_keyword_keywords = set () for kw in self . keywords : for w in n . simple_extend ( kw ): if w and isinstance ( w , str ): nlp_registry [ w ] . add ( ResultInfo ( C , \"start\" , orig_word = kw )) seen_keyword_keywords . add ( w ) nlp_registry [ kw ] . add ( ResultInfo ( C , \"start\" , orig_word = kw )) seen_keyword_keywords . add ( kw ) # nlp_keywords_column = [] # if \"keywords\" in data: # nlp_keywords_column = [\"keywords\"] BLACKLIST = seen_keyword_keywords . union ( COLUMN_BLACKLIST ) for col in self . nlp_columns : words = get_token_set ( self [ col ] . unique ()) for word in words : if word in BLACKLIST : continue for w in n . simple_extend ( word ): if w in BLACKLIST : continue if w and isinstance ( w , str ): nlp_registry [ w ] . add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) if n . is_verb ( word ): for w in n . get_verbs ( word ) . values (): if w in BLACKLIST : continue if w in words or not isinstance ( w , str ): continue nlp_registry [ w ] . add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) if n . is_verb ( word ): for w in n . get_verbs ( word ) . values (): if w in BLACKLIST : continue if w in words or not isinstance ( w , str ): continue nlp_registry [ w ] . add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) nlp_registry [ word ] . add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) if self . nlp_when : nlp_registry [ \"when\" ] . add ( ResultInfo ( C , \"end\" , time , orig_word = \"when\" )) @classmethod def ingest ( cls ): load_from_download ( vendor = cls . vendor , ** cls . ingest_settings ) @property def df_name ( self ): name = normalize_name ( self . __class__ . __name__ ) if self . vendor is not None and not name . startswith ( self . vendor ): name = self . vendor + \"_\" + name return name @classmethod def df_label ( cls ): return normalize_name ( cls . __name__ ) . replace ( \"_\" , \" \" ) . title () @property def time ( self ): if not self . inferred_time : self . infer_time () self . inferred_time = True if self . _time_col == \"time\" : return self . _get_item_cache ( self . _time_col ) if self . _time_col not in self : return None return getattr ( self , self . _time_col ) # @property # def _constructor(self): # return self.__class__ def duration_longer_than ( self , ** timedelta_kwargs ): return self [( self . end - self . time ) & gt ; = timedelta ( ** timedelta_kwargs )] def duration_shorter_than ( self , ** timedelta_kwargs ): return self [( self . end - self . time ) & lt ; = timedelta ( ** timedelta_kwargs )] def take_from ( self , registry_ending , col_name ): for registry_type in registry : if not registry_type . endswith ( registry_ending ): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self . columns : return self [ new_name ] tp = get_type_from_registry ( registry_type ) results = [] if not self . inferred_time : self . infer_time () for x in self [ self . _time_col ]: try : res = tp . loc [ x ] if not isinstance ( res , pd . Series ): res = res . iloc [ 0 ] res = res [ col_name ] except ( KeyError , TypeError ): res = np . nan results . append ( res ) self [ new_name ] = results return self [ new_name ] def add_heartrate ( self ): return self . take_from ( \"heartrate\" , \"value\" ) def heartrate_range ( self , low , high = None ): if \"heartrate_value\" not in self . columns : self . add_heartrate () if high is not None and low is not None : return self [( self [ \"heartrate_value\" ] & gt ; = low ) & amp ; self [ \"heartrate_value\" ] & lt ; high ] if low is not None : return self [ self [ \"heartrate_value\" ] & gt ; = low ] if high is not None : return self [ self [ \"heartrate_value\" ] & lt ; high ] def heartrate_above ( self , value ): return self . heartrate_range ( value ) def heartrate_below ( self , value ): return self . heartrate_range ( None , value ) @classmethod def get_schema ( cls , * args , ** kwargs ): sample = cls . load ( * args , nrows = 5 , ** kwargs ) return { k : v for k , v in zip ( sample . columns , sample . dtypes )} @property def start ( self ): if not self . inferred_time : self . infer_time () self . inferred_time = True if self . _start_col == \"start\" : return self . _get_item_cache ( self . _start_col ) if self . _start_col not in self : return None return getattr ( self , self . _start_col ) @property def _office_days ( self ): return self . time . dt . weekday & lt ; 5 @property def _office_hours ( self ): if ( self . time . dt . hour == 0 ) . all (): raise ValueError ( \"Hours are not set, thus unreliable. Use `office_days` instead?\" ) if self . start is not None : return np . array ( [ ( x & gt ; = 8 and x & lt ; = 17 and y & gt ; = 8 and y & lt ; = 17 ) for x , y in zip ( self . start . dt . hour , self . end . dt . hour ) ] ) return np . array ([( x & gt ; = 8 and x & lt ; = 17 ) for x in self . time . dt . hour ]) @property def in_office_days ( self ): return self [ self . _office_days ] @property def in_office_hours ( self ): return self [ self . _office_hours ] @property def during_office_hours ( self ): return self [ self . _office_hours & amp ; self . _office_days ] @property def outside_office_hours ( self ): return self [ ~ ( self . _office_hours & amp ; self . _office_days )] @property def end ( self ): if not self . inferred_time : self . infer_time () self . inferred_time = True if self . _end_col == \"end\" : return self . _get_item_cache ( self . _end_col ) if self . _end_col not in self : return None return getattr ( self , self . _end_col ) def time_level ( self , col ): if ( col . dt . microsecond != 0 ) . any (): return 4 if ( col . dt . second != 0 ) . any (): return 3 if ( col . dt . minute != 0 ) . any (): return 2 if ( col . dt . hour != 0 ) . any (): return 1 return 0 def infer_time ( self ): if self . __class__ . __name__ == \"Results\" : self . _start_col , self . _time_col , self . _end_col = \"start\" , \"start\" , \"end\" return times = [ x for x , y in zip ( self . columns , self . dtypes ) if \"datetime\" in str ( y )] levels = [ self . time_level ( self [ x ]) for x in times ] if not levels : raise ValueError ( f \"No datetime found in {self.__class__.__name__}\" ) max_level = max ( levels ) # workaround # start: 10:00:00 # end: 10:00:59 times = [ t for t , l in zip ( times , levels ) if l == max_level or ( l == 2 and max_level == 3 )] num_times = len ( times ) self . num_times = num_times if num_times == 0 : self . _start_col , self . _time_col , self . _end_col = None , None , None elif num_times == 1 : self . _start_col , self . _time_col , self . _end_col = None , times [ 0 ], None elif num_times == 2 : col1 , col2 = times sub = self [ self [ col1 ] . notnull () & amp ; self [ col2 ] . notnull ()] a , b = sub [ col1 ], sub [ col2 ] if ( a & gt ; b ) . all (): col1 , col2 = col2 , col1 elif not ( a & lt ; = b ) . all (): raise ValueError ( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\" : col2 , col1 = col1 , col2 self . _start_col , self . _time_col , self . _end_col = col1 , col1 , col2 else : msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception ( msg + \" Found: \" + str ( times )) def when_at ( self , other , ** window_kwargs ): if isinstance ( other , str ): other = get_type_from_registry ( \"places\" ) . containing ( other ) return self . __class__ ( join_time ( other , self , ** window_kwargs )) def browsing ( self , other , ** window_kwargs ): if isinstance ( other , str ): other = get_type_from_registry ( \"browser\" ) . containing ( other ) return self . __class__ ( join_time ( other , self , ** window_kwargs )) def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None : return self [( self . start . dt . hour & gt ; start ) | ( self . end . dt . hour & lt ; end )] return self [( self . time . dt . hour & gt ; start ) | ( self . time . dt . hour & lt ; end )] @property def when_asleep ( self ): return self . __class__ ( join_time ( registry [ \"sleep\" ] . asleep , self )) # browser[\"session\"] = browser.time.diff().apply(lambda x: x.seconds &gt; 20 * 60).cumsum()+10 # session, start, end = zip(*[(name, group.iloc[0].time - pd.Timedelta(minutes=10), group.iloc[-1].time + pd.Timedelta(minutes=10)) for name, group in browser.groupby(\"session\")]) # df = pd.DataFrame({\"session\": session}, index=pd.IntervalIndex.from_arrays(start, end)) # df.index.get_loc(pd.Timestamp(parse_date_tz(\"2019-09-18 18:40:56.729633\").start_date)) def col_contains ( self , string , col_name , case = False , regex = False , na = False ): return self [ self [ col_name ] . str . contains ( string , case = case , regex = regex , na = na )] def __getitem__ ( self , key ): new = super () . __getitem__ ( key ) if isinstance ( new , pd . DataFrame ): new = self . __class__ ( new ) return new @property def text_cols ( self ): return [ x for x , t in zip ( self . columns , self . dtypes ) if t == np . dtype ( 'O' )] @nlp ( \"filter\" , \"by me\" , \"i\" , \"my\" ) def by_me ( self ): return self def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound : string = r \"\\b\" + string + r \"\\b\" if col_name is not None : return self . col_contains ( string , col_name , case , regex , na ) bool_cols = [ self [ x ] . str . contains ( string , case = case , regex = regex , na = na ) for x in self . text_cols ] bool_array = bool_cols [ 0 ] for b in bool_cols [ 1 :]: bool_array = np . logical_or ( bool_array , b ) return self . __class__ ( self [ bool_array ]) def query ( self , expr ): return self . __class__ ( super () . query ( expr )) def as_simple ( self , max_n = None ): data = { \"title\" : self . df_name , # default, to be overwritten \"url\" : None , \"start\" : None , \"end\" : None , # \"body\": None, \"type\" : self . df_name , \"interval\" : True , \"sender\" : None , \"value\" : getattr ( self , \"value\" , None ), \"index_loc\" : self . index , } for x in [ \"title\" , \"name\" , \"naam\" , \"subject\" , \"url\" , \"content\" , \"text\" , \"value\" ]: res = getattr ( self , x , None ) if res is not None : data [ \"title\" ] = res break res = getattr ( self , \"sender\" , None ) if res is not None : data [ \"sender\" ] = res for x in [ \"url\" , \"path\" , \"file\" ]: res = getattr ( self , x , None ) if res is not None : data [ \"url\" ] = res break for x in [ \"start\" , \"time\" , \"timestamp\" ]: res = getattr ( self , x , None ) if res is not None : data [ \"start\" ] = res break for x in [ \"end\" ]: res = getattr ( self , x , None ) if res is not None : data [ \"end\" ] = res - pd . Timedelta ( microseconds = 1 ) break if data [ \"end\" ] is None : data [ \"end\" ] = data [ \"start\" ] + pd . Timedelta ( minutes = 5 ) data [ \"interval\" ] = False try : data = pd . DataFrame ( data ) . sort_values ( \"start\" ) if max_n is not None : data = data . iloc [ - max_n :] return data except ValueError : raise ValueError ( f \"No fields are mapped for {self.__class__.__name__}\" ) def _select_at_day ( self , day_or_class ): if isinstance ( day_or_class , pd . DataFrame ): days = day_or_class . time . dt . date . unique () return self . time . dt . date . isin ( days ) elif isinstance ( day_or_class , ( list , tuple , set , pd . Series )): return self . time . dt . date . isin ( set ( day_or_class )) else : mp = parse_date_tz ( day_or_class ) return ( self . time . dt . date & gt ; = mp . start_date . date ()) & amp ; ( self . time . dt . date & lt ; mp . end_date . date () ) def at_day ( self , day_or_class ): return self [ self . _select_at_day ( day_or_class )] def not_at_day ( self , day_or_class ): return self [ ~ self . _select_at_day ( day_or_class )] @property def last_day ( self ): return self [( self . time & gt ; yesterday ()) & amp ; ( self . time & lt ; now ())] @property def yesterday ( self ): return self [( self . time & gt ; yesterday ()) & amp ; ( self . time & lt ; now ())] @property def last_week ( self ): return self [( self . time & gt ; last_week ()) & amp ; ( self . time & lt ; now ())] @property def last_month ( self ): return self [( self . time & gt ; last_month ()) & amp ; ( self . time & lt ; now ())] @property def last_year ( self ): return self [( self . time & gt ; last_year ()) & amp ; ( self . time & lt ; now ())] def near ( self , s ): if isinstance ( s , NDF ) and s . df_name . endswith ( \"places\" ): selection = s else : selection = get_type_from_registry ( \"places\" ) . containing ( s ) return self . when_at ( selection ) def to_place ( self ): results = [] places = get_type_from_registry ( \"places\" ) for time in self . time : try : results . append ( places . iloc [ places . index . get_loc ( time )] . iloc [ 0 ]) except ( TypeError , KeyError ): pass return places . __class__ ( results ) def in_a ( self , s ): return self . near ( s ) def read ( self , index ): return just . read ( self . path [ index ]) def view ( self , index ): view ( self . path [ index ]) def head ( self , * args , ** kwargs ): return self . __class__ ( super () . head ( * args , ** kwargs )) def tail ( self , * args , ** kwargs ): return self . __class__ ( super () . tail ( * args , ** kwargs )) @nlp ( \"end\" , \"how many\" , \"how many times\" , \"how often\" ) def count ( self ): return self . shape [ 0 ] @property def at_home ( self ): self . take_from ( \"places\" , \"category\" ) return self [ self [ \"places_category\" ] == \"Home\" ] @property def at_work ( self ): self . take_from ( \"places\" , \"category\" ) return self [ self [ \"places_category\" ] == \"Work\" ] def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ): if is_mp ( start ): start = start . start_date end = start . end_date elif isinstance ( start , str ) and end is None : mp = parse_date_tz ( start ) start = mp . start_date end = mp . end_date elif isinstance ( start , str ) and isinstance ( end , str ): mp = parse_date_tz ( start ) start = mp . start_date mp = parse_date_tz ( end ) end = mp . start_date elif end is None and window_kwargs : end = start elif end is None : raise ValueError ( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self . infer_time () if window_kwargs : start = start - pd . Timedelta ( ** window_kwargs ) end = end + pd . Timedelta ( ** window_kwargs ) if self . _start_col is None : res = self [ ab_overlap_c ( start , end , self [ self . _time_col ])] else : res = self [ ab_overlap_cd ( self [ self . _start_col ], self [ self . _end_col ], start , end )] if not res . empty and sort_diff : # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res [ \"sort_score\" ] = res [ self . _time_col ] res = res . sort_values ( 'sort_score' ) . drop ( 'sort_score' , axis = 1 ) return self . __class__ ( res ) when = when_at @property def duration ( self ): return self . end - self . start def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ): return self . __class__ ( pd . DataFrame . sort_values ( self , by , axis , ascending , inplace , kind , na_position ) ) @nlp ( \"filter\" , \"last\" , \"last time\" , \"most recently\" ) def last ( self ): _ = self . time # to get inferred time if not set col = self . _time_col or self . _start_col return self . __class__ ( self . sort_values ( col , na_position = \"last\" , ascending = False ) . iloc [: 1 ]) # maybe which/what could be showing only unique? @nlp ( \"end\" , \"show\" , \"show me\" , \"show me the\" , \"show the\" , \"what\" ) def show_me ( self ): _ = self . time # to get inferred time if not set col = self . _time_col or self . _start_col return self . __class__ ( self . sort_values ( col , na_position = \"last\" , ascending = False )) def at ( self , time_or_place ): if isinstance ( time_or_place , NDF ) and time_or_place . df_name . endswith ( \"places\" ): return self . when_at ( time_or_place ) if isinstance ( time_or_place , str ): mp = parse_date_tz ( time_or_place ) if mp : start = mp . start_date end = mp . end_date return self . at_time ( start , end ) else : return self . when_at ( get_type_from_registry ( \"places\" ) . containing ( time_or_place )) raise ValueError ( \"neither time nor place was passed\" ) def to_html ( self ): if self . selected_columns : data = pd . DataFrame ({ x : getattr ( self , x ) for x in self . selected_columns }) return data . to_html () return super () . to_html () def get_type_from_registry ( self , tp ): for key , value in rergistry . items (): if key . endswith ( tp ): return value def __call_ ( self ): return self # ov = ab_overlap_cd( # places.start, # places.end, # parse(\"2015-02-05 21:53:09.581001+00:00\"), # parse(\"2015-02-05 21:53:13.881000+00:00\"), # ) class Results ( NDF ): @classmethod def merge ( cls , * dfs , max_n = None ): data = pd . concat ([ x . as_simple ( max_n ) for x in dfs ]) data = data . set_index ( \"start\" ) data = data . sort_values ( \"start\" , na_position = \"first\" ) data [ \"start\" ] = data . index return Results ( data ) def get_original ( self , index_loc ): row = self . iloc [ index_loc ] rec = registry [ row . type ] . loc [ row [ \"index_loc\" ]] # rec.add_heartrate() # not here, but at the results level.. but then just to display # if a certain value if not isinstance ( rec , pd . Series ): rec = rec . iloc [ 0 ] return rec","title":"Module nostalgia.ndf"},{"location":"reference/nostalgia/ndf/#variables","text":"COLUMN_BLACKLIST n nlp_registry registry","title":"Variables"},{"location":"reference/nostalgia/ndf/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/ndf/#ab_overlap_c","text":"def ab_overlap_c ( a , b , c ) View Source def ab_overlap_c(a, b, c): return (a &lt;= c) &amp; (c &lt;= b)","title":"ab_overlap_c"},{"location":"reference/nostalgia/ndf/#ab_overlap_cd","text":"def ab_overlap_cd ( a , b , c , d ) View Source def ab_overlap_cd(a, b, c, d): return ((a &lt; d) &amp; (b &gt; c)) | ((b &gt; c) &amp; (d &gt; a))","title":"ab_overlap_cd"},{"location":"reference/nostalgia/ndf/#col_contains_wrapper","text":"def col_contains_wrapper ( word , col ) View Source def col_contains_wrapper(word, col): def col_contains(x): return x.col_contains(word, col) return col_contains","title":"col_contains_wrapper"},{"location":"reference/nostalgia/ndf/#get_type_from_registry","text":"def get_type_from_registry ( tp ) View Source def get_type_from_registry(tp): for key, value in registry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/ndf/#join_time","text":"def join_time ( locs , df , ** window_kwargs ) View Source def join_time(locs, df, **window_kwargs): if df.start is not None: return join_time_naive(locs, df, **window_kwargs) if not str(df.index.dtype).startswith(\"interval\"): return join_time_naive(locs, df, **window_kwargs) W = locs.shape[0] L = df.shape[0] y = 0.001 + W * -0.318 + L * 0.013 if y &gt; 300: p = 1 else: exponent = np.exp(y) p = exponent / (1 + exponent) if p &gt; 0.5: fn = join_time_naive else: fn = join_time_index return fn(locs, df, **window_kwargs)","title":"join_time"},{"location":"reference/nostalgia/ndf/#join_time_index","text":"def join_time_index ( locs , df , ** window_kwargs ) View Source def join_time_index(locs, df, **window_kwargs): try: return df[df.time.apply(lambda x: locs.index.contains(x))] except TypeError: print(\"warning\") return join_time_naive(locs, df, **window_kwargs)","title":"join_time_index"},{"location":"reference/nostalgia/ndf/#join_time_naive","text":"def join_time_naive ( locs , df , ** window_kwargs ) View Source def join_time_naive ( locs , df , ** window_kwargs ): tmp = [] if not locs . inferred_time : locs . infer_time () for _ , row in locs . iterrows (): if df . start is not None : if locs . _start_col is None : start = row [ locs . _time_col ] - pd . Timedelta ( ** window_kwargs ) end = row [ locs . _time_col ] + pd . Timedelta ( ** window_kwargs ) else : start = row [ locs . _start_col ] end = row [ locs . _end_col ] res = df [ ab_overlap_cd ( df . start , df . end , start , end )] else : if locs . _start_col is None : start = row [ locs . _time_col ] - pd . Timedelta ( ** window_kwargs ) end = row [ locs . _time_col ] + pd . Timedelta ( ** window_kwargs ) else : start , end = row . start , row . end res = df [ df . time . between ( start , end )] if not res . empty : # import pdb # pdb.set_trace() tmp . append ( res ) if not tmp : return df [: 0 ] tmp = pd . concat ([ pd . DataFrame ( x ) for x in tmp ]) try : tmp = tmp . drop_duplicates () except TypeError : tmp = tmp [ ~ tmp . astype ( str ) . duplicated ()] return tmp","title":"join_time_naive"},{"location":"reference/nostalgia/ndf/#time","text":"def time ( x ) View Source def time(x): return x.time","title":"time"},{"location":"reference/nostalgia/ndf/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/ndf/#ndf","text":"class NDF ( data ) View Source class NDF: keywords = [] nlp_columns = [] nlp_when = True selected_columns = [] vendor = None def __init__ ( self , data ): super (). __init__ ( data ) C = self . __class__ self . num_times = None self . _start_col , self . _time_col , self . _end_col = None , None , None self . inferred_time = False # IF BIG BREAK, THEN THIS IS HERE # if self.df_name not in registry: # registry[self.df_name] = self if self . df_name != \"results\" : if self . df_name not in registry: registry [ self . df_name ] = self # if the new data is smaller than what is in the registry, do not overwrite the registry elif data . shape [ 0 ] &gt ; registry [ self . df_name ]. shape [ 0 ]: registry [ self . df_name ] = self if n is None: return seen_keyword_keywords = set () for kw in self . keywords: for w in n . simple_extend ( kw ): if w and isinstance ( w , str ): nlp_registry [ w ]. add ( ResultInfo ( C , \"start\" , orig_word = kw )) seen_keyword_keywords . add ( w ) nlp_registry [ kw ]. add ( ResultInfo ( C , \"start\" , orig_word = kw )) seen_keyword_keywords . add ( kw ) # nlp_keywords_column = [] # if \"keywords\" in data: # nlp_keywords_column = [\"keywords\"] BLACKLIST = seen_keyword_keywords . union ( COLUMN_BLACKLIST ) for col in self . nlp_columns: words = get_token_set ( self [ col ]. unique ()) for word in words: if word in BLACKLIST: continue for w in n . simple_extend ( word ): if w in BLACKLIST: continue if w and isinstance ( w , str ): nlp_registry [ w ]. add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) if n . is_verb ( word ): for w in n . get_verbs ( word ). values (): if w in BLACKLIST: continue if w in words or not isinstance ( w , str ): continue nlp_registry [ w ]. add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) if n . is_verb ( word ): for w in n . get_verbs ( word ). values (): if w in BLACKLIST: continue if w in words or not isinstance ( w , str ): continue nlp_registry [ w ]. add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) nlp_registry [ word ]. add ( ResultInfo ( C , \"filter\" , col_contains_wrapper ( word , col ), col , word ) ) if self . nlp_when: nlp_registry [ \"when\" ]. add ( ResultInfo ( C , \"end\" , time , orig_word = \"when\" )) @classmethod def ingest ( cls ): load_from_download ( vendor = cls . vendor , ** cls . ingest_settings ) @property def df_name ( self ): name = normalize_name ( self . __class__ . __name__ ) if self . vendor is not None and not name . startswith ( self . vendor ): name = self . vendor + \"_\" + name return name @classmethod def df_label ( cls ): return normalize_name ( cls . __name__ ). replace ( \"_\" , \" \" ). title () @property def time ( self ): if not self . inferred_time: self . infer_time () self . inferred_time = True if self . _time_col == \"time\" : return self . _get_item_cache ( self . _time_col ) if self . _time_col not in self: return None return getattr ( self , self . _time_col ) # @property # def _constructor(self): # return self.__class__ def duration_longer_than ( self , ** timedelta_kwargs ): return self [( self . end - self . time ) &gt ;= timedelta (** timedelta_kwargs )] def duration_shorter_than ( self , ** timedelta_kwargs ): return self [( self . end - self . time ) &lt ;= timedelta (** timedelta_kwargs )] def take_from ( self , registry_ending , col_name ): for registry_type in registry: if not registry_type . endswith ( registry_ending ): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self . columns: return self [ new_name ] tp = get_type_from_registry ( registry_type ) results = [] if not self . inferred_time: self . infer_time () for x in self [ self . _time_col ]: try: res = tp . loc [ x ] if not isinstance ( res , pd . Series ): res = res . iloc [ 0 ] res = res [ col_name ] except ( KeyError , TypeError ): res = np . nan results . append ( res ) self [ new_name ] = results return self [ new_name ] def add_heartrate ( self ): return self . take_from ( \"heartrate\" , \"value\" ) def heartrate_range ( self , low , high = None ): if \"heartrate_value\" not in self . columns: self . add_heartrate () if high is not None and low is not None: return self [( self [ \"heartrate_value\" ] &gt ;= low ) &amp ; self [ \"heartrate_value\" ] &lt ; high ] if low is not None: return self [ self [ \"heartrate_value\" ] &gt ;= low ] if high is not None: return self [ self [ \"heartrate_value\" ] &lt ; high ] def heartrate_above ( self , value ): return self . heartrate_range ( value ) def heartrate_below ( self , value ): return self . heartrate_range ( None , value ) @classmethod def get_schema ( cls , * args , ** kwargs ): sample = cls . load (* args , nrows = 5 , ** kwargs ) return { k: v for k , v in zip ( sample . columns , sample . dtypes )} @property def start ( self ): if not self . inferred_time: self . infer_time () self . inferred_time = True if self . _start_col == \"start\" : return self . _get_item_cache ( self . _start_col ) if self . _start_col not in self: return None return getattr ( self , self . _start_col ) @property def _office_days ( self ): return self . time . dt . weekday &lt ; 5 @property def _office_hours ( self ): if ( self . time . dt . hour == 0 ). all (): raise ValueError ( \"Hours are not set, thus unreliable. Use `office_days` instead?\" ) if self . start is not None: return np . array ( [ ( x &gt ;= 8 and x &lt ;= 17 and y &gt ;= 8 and y &lt ;= 17 ) for x , y in zip ( self . start . dt . hour , self . end . dt . hour ) ] ) return np . array ([( x &gt ;= 8 and x &lt ;= 17 ) for x in self . time . dt . hour ]) @property def in_office_days ( self ): return self [ self . _office_days ] @property def in_office_hours ( self ): return self [ self . _office_hours ] @property def during_office_hours ( self ): return self [ self . _office_hours &amp ; self . _office_days ] @property def outside_office_hours ( self ): return self [~( self . _office_hours &amp ; self . _office_days )] @property def end ( self ): if not self . inferred_time: self . infer_time () self . inferred_time = True if self . _end_col == \"end\" : return self . _get_item_cache ( self . _end_col ) if self . _end_col not in self: return None return getattr ( self , self . _end_col ) def time_level ( self , col ): if ( col . dt . microsecond != 0 ). any (): return 4 if ( col . dt . second != 0 ). any (): return 3 if ( col . dt . minute != 0 ). any (): return 2 if ( col . dt . hour != 0 ). any (): return 1 return 0 def infer_time ( self ): if self . __class__ . __name__ == \"Results\" : self . _start_col , self . _time_col , self . _end_col = \"start\" , \"start\" , \"end\" return times = [ x for x , y in zip ( self . columns , self . dtypes ) if \"datetime\" in str ( y )] levels = [ self . time_level ( self [ x ]) for x in times ] if not levels: raise ValueError ( f \"No datetime found in {self.__class__.__name__}\" ) max_level = max ( levels ) # workaround # start: 10:00:00 # end: 10:00:59 times = [ t for t , l in zip ( times , levels ) if l == max_level or ( l == 2 and max_level == 3 )] num_times = len ( times ) self . num_times = num_times if num_times == 0 : self . _start_col , self . _time_col , self . _end_col = None , None , None elif num_times == 1 : self . _start_col , self . _time_col , self . _end_col = None , times [ 0 ], None elif num_times == 2 : col1 , col2 = times sub = self [ self [ col1 ]. notnull () &amp ; self [ col2 ]. notnull ()] a , b = sub [ col1 ], sub [ col2 ] if ( a &gt ; b ). all (): col1 , col2 = col2 , col1 elif not ( a &lt ;= b ). all (): raise ValueError ( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\" : col2 , col1 = col1 , col2 self . _start_col , self . _time_col , self . _end_col = col1 , col1 , col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception ( msg + \" Found: \" + str ( times )) def when_at ( self , other , ** window_kwargs ): if isinstance ( other , str ): other = get_type_from_registry ( \"places\" ). containing ( other ) return self . __class__ ( join_time ( other , self , ** window_kwargs )) def browsing ( self , other , ** window_kwargs ): if isinstance ( other , str ): other = get_type_from_registry ( \"browser\" ). containing ( other ) return self . __class__ ( join_time ( other , self , ** window_kwargs )) def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] @property def when_asleep ( self ): return self . __class__ ( join_time ( registry [ \"sleep\" ]. asleep , self )) # browser[\"session\"] = browser.time.diff().apply(lambda x: x.seconds &gt; 20 * 60).cumsum()+10 # session, start, end = zip(*[(name, group.iloc[0].time - pd.Timedelta(minutes=10), group.iloc[-1].time + pd.Timedelta(minutes=10)) for name, group in browser.groupby(\"session\")]) # df = pd.DataFrame({\"session\": session}, index=pd.IntervalIndex.from_arrays(start, end)) # df.index.get_loc(pd.Timestamp(parse_date_tz(\"2019-09-18 18:40:56.729633\").start_date)) def col_contains ( self , string , col_name , case = False , regex = False , na = False ): return self [ self [ col_name ]. str . contains ( string , case = case , regex = regex , na = na )] def __getitem__ ( self , key ): new = super (). __getitem__ ( key ) if isinstance ( new , pd . DataFrame ): new = self . __class__ ( new ) return new @property def text_cols ( self ): return [ x for x , t in zip ( self . columns , self . dtypes ) if t == np . dtype ( 'O' )] @nlp ( \"filter\" , \"by me\" , \"i\" , \"my\" ) def by_me ( self ): return self def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r \"\\b\" + string + r \"\\b\" if col_name is not None: return self . col_contains ( string , col_name , case , regex , na ) bool_cols = [ self [ x ]. str . contains ( string , case = case , regex = regex , na = na ) for x in self . text_cols ] bool_array = bool_cols [ 0 ] for b in bool_cols [ 1 :]: bool_array = np . logical_or ( bool_array , b ) return self . __class__ ( self [ bool_array ]) def query ( self , expr ): return self . __class__ ( super (). query ( expr )) def as_simple ( self , max_n = None ): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [ \"title\" , \"name\" , \"naam\" , \"subject\" , \"url\" , \"content\" , \"text\" , \"value\" ]: res = getattr ( self , x , None ) if res is not None: data [ \"title\" ] = res break res = getattr ( self , \"sender\" , None ) if res is not None: data [ \"sender\" ] = res for x in [ \"url\" , \"path\" , \"file\" ]: res = getattr ( self , x , None ) if res is not None: data [ \"url\" ] = res break for x in [ \"start\" , \"time\" , \"timestamp\" ]: res = getattr ( self , x , None ) if res is not None: data [ \"start\" ] = res break for x in [ \"end\" ]: res = getattr ( self , x , None ) if res is not None: data [ \"end\" ] = res - pd . Timedelta ( microseconds = 1 ) break if data [ \"end\" ] is None: data [ \"end\" ] = data [ \"start\" ] + pd . Timedelta ( minutes = 5 ) data [ \"interval\" ] = False try: data = pd . DataFrame ( data ). sort_values ( \"start\" ) if max_n is not None: data = data . iloc [- max_n: ] return data except ValueError: raise ValueError ( f \"No fields are mapped for {self.__class__.__name__}\" ) def _select_at_day ( self , day_or_class ): if isinstance ( day_or_class , pd . DataFrame ): days = day_or_class . time . dt . date . unique () return self . time . dt . date . isin ( days ) elif isinstance ( day_or_class , ( list , tuple , set , pd . Series )): return self . time . dt . date . isin ( set ( day_or_class )) else: mp = parse_date_tz ( day_or_class ) return ( self . time . dt . date &gt ;= mp . start_date . date ()) &amp ; ( self . time . dt . date &lt ; mp . end_date . date () ) def at_day ( self , day_or_class ): return self [ self . _select_at_day ( day_or_class )] def not_at_day ( self , day_or_class ): return self [~ self . _select_at_day ( day_or_class )] @property def last_day ( self ): return self [( self . time &gt ; yesterday ()) &amp ; ( self . time &lt ; now ())] @property def yesterday ( self ): return self [( self . time &gt ; yesterday ()) &amp ; ( self . time &lt ; now ())] @property def last_week ( self ): return self [( self . time &gt ; last_week ()) &amp ; ( self . time &lt ; now ())] @property def last_month ( self ): return self [( self . time &gt ; last_month ()) &amp ; ( self . time &lt ; now ())] @property def last_year ( self ): return self [( self . time &gt ; last_year ()) &amp ; ( self . time &lt ; now ())] def near ( self , s ): if isinstance ( s , NDF ) and s . df_name . endswith ( \"places\" ): selection = s else: selection = get_type_from_registry ( \"places\" ). containing ( s ) return self . when_at ( selection ) def to_place ( self ): results = [] places = get_type_from_registry ( \"places\" ) for time in self . time: try: results . append ( places . iloc [ places . index . get_loc ( time )]. iloc [ 0 ]) except ( TypeError , KeyError ): pass return places . __class__ ( results ) def in_a ( self , s ): return self . near ( s ) def read ( self , index ): return just . read ( self . path [ index ]) def view ( self , index ): view ( self . path [ index ]) def head ( self , * args , ** kwargs ): return self . __class__ ( super (). head (* args , ** kwargs )) def tail ( self , * args , ** kwargs ): return self . __class__ ( super (). tail (* args , ** kwargs )) @nlp ( \"end\" , \"how many\" , \"how many times\" , \"how often\" ) def count ( self ): return self . shape [ 0 ] @property def at_home ( self ): self . take_from ( \"places\" , \"category\" ) return self [ self [ \"places_category\" ] == \"Home\" ] @property def at_work ( self ): self . take_from ( \"places\" , \"category\" ) return self [ self [ \"places_category\" ] == \"Work\" ] def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ): if is_mp ( start ): start = start . start_date end = start . end_date elif isinstance ( start , str ) and end is None: mp = parse_date_tz ( start ) start = mp . start_date end = mp . end_date elif isinstance ( start , str ) and isinstance ( end , str ): mp = parse_date_tz ( start ) start = mp . start_date mp = parse_date_tz ( end ) end = mp . start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError ( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self . infer_time () if window_kwargs: start = start - pd . Timedelta (** window_kwargs ) end = end + pd . Timedelta (** window_kwargs ) if self . _start_col is None: res = self [ ab_overlap_c ( start , end , self [ self . _time_col ])] else: res = self [ ab_overlap_cd ( self [ self . _start_col ], self [ self . _end_col ], start , end )] if not res . empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res [ \"sort_score\" ] = res [ self . _time_col ] res = res . sort_values ( 'sort_score' ). drop ( 'sort_score' , axis = 1 ) return self . __class__ ( res ) when = when_at @property def duration ( self ): return self . end - self . start def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ): return self . __class__ ( pd . DataFrame . sort_values ( self , by , axis , ascending , inplace , kind , na_position ) ) @nlp ( \"filter\" , \"last\" , \"last time\" , \"most recently\" ) def last ( self ): _ = self . time # to get inferred time if not set col = self . _time_col or self . _start_col return self . __class__ ( self . sort_values ( col , na_position = \"last\" , ascending = False ). iloc [: 1 ]) # maybe which/what could be showing only unique? @nlp ( \"end\" , \"show\" , \"show me\" , \"show me the\" , \"show the\" , \"what\" ) def show_me ( self ): _ = self . time # to get inferred time if not set col = self . _time_col or self . _start_col return self . __class__ ( self . sort_values ( col , na_position = \"last\" , ascending = False )) def at ( self , time_or_place ): if isinstance ( time_or_place , NDF ) and time_or_place . df_name . endswith ( \"places\" ): return self . when_at ( time_or_place ) if isinstance ( time_or_place , str ): mp = parse_date_tz ( time_or_place ) if mp: start = mp . start_date end = mp . end_date return self . at_time ( start , end ) else: return self . when_at ( get_type_from_registry ( \"places\" ). containing ( time_or_place )) raise ValueError ( \"neither time nor place was passed\" ) def to_html ( self ): if self . selected_columns: data = pd . DataFrame ({ x: getattr(self, x) for x in self.selected_columns }) return data . to_html () return super (). to_html () def get_type_from_registry ( self , tp ): for key , value in rergistry . items (): if key . endswith ( tp ): return value def __call_ ( self ): return self","title":"NDF"},{"location":"reference/nostalgia/ndf/#descendants","text":"nostalgia.ndf.Results nostalgia.interfaces.chat.ChatInterface nostalgia.interfaces.places.Places nostalgia.interfaces.post.PostInterface nostalgia.sources.chrome_history.WebHistory nostalgia.sources.emacs_file_visits.FileVisits nostalgia.sources.facebook.Facebook nostalgia.sources.fitbit.heartrate.FitbitHeartrate nostalgia.sources.fitbit.sleep.FitbitSleep nostalgia.sources.google.Google nostalgia.sources.ing_banking.mijn_ing.Payments nostalgia.sources.mijn_chipkaart.MijnChipkaart nostalgia.sources.samsung.Samsung nostalgia.sources.samsung.heartrate.SamsungHeartrate nostalgia.sources.samsung.stress.SamsungStress nostalgia.sources.screenshots.Screenshots nostalgia.sources.shazam.Shazam nostalgia.sources.sleepcycle.SleepCycle nostalgia.sources.web.linked_events.Events nostalgia.sources.web.linked_google_search.GoogleSearch nostalgia.sources.web.linked_offers.Offers nostalgia.sources.web.linked_person.Person nostalgia.sources.web.linked_videos.Videos nostalgia.sources.web.videos_watched.VideosWatched nostalgia.sources.whereami.Whereami","title":"Descendants"},{"location":"reference/nostalgia/ndf/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/ndf/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/ndf/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/ndf/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/ndf/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/ndf/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/ndf/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/ndf/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/ndf/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/ndf/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/ndf/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/ndf/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/ndf/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/ndf/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/ndf/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/ndf/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/ndf/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/ndf/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/ndf/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/ndf/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/ndf/#get_type_from_registry_1","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/ndf/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/ndf/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/ndf/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/ndf/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/ndf/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/ndf/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/ndf/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/ndf/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/ndf/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/ndf/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/ndf/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/ndf/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/ndf/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/ndf/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/ndf/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/ndf/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/ndf/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/ndf/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/ndf/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/ndf/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/ndf/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/ndf/#results","text":"class Results ( data ) View Source class Results ( NDF ) : @classmethod def merge ( cls , * dfs , max_n = None ) : data = pd . concat ([ x . as_simple ( max_n ) for x in dfs ]) data = data . set_index ( \"start\" ) data = data . sort_values ( \"start\" , na_position = \"first\" ) data [ \"start\" ] = data . index return Results ( data ) def get_original ( self , index_loc ) : row = self . iloc [ index_loc ] rec = registry [ row . type ]. loc [ row [ \"index_loc\" ]] # rec . add_heartrate () # not here , but at the results level .. but then just to display # if a certain value if not isinstance ( rec , pd . Series ) : rec = rec . iloc [ 0 ] return rec","title":"Results"},{"location":"reference/nostalgia/ndf/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/ndf/#class-variables_1","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/ndf/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/nostalgia/ndf/#df_label_1","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/ndf/#get_schema_1","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/ndf/#ingest_1","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/ndf/#merge","text":"def merge ( * dfs , max_n = None ) View Source @classmethod def merge(cls, *dfs, max_n=None): data = pd.concat([x.as_simple(max_n) for x in dfs]) data = data.set_index(\"start\") data = data.sort_values(\"start\", na_position=\"first\") data[\"start\"] = data.index return Results(data)","title":"merge"},{"location":"reference/nostalgia/ndf/#instance-variables_1","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/ndf/#methods_1","text":"","title":"Methods"},{"location":"reference/nostalgia/ndf/#add_heartrate_1","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/ndf/#as_simple_1","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/ndf/#at_1","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/ndf/#at_day_1","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/ndf/#at_night_1","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/ndf/#at_time_1","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/ndf/#browsing_1","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/ndf/#by_me_1","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/ndf/#col_contains_1","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/ndf/#containing_1","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/ndf/#count_1","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/ndf/#duration_longer_than_1","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/ndf/#duration_shorter_than_1","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/ndf/#get_original","text":"def get_original ( self , index_loc ) View Source def get_original(self, index_loc): row = self.iloc[index_loc] rec = registry[row.type].loc[row[\"index_loc\"]] # rec.add_heartrate() # not here, but at the results level.. but then just to display # if a certain value if not isinstance(rec, pd.Series): rec = rec.iloc[0] return rec","title":"get_original"},{"location":"reference/nostalgia/ndf/#get_type_from_registry_2","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/ndf/#head_1","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/ndf/#heartrate_above_1","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/ndf/#heartrate_below_1","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/ndf/#heartrate_range_1","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/ndf/#in_a_1","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/ndf/#infer_time_1","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/ndf/#last_1","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/ndf/#near_1","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/ndf/#not_at_day_1","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/ndf/#query_1","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/ndf/#read_1","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/ndf/#show_me_1","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/ndf/#sort_values_1","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/ndf/#tail_1","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/ndf/#take_from_1","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/ndf/#time_level_1","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/ndf/#to_html_1","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/ndf/#to_place_1","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/ndf/#view_1","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/ndf/#when_1","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/ndf/#when_at_1","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/nlp/","text":"Module nostalgia.nlp View Source import inspect from collections import defaultdict from functools import wraps from nostalgia.times import parse_date_tz nlp_registry = defaultdict ( set ) regex_registry = defaultdict ( set ) ts = None # from nearnlp import Near # n = Near() n = None class ResultInfo ( object ): __slots__ = ( 'cls' , 'role' , 'cls_fn' , 'col_name' , 'orig_word' ) def __init__ ( self , cls , role , cls_fn = None , col_name = None , orig_word = None ): self . cls = cls self . role = role self . cls_fn = cls_fn self . col_name = col_name self . orig_word = orig_word def repr_wrapper ( self , x ): v = None if x == \"cls_fn\" : try : v = str ( self . cls_fn ) . split ()[ 1 ] . split ( \".\" )[ - 1 ] except IndexError : pass if v is None : v = getattr ( self , x ) return v @property def tp ( self ): if isinstance ( self . cls , str ): return self . cls . split ( \".\" )[ 0 ] return self . cls . __name__ def __repr__ ( self ): try : c = getattr ( self , \"cls\" ) . __name__ except AttributeError : c = getattr ( self , \"cls\" ) return \"!{}({})\" . format ( c , \", \" . join ( [ \"{}={!r}\" . format ( x , self . repr_wrapper ( x )) for x in self . __slots__ if x != \"cls\" ] ), ) def __hash__ ( self ): return hash (( self . cls , self . col_name , self . orig_word )) def __eq__ ( self , o ): return ( self . cls , self . col_name , self . orig_word ) == ( o . cls , o . col_name , o . orig_word ) COLUMN_BLACKLIST = set ([ \"did\" , \"was\" , \"the\" , \"a\" , \"are\" , \"i\" , \"from\" , \"hard\" , \"for\" ]) def att_getter ( fn ): def att_inner ( x , * args ): try : return getattr ( x , fn . __name__ )( * args ) except TypeError : return getattr ( x , fn . __name__ )() return att_inner # has to be completed still def regex ( role , * keywords ): def real_decorator ( fn ): for keyword in keywords : if isinstance ( keyword , str ): nlp_registry [ keyword ] . add ( ResultInfo ( fn . __qualname__ , role , att_getter ( fn ), orig_word = keyword ) ) else : for kw in keyword : nlp_registry [ kw ] . add ( ResultInfo ( fn . __qualname__ , role , att_getter ( fn ), orig_word = kw ) ) @wraps ( fn ) def wrapper ( * args , ** kwargs ): return fn ( * args , ** kwargs ) return wrapper return real_decorator def nlp ( role , * keywords ): def real_decorator ( fn ): # this does not work!!!! # is_prop = inspect.isdatadescriptor(fn) # defin = \", \".join([x for x in inspect.getfullargspec(fn).args if x != \"self\"]) # if not is_prop: # defin = \"({})\".format(defin) for keyword in keywords : if isinstance ( keyword , str ): nlp_registry [ keyword ] . add ( ResultInfo ( fn . __qualname__ , role , att_getter ( fn ), orig_word = keyword ) ) else : for kw in keyword : nlp_registry [ kw ] . add ( ResultInfo ( fn . __qualname__ , role , att_getter ( fn ), orig_word = kw ) ) @wraps ( fn ) def wrapper ( * args , ** kwargs ): return fn ( * args , ** kwargs ) return wrapper return real_decorator from textsearch import TextSearch def get_ts (): ts = TextSearch ( \"insensitive\" , \"object\" ) ts . add ( nlp_registry ) return ts def at_time_wrapper ( mp ): def at_time ( x ): return x . at_time ( mp ) return at_time def find_entities ( sentence ): global ts if ts is None : ts = get_ts () mp = parse_date_tz ( sentence ) try : ents = ts . findall ( sentence ) except AttributeError : raise AttributeError ( \"No entities have been registered\" ) # remove metaperiod tokens from otherwise matching if mp is not None : wrongs = set () for l , e in mp . spans : wrongs . update ( range ( l , e )) ents = [ x for x in ents if x . start not in wrongs and x . end not in wrongs ] ents . append ( ResultInfo ( \"MP\" , \"filter\" , at_time_wrapper ( mp ), orig_word = \" \" . join ( mp . matches ))) return ents Variables COLUMN_BLACKLIST n nlp_registry regex_registry ts Functions at_time_wrapper def at_time_wrapper ( mp ) View Source def at_time_wrapper(mp): def at_time(x): return x.at_time(mp) return at_time att_getter def att_getter ( fn ) View Source def att_getter(fn): def att_inner(x, *args): try: return getattr(x, fn.__name__)(*args) except TypeError: return getattr(x, fn.__name__)() return att_inner find_entities def find_entities ( sentence ) View Source def find_entities(sentence): global ts if ts is None: ts = get_ts() mp = parse_date_tz(sentence) try: ents = ts.findall(sentence) except AttributeError: raise AttributeError(\"No entities have been registered\") # remove metaperiod tokens from otherwise matching if mp is not None: wrongs = set() for l, e in mp.spans: wrongs.update(range(l, e)) ents = [x for x in ents if x.start not in wrongs and x.end not in wrongs] ents.append(ResultInfo(\"MP\", \"filter\", at_time_wrapper(mp), orig_word=\" \".join(mp.matches))) return ents get_ts def get_ts ( ) View Source def get_ts(): ts = TextSearch(\"insensitive\", \"object\") ts.add(nlp_registry) return ts nlp def nlp ( role , * keywords ) View Source def nlp(role, *keywords): def real_decorator(fn): # this does not work!!!! # is_prop = inspect.isdatadescriptor(fn) # defin = \", \".join([x for x in inspect.getfullargspec(fn).args if x != \"self\"]) # if not is_prop: # defin = \"({})\".format(defin) for keyword in keywords: if isinstance(keyword, str): nlp_registry[keyword].add( ResultInfo(fn.__qualname__, role, att_getter(fn), orig_word=keyword) ) else: for kw in keyword: nlp_registry[kw].add( ResultInfo(fn.__qualname__, role, att_getter(fn), orig_word=kw) ) @wraps(fn) def wrapper(*args, **kwargs): return fn(*args, **kwargs) return wrapper return real_decorator regex def regex ( role , * keywords ) View Source def regex(role, *keywords): def real_decorator(fn): for keyword in keywords: if isinstance(keyword, str): nlp_registry[keyword].add( ResultInfo(fn.__qualname__, role, att_getter(fn), orig_word=keyword) ) else: for kw in keyword: nlp_registry[kw].add( ResultInfo(fn.__qualname__, role, att_getter(fn), orig_word=kw) ) @wraps(fn) def wrapper(*args, **kwargs): return fn(*args, **kwargs) return wrapper return real_decorator Classes ResultInfo class ResultInfo ( cls , role , cls_fn = None , col_name = None , orig_word = None ) View Source class ResultInfo ( object ) : __slots__ = ( 'cls' , 'role' , 'cls_fn' , 'col_name' , 'orig_word' ) def __init__ ( self , cls , role , cls_fn = None , col_name = None , orig_word = None ) : self . cls = cls self . role = role self . cls_fn = cls_fn self . col_name = col_name self . orig_word = orig_word def repr_wrapper ( self , x ) : v = None if x == \"cls_fn\" : try : v = str ( self . cls_fn ). split ()[ 1 ]. split ( \".\" )[ - 1 ] except IndexError : pass if v is None : v = getattr ( self , x ) return v @property def tp ( self ) : if isinstance ( self . cls , str ) : return self . cls . split ( \".\" )[ 0 ] return self . cls . __name__ def __repr__ ( self ) : try : c = getattr ( self , \"cls\" ). __name__ except AttributeError : c = getattr ( self , \"cls\" ) return \"!{}({})\" . format ( c , \", \" . join ( [ \"{}={!r}\" . format ( x , self . repr_wrapper ( x )) for x in self . __slots__ if x != \"cls\" ] ), ) def __hash__ ( self ) : return hash (( self . cls , self . col_name , self . orig_word )) def __eq__ ( self , o ) : return ( self . cls , self . col_name , self . orig_word ) == ( o . cls , o . col_name , o . orig_word ) Instance variables cls cls_fn col_name orig_word role tp Methods repr_wrapper def repr_wrapper ( self , x ) View Source def repr_wrapper(self, x): v = None if x == \"cls_fn\": try: v = str(self.cls_fn).split()[1].split(\".\")[-1] except IndexError: pass if v is None: v = getattr(self, x) return v","title":"Nlp"},{"location":"reference/nostalgia/nlp/#module-nostalgianlp","text":"View Source import inspect from collections import defaultdict from functools import wraps from nostalgia.times import parse_date_tz nlp_registry = defaultdict ( set ) regex_registry = defaultdict ( set ) ts = None # from nearnlp import Near # n = Near() n = None class ResultInfo ( object ): __slots__ = ( 'cls' , 'role' , 'cls_fn' , 'col_name' , 'orig_word' ) def __init__ ( self , cls , role , cls_fn = None , col_name = None , orig_word = None ): self . cls = cls self . role = role self . cls_fn = cls_fn self . col_name = col_name self . orig_word = orig_word def repr_wrapper ( self , x ): v = None if x == \"cls_fn\" : try : v = str ( self . cls_fn ) . split ()[ 1 ] . split ( \".\" )[ - 1 ] except IndexError : pass if v is None : v = getattr ( self , x ) return v @property def tp ( self ): if isinstance ( self . cls , str ): return self . cls . split ( \".\" )[ 0 ] return self . cls . __name__ def __repr__ ( self ): try : c = getattr ( self , \"cls\" ) . __name__ except AttributeError : c = getattr ( self , \"cls\" ) return \"!{}({})\" . format ( c , \", \" . join ( [ \"{}={!r}\" . format ( x , self . repr_wrapper ( x )) for x in self . __slots__ if x != \"cls\" ] ), ) def __hash__ ( self ): return hash (( self . cls , self . col_name , self . orig_word )) def __eq__ ( self , o ): return ( self . cls , self . col_name , self . orig_word ) == ( o . cls , o . col_name , o . orig_word ) COLUMN_BLACKLIST = set ([ \"did\" , \"was\" , \"the\" , \"a\" , \"are\" , \"i\" , \"from\" , \"hard\" , \"for\" ]) def att_getter ( fn ): def att_inner ( x , * args ): try : return getattr ( x , fn . __name__ )( * args ) except TypeError : return getattr ( x , fn . __name__ )() return att_inner # has to be completed still def regex ( role , * keywords ): def real_decorator ( fn ): for keyword in keywords : if isinstance ( keyword , str ): nlp_registry [ keyword ] . add ( ResultInfo ( fn . __qualname__ , role , att_getter ( fn ), orig_word = keyword ) ) else : for kw in keyword : nlp_registry [ kw ] . add ( ResultInfo ( fn . __qualname__ , role , att_getter ( fn ), orig_word = kw ) ) @wraps ( fn ) def wrapper ( * args , ** kwargs ): return fn ( * args , ** kwargs ) return wrapper return real_decorator def nlp ( role , * keywords ): def real_decorator ( fn ): # this does not work!!!! # is_prop = inspect.isdatadescriptor(fn) # defin = \", \".join([x for x in inspect.getfullargspec(fn).args if x != \"self\"]) # if not is_prop: # defin = \"({})\".format(defin) for keyword in keywords : if isinstance ( keyword , str ): nlp_registry [ keyword ] . add ( ResultInfo ( fn . __qualname__ , role , att_getter ( fn ), orig_word = keyword ) ) else : for kw in keyword : nlp_registry [ kw ] . add ( ResultInfo ( fn . __qualname__ , role , att_getter ( fn ), orig_word = kw ) ) @wraps ( fn ) def wrapper ( * args , ** kwargs ): return fn ( * args , ** kwargs ) return wrapper return real_decorator from textsearch import TextSearch def get_ts (): ts = TextSearch ( \"insensitive\" , \"object\" ) ts . add ( nlp_registry ) return ts def at_time_wrapper ( mp ): def at_time ( x ): return x . at_time ( mp ) return at_time def find_entities ( sentence ): global ts if ts is None : ts = get_ts () mp = parse_date_tz ( sentence ) try : ents = ts . findall ( sentence ) except AttributeError : raise AttributeError ( \"No entities have been registered\" ) # remove metaperiod tokens from otherwise matching if mp is not None : wrongs = set () for l , e in mp . spans : wrongs . update ( range ( l , e )) ents = [ x for x in ents if x . start not in wrongs and x . end not in wrongs ] ents . append ( ResultInfo ( \"MP\" , \"filter\" , at_time_wrapper ( mp ), orig_word = \" \" . join ( mp . matches ))) return ents","title":"Module nostalgia.nlp"},{"location":"reference/nostalgia/nlp/#variables","text":"COLUMN_BLACKLIST n nlp_registry regex_registry ts","title":"Variables"},{"location":"reference/nostalgia/nlp/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/nlp/#at_time_wrapper","text":"def at_time_wrapper ( mp ) View Source def at_time_wrapper(mp): def at_time(x): return x.at_time(mp) return at_time","title":"at_time_wrapper"},{"location":"reference/nostalgia/nlp/#att_getter","text":"def att_getter ( fn ) View Source def att_getter(fn): def att_inner(x, *args): try: return getattr(x, fn.__name__)(*args) except TypeError: return getattr(x, fn.__name__)() return att_inner","title":"att_getter"},{"location":"reference/nostalgia/nlp/#find_entities","text":"def find_entities ( sentence ) View Source def find_entities(sentence): global ts if ts is None: ts = get_ts() mp = parse_date_tz(sentence) try: ents = ts.findall(sentence) except AttributeError: raise AttributeError(\"No entities have been registered\") # remove metaperiod tokens from otherwise matching if mp is not None: wrongs = set() for l, e in mp.spans: wrongs.update(range(l, e)) ents = [x for x in ents if x.start not in wrongs and x.end not in wrongs] ents.append(ResultInfo(\"MP\", \"filter\", at_time_wrapper(mp), orig_word=\" \".join(mp.matches))) return ents","title":"find_entities"},{"location":"reference/nostalgia/nlp/#get_ts","text":"def get_ts ( ) View Source def get_ts(): ts = TextSearch(\"insensitive\", \"object\") ts.add(nlp_registry) return ts","title":"get_ts"},{"location":"reference/nostalgia/nlp/#nlp","text":"def nlp ( role , * keywords ) View Source def nlp(role, *keywords): def real_decorator(fn): # this does not work!!!! # is_prop = inspect.isdatadescriptor(fn) # defin = \", \".join([x for x in inspect.getfullargspec(fn).args if x != \"self\"]) # if not is_prop: # defin = \"({})\".format(defin) for keyword in keywords: if isinstance(keyword, str): nlp_registry[keyword].add( ResultInfo(fn.__qualname__, role, att_getter(fn), orig_word=keyword) ) else: for kw in keyword: nlp_registry[kw].add( ResultInfo(fn.__qualname__, role, att_getter(fn), orig_word=kw) ) @wraps(fn) def wrapper(*args, **kwargs): return fn(*args, **kwargs) return wrapper return real_decorator","title":"nlp"},{"location":"reference/nostalgia/nlp/#regex","text":"def regex ( role , * keywords ) View Source def regex(role, *keywords): def real_decorator(fn): for keyword in keywords: if isinstance(keyword, str): nlp_registry[keyword].add( ResultInfo(fn.__qualname__, role, att_getter(fn), orig_word=keyword) ) else: for kw in keyword: nlp_registry[kw].add( ResultInfo(fn.__qualname__, role, att_getter(fn), orig_word=kw) ) @wraps(fn) def wrapper(*args, **kwargs): return fn(*args, **kwargs) return wrapper return real_decorator","title":"regex"},{"location":"reference/nostalgia/nlp/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/nlp/#resultinfo","text":"class ResultInfo ( cls , role , cls_fn = None , col_name = None , orig_word = None ) View Source class ResultInfo ( object ) : __slots__ = ( 'cls' , 'role' , 'cls_fn' , 'col_name' , 'orig_word' ) def __init__ ( self , cls , role , cls_fn = None , col_name = None , orig_word = None ) : self . cls = cls self . role = role self . cls_fn = cls_fn self . col_name = col_name self . orig_word = orig_word def repr_wrapper ( self , x ) : v = None if x == \"cls_fn\" : try : v = str ( self . cls_fn ). split ()[ 1 ]. split ( \".\" )[ - 1 ] except IndexError : pass if v is None : v = getattr ( self , x ) return v @property def tp ( self ) : if isinstance ( self . cls , str ) : return self . cls . split ( \".\" )[ 0 ] return self . cls . __name__ def __repr__ ( self ) : try : c = getattr ( self , \"cls\" ). __name__ except AttributeError : c = getattr ( self , \"cls\" ) return \"!{}({})\" . format ( c , \", \" . join ( [ \"{}={!r}\" . format ( x , self . repr_wrapper ( x )) for x in self . __slots__ if x != \"cls\" ] ), ) def __hash__ ( self ) : return hash (( self . cls , self . col_name , self . orig_word )) def __eq__ ( self , o ) : return ( self . cls , self . col_name , self . orig_word ) == ( o . cls , o . col_name , o . orig_word )","title":"ResultInfo"},{"location":"reference/nostalgia/nlp/#instance-variables","text":"cls cls_fn col_name orig_word role tp","title":"Instance variables"},{"location":"reference/nostalgia/nlp/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/nlp/#repr_wrapper","text":"def repr_wrapper ( self , x ) View Source def repr_wrapper(self, x): v = None if x == \"cls_fn\": try: v = str(self.cls_fn).split()[1].split(\".\")[-1] except IndexError: pass if v is None: v = getattr(self, x) return v","title":"repr_wrapper"},{"location":"reference/nostalgia/selenium/","text":"Module nostalgia.selenium View Source import getpass from selenium import webdriver from nostalgia.utils import MockCredentials def get_driver ( executable_path = \"chromedriver\" , binary_location = None , headless = False , credentials = None , login_url = None , user_xpath_or_id = \"username\" , password_xpath_or_id = \"password\" , ** options , ): # set options chrome_options = webdriver . ChromeOptions () if headless : chrome_options . add_argument ( 'headless' ) if binary_location is not None : setattr ( chrome_options , \"binary_location\" , binary_location ) for k , v in options . items (): setattr ( chrome_options , k , v ) driver = webdriver . Chrome ( executable_path = executable_path , options = chrome_options ) # login if neccessary if isinstance ( credentials , dict ): credentials = MockCredentials ( credentials ) if credentials is not None : driver . get ( login_url ) if user_xpath_or_id . startswith ( \"/\" ): user_element = driver . find_elements_by_xpath ( user_xpath_or_id )[ 0 ] else : user_element = driver . find_element_by_id ( user_xpath_or_id ) if password_xpath_or_id . startswith ( \"/\" ): password_element = driver . find_elements_by_xpath ( password_xpath_or_id )[ 0 ] else : password_element = driver . find_element_by_id ( password_xpath_or_id ) user_element . send_keys ( credentials . username ) password = credentials . password password_element . send_keys ( password ) # h password_element . submit () return driver Functions get_driver def get_driver ( executable_path = 'chromedriver' , binary_location = None , headless = False , credentials = None , login_url = None , user_xpath_or_id = 'username' , password_xpath_or_id = 'password' , ** options ) View Source def get_driver( executable_path=\"chromedriver\", binary_location=None, headless=False, credentials=None, login_url=None, user_xpath_or_id=\"username\", password_xpath_or_id=\"password\", **options, ): # set options chrome_options = webdriver.ChromeOptions() if headless: chrome_options.add_argument('headless') if binary_location is not None: setattr(chrome_options, \"binary_location\", binary_location) for k, v in options.items(): setattr(chrome_options, k, v) driver = webdriver.Chrome(executable_path=executable_path, options=chrome_options) # login if neccessary if isinstance(credentials, dict): credentials = MockCredentials(credentials) if credentials is not None: driver.get(login_url) if user_xpath_or_id.startswith(\"/\"): user_element = driver.find_elements_by_xpath(user_xpath_or_id)[0] else: user_element = driver.find_element_by_id(user_xpath_or_id) if password_xpath_or_id.startswith(\"/\"): password_element = driver.find_elements_by_xpath(password_xpath_or_id)[0] else: password_element = driver.find_element_by_id(password_xpath_or_id) user_element.send_keys(credentials.username) password = credentials.password password_element.send_keys(password) # h password_element.submit() return driver","title":"Selenium"},{"location":"reference/nostalgia/selenium/#module-nostalgiaselenium","text":"View Source import getpass from selenium import webdriver from nostalgia.utils import MockCredentials def get_driver ( executable_path = \"chromedriver\" , binary_location = None , headless = False , credentials = None , login_url = None , user_xpath_or_id = \"username\" , password_xpath_or_id = \"password\" , ** options , ): # set options chrome_options = webdriver . ChromeOptions () if headless : chrome_options . add_argument ( 'headless' ) if binary_location is not None : setattr ( chrome_options , \"binary_location\" , binary_location ) for k , v in options . items (): setattr ( chrome_options , k , v ) driver = webdriver . Chrome ( executable_path = executable_path , options = chrome_options ) # login if neccessary if isinstance ( credentials , dict ): credentials = MockCredentials ( credentials ) if credentials is not None : driver . get ( login_url ) if user_xpath_or_id . startswith ( \"/\" ): user_element = driver . find_elements_by_xpath ( user_xpath_or_id )[ 0 ] else : user_element = driver . find_element_by_id ( user_xpath_or_id ) if password_xpath_or_id . startswith ( \"/\" ): password_element = driver . find_elements_by_xpath ( password_xpath_or_id )[ 0 ] else : password_element = driver . find_element_by_id ( password_xpath_or_id ) user_element . send_keys ( credentials . username ) password = credentials . password password_element . send_keys ( password ) # h password_element . submit () return driver","title":"Module nostalgia.selenium"},{"location":"reference/nostalgia/selenium/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/selenium/#get_driver","text":"def get_driver ( executable_path = 'chromedriver' , binary_location = None , headless = False , credentials = None , login_url = None , user_xpath_or_id = 'username' , password_xpath_or_id = 'password' , ** options ) View Source def get_driver( executable_path=\"chromedriver\", binary_location=None, headless=False, credentials=None, login_url=None, user_xpath_or_id=\"username\", password_xpath_or_id=\"password\", **options, ): # set options chrome_options = webdriver.ChromeOptions() if headless: chrome_options.add_argument('headless') if binary_location is not None: setattr(chrome_options, \"binary_location\", binary_location) for k, v in options.items(): setattr(chrome_options, k, v) driver = webdriver.Chrome(executable_path=executable_path, options=chrome_options) # login if neccessary if isinstance(credentials, dict): credentials = MockCredentials(credentials) if credentials is not None: driver.get(login_url) if user_xpath_or_id.startswith(\"/\"): user_element = driver.find_elements_by_xpath(user_xpath_or_id)[0] else: user_element = driver.find_element_by_id(user_xpath_or_id) if password_xpath_or_id.startswith(\"/\"): password_element = driver.find_elements_by_xpath(password_xpath_or_id)[0] else: password_element = driver.find_element_by_id(password_xpath_or_id) user_element.send_keys(credentials.username) password = credentials.password password_element.send_keys(password) # h password_element.submit() return driver","title":"get_driver"},{"location":"reference/nostalgia/times/","text":"Module nostalgia.times View Source from pytz import timezone from pytz.reference import Local from metadate import parse_date , Units from datetime import datetime from dateutil.parser import parse from dateutil.relativedelta import relativedelta import dateutil # tz = timezone('Europe/Amsterdam') tz = timezone ( Local . tzname ( datetime . now ())) utc = timezone ( 'UTC' ) def now ( ** kwargs ): return datetime . now ( tz = tz ) - relativedelta ( ** kwargs ) def last_days ( days ): return now ( days = days ) . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) def today (): return last_days ( 0 ) def yesterday (): return last_days ( 1 ) def last_week (): return now ( days = 7 ) . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) def last_month (): return now ( months = 1 ) . replace ( day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) def last_year (): return now ( years = 1 ) . replace ( month = 1 , day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) def week_ago ( weeks ): begin = now ( days = 7 * weeks ) . date () end = now ( days = 7 * ( weeks - 1 )) . date () return begin , end def days_ago ( days ): begin = now ( days = days ) . date () end = now ( days = days - 1 ) . date () return lambda : ( begin , end ) def months_ago ( months ): begin = now ( months = months ) . date () end = now ( months = months - 1 ) . date () return lambda : ( begin , end ) def years_ago ( years ): begin = now ( years = years ) . date () end = now ( years = years - 1 ) . date () return lambda : ( begin , end ) def in_month ( month ): year = now () . year - 1 if now () . month & lt ; = month else now () . year return lambda : ( datetime ( year , month , 1 ) . date (), datetime ( year , month , 1 ) . date () + relativedelta ( month = month + 1 ), ) def in_year ( year ): return lambda : ( datetime ( year , 1 , 1 ) . date (), datetime ( year + 1 , 1 , 1 ) . date ()) def try_date ( x , min_level = None , max_level = None ): try : # apparently one col is dayfirst, the other isnt mps = parse_date ( x , multi = True ) for mp in mps : if mp is None : continue if Units . DAY not in mp . levels : continue if min_level is not None and mp . min_level & gt ; min_level : # above minute level continue if max_level is not None and mp . max_level & lt ; max_level : continue date = mp . start_date if date . tzinfo is None : date = tz . localize ( date ) return date except : return None def parse_date_tz ( text ): mp = parse_date ( text ) if mp is not None : if mp . start_date . tzinfo is None : mp . start_date = tz . localize ( mp . start_date ) if mp . end_date . tzinfo is None : mp . end_date = tz . localize ( mp . end_date ) return mp def datetime_tz ( * args ): return tz . localize ( datetime ( * args )) def datetime_from_timestamp ( x , tzone = tz ): # would be year 3000 if x & gt ; 32503683600 : x = x // 1000 if isinstance ( tzone , str ): tzone = timezone ( tzone ) x = datetime . fromtimestamp ( x , tz = tzone ) if tzone != tz : x = x . astimezone ( tz ) return x def datetime_from_format ( s , fmt , in_utc = False ): base = datetime . strptime ( s , fmt ) if in_utc : return utc . localize ( base ) . astimezone ( tz ) else : return tz . localize ( base ) Variables tz utc Functions datetime_from_format def datetime_from_format ( s , fmt , in_utc = False ) View Source def datetime_from_format(s, fmt, in_utc=False): base = datetime.strptime(s, fmt) if in_utc: return utc.localize(base).astimezone(tz) else: return tz.localize(base) datetime_from_timestamp def datetime_from_timestamp ( x , tzone =< DstTzInfo 'CET' CET + 1 : 00 : 00 STD > ) View Source def datetime_from_timestamp(x, tzone=tz): # would be year 3000 if x &gt; 32503683600: x = x // 1000 if isinstance(tzone, str): tzone = timezone(tzone) x = datetime.fromtimestamp(x, tz=tzone) if tzone != tz: x = x.astimezone(tz) return x datetime_tz def datetime_tz ( * args ) View Source def datetime_tz(*args): return tz.localize(datetime(*args)) days_ago def days_ago ( days ) View Source def days_ago(days): begin = now(days=days).date() end = now(days=days - 1).date() return lambda: (begin, end) in_month def in_month ( month ) View Source def in_month(month): year = now().year - 1 if now().month &lt;= month else now().year return lambda: ( datetime(year, month, 1).date(), datetime(year, month, 1).date() + relativedelta(month=month + 1), ) in_year def in_year ( year ) View Source def in_year(year): return lambda: (datetime(year, 1, 1).date(), datetime(year + 1, 1, 1).date()) last_days def last_days ( days ) View Source def last_days(days): return now(days=days).replace(hour=0, minute=0, second=0, microsecond=0) last_month def last_month ( ) View Source def last_month(): return now(months=1).replace(day=1, hour=0, minute=0, second=0, microsecond=0) last_week def last_week ( ) View Source def last_week(): return now(days=7).replace(hour=0, minute=0, second=0, microsecond=0) last_year def last_year ( ) View Source def last_year(): return now(years=1).replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0) months_ago def months_ago ( months ) View Source def months_ago(months): begin = now(months=months).date() end = now(months=months - 1).date() return lambda: (begin, end) now def now ( ** kwargs ) View Source def now(**kwargs): return datetime.now(tz=tz) - relativedelta(**kwargs) parse_date_tz def parse_date_tz ( text ) View Source def parse_date_tz(text): mp = parse_date(text) if mp is not None: if mp.start_date.tzinfo is None: mp.start_date = tz.localize(mp.start_date) if mp.end_date.tzinfo is None: mp.end_date = tz.localize(mp.end_date) return mp today def today ( ) View Source def today(): return last_days(0) try_date def try_date ( x , min_level = None , max_level = None ) View Source def try_date(x, min_level=None, max_level=None): try: # apparently one col is dayfirst, the other isnt mps = parse_date(x, multi=True) for mp in mps: if mp is None: continue if Units.DAY not in mp.levels: continue if min_level is not None and mp.min_level &gt; min_level: # above minute level continue if max_level is not None and mp.max_level &lt; max_level: continue date = mp.start_date if date.tzinfo is None: date = tz.localize(date) return date except: return None week_ago def week_ago ( weeks ) View Source def week_ago(weeks): begin = now(days=7 * weeks).date() end = now(days=7 * (weeks - 1)).date() return begin, end years_ago def years_ago ( years ) View Source def years_ago(years): begin = now(years=years).date() end = now(years=years - 1).date() return lambda: (begin, end) yesterday def yesterday ( ) View Source def yesterday(): return last_days(1)","title":"Times"},{"location":"reference/nostalgia/times/#module-nostalgiatimes","text":"View Source from pytz import timezone from pytz.reference import Local from metadate import parse_date , Units from datetime import datetime from dateutil.parser import parse from dateutil.relativedelta import relativedelta import dateutil # tz = timezone('Europe/Amsterdam') tz = timezone ( Local . tzname ( datetime . now ())) utc = timezone ( 'UTC' ) def now ( ** kwargs ): return datetime . now ( tz = tz ) - relativedelta ( ** kwargs ) def last_days ( days ): return now ( days = days ) . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) def today (): return last_days ( 0 ) def yesterday (): return last_days ( 1 ) def last_week (): return now ( days = 7 ) . replace ( hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) def last_month (): return now ( months = 1 ) . replace ( day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) def last_year (): return now ( years = 1 ) . replace ( month = 1 , day = 1 , hour = 0 , minute = 0 , second = 0 , microsecond = 0 ) def week_ago ( weeks ): begin = now ( days = 7 * weeks ) . date () end = now ( days = 7 * ( weeks - 1 )) . date () return begin , end def days_ago ( days ): begin = now ( days = days ) . date () end = now ( days = days - 1 ) . date () return lambda : ( begin , end ) def months_ago ( months ): begin = now ( months = months ) . date () end = now ( months = months - 1 ) . date () return lambda : ( begin , end ) def years_ago ( years ): begin = now ( years = years ) . date () end = now ( years = years - 1 ) . date () return lambda : ( begin , end ) def in_month ( month ): year = now () . year - 1 if now () . month & lt ; = month else now () . year return lambda : ( datetime ( year , month , 1 ) . date (), datetime ( year , month , 1 ) . date () + relativedelta ( month = month + 1 ), ) def in_year ( year ): return lambda : ( datetime ( year , 1 , 1 ) . date (), datetime ( year + 1 , 1 , 1 ) . date ()) def try_date ( x , min_level = None , max_level = None ): try : # apparently one col is dayfirst, the other isnt mps = parse_date ( x , multi = True ) for mp in mps : if mp is None : continue if Units . DAY not in mp . levels : continue if min_level is not None and mp . min_level & gt ; min_level : # above minute level continue if max_level is not None and mp . max_level & lt ; max_level : continue date = mp . start_date if date . tzinfo is None : date = tz . localize ( date ) return date except : return None def parse_date_tz ( text ): mp = parse_date ( text ) if mp is not None : if mp . start_date . tzinfo is None : mp . start_date = tz . localize ( mp . start_date ) if mp . end_date . tzinfo is None : mp . end_date = tz . localize ( mp . end_date ) return mp def datetime_tz ( * args ): return tz . localize ( datetime ( * args )) def datetime_from_timestamp ( x , tzone = tz ): # would be year 3000 if x & gt ; 32503683600 : x = x // 1000 if isinstance ( tzone , str ): tzone = timezone ( tzone ) x = datetime . fromtimestamp ( x , tz = tzone ) if tzone != tz : x = x . astimezone ( tz ) return x def datetime_from_format ( s , fmt , in_utc = False ): base = datetime . strptime ( s , fmt ) if in_utc : return utc . localize ( base ) . astimezone ( tz ) else : return tz . localize ( base )","title":"Module nostalgia.times"},{"location":"reference/nostalgia/times/#variables","text":"tz utc","title":"Variables"},{"location":"reference/nostalgia/times/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/times/#datetime_from_format","text":"def datetime_from_format ( s , fmt , in_utc = False ) View Source def datetime_from_format(s, fmt, in_utc=False): base = datetime.strptime(s, fmt) if in_utc: return utc.localize(base).astimezone(tz) else: return tz.localize(base)","title":"datetime_from_format"},{"location":"reference/nostalgia/times/#datetime_from_timestamp","text":"def datetime_from_timestamp ( x , tzone =< DstTzInfo 'CET' CET + 1 : 00 : 00 STD > ) View Source def datetime_from_timestamp(x, tzone=tz): # would be year 3000 if x &gt; 32503683600: x = x // 1000 if isinstance(tzone, str): tzone = timezone(tzone) x = datetime.fromtimestamp(x, tz=tzone) if tzone != tz: x = x.astimezone(tz) return x","title":"datetime_from_timestamp"},{"location":"reference/nostalgia/times/#datetime_tz","text":"def datetime_tz ( * args ) View Source def datetime_tz(*args): return tz.localize(datetime(*args))","title":"datetime_tz"},{"location":"reference/nostalgia/times/#days_ago","text":"def days_ago ( days ) View Source def days_ago(days): begin = now(days=days).date() end = now(days=days - 1).date() return lambda: (begin, end)","title":"days_ago"},{"location":"reference/nostalgia/times/#in_month","text":"def in_month ( month ) View Source def in_month(month): year = now().year - 1 if now().month &lt;= month else now().year return lambda: ( datetime(year, month, 1).date(), datetime(year, month, 1).date() + relativedelta(month=month + 1), )","title":"in_month"},{"location":"reference/nostalgia/times/#in_year","text":"def in_year ( year ) View Source def in_year(year): return lambda: (datetime(year, 1, 1).date(), datetime(year + 1, 1, 1).date())","title":"in_year"},{"location":"reference/nostalgia/times/#last_days","text":"def last_days ( days ) View Source def last_days(days): return now(days=days).replace(hour=0, minute=0, second=0, microsecond=0)","title":"last_days"},{"location":"reference/nostalgia/times/#last_month","text":"def last_month ( ) View Source def last_month(): return now(months=1).replace(day=1, hour=0, minute=0, second=0, microsecond=0)","title":"last_month"},{"location":"reference/nostalgia/times/#last_week","text":"def last_week ( ) View Source def last_week(): return now(days=7).replace(hour=0, minute=0, second=0, microsecond=0)","title":"last_week"},{"location":"reference/nostalgia/times/#last_year","text":"def last_year ( ) View Source def last_year(): return now(years=1).replace(month=1, day=1, hour=0, minute=0, second=0, microsecond=0)","title":"last_year"},{"location":"reference/nostalgia/times/#months_ago","text":"def months_ago ( months ) View Source def months_ago(months): begin = now(months=months).date() end = now(months=months - 1).date() return lambda: (begin, end)","title":"months_ago"},{"location":"reference/nostalgia/times/#now","text":"def now ( ** kwargs ) View Source def now(**kwargs): return datetime.now(tz=tz) - relativedelta(**kwargs)","title":"now"},{"location":"reference/nostalgia/times/#parse_date_tz","text":"def parse_date_tz ( text ) View Source def parse_date_tz(text): mp = parse_date(text) if mp is not None: if mp.start_date.tzinfo is None: mp.start_date = tz.localize(mp.start_date) if mp.end_date.tzinfo is None: mp.end_date = tz.localize(mp.end_date) return mp","title":"parse_date_tz"},{"location":"reference/nostalgia/times/#today","text":"def today ( ) View Source def today(): return last_days(0)","title":"today"},{"location":"reference/nostalgia/times/#try_date","text":"def try_date ( x , min_level = None , max_level = None ) View Source def try_date(x, min_level=None, max_level=None): try: # apparently one col is dayfirst, the other isnt mps = parse_date(x, multi=True) for mp in mps: if mp is None: continue if Units.DAY not in mp.levels: continue if min_level is not None and mp.min_level &gt; min_level: # above minute level continue if max_level is not None and mp.max_level &lt; max_level: continue date = mp.start_date if date.tzinfo is None: date = tz.localize(date) return date except: return None","title":"try_date"},{"location":"reference/nostalgia/times/#week_ago","text":"def week_ago ( weeks ) View Source def week_ago(weeks): begin = now(days=7 * weeks).date() end = now(days=7 * (weeks - 1)).date() return begin, end","title":"week_ago"},{"location":"reference/nostalgia/times/#years_ago","text":"def years_ago ( years ) View Source def years_ago(years): begin = now(years=years).date() end = now(years=years - 1).date() return lambda: (begin, end)","title":"years_ago"},{"location":"reference/nostalgia/times/#yesterday","text":"def yesterday ( ) View Source def yesterday(): return last_days(1)","title":"yesterday"},{"location":"reference/nostalgia/unzip/","text":"Module nostalgia.unzip View Source import tqdm import sys import just import zipfile if __name__ == \"__main__\" : for x in tqdm . tqdm ( sys . argv [ 1 :]): with zipfile . ZipFile ( x , \"r\" ) as zip_ref : zip_ref . extractall () just . remove ( x )","title":"Unzip"},{"location":"reference/nostalgia/unzip/#module-nostalgiaunzip","text":"View Source import tqdm import sys import just import zipfile if __name__ == \"__main__\" : for x in tqdm . tqdm ( sys . argv [ 1 :]): with zipfile . ZipFile ( x , \"r\" ) as zip_ref : zip_ref . extractall () just . remove ( x )","title":"Module nostalgia.unzip"},{"location":"reference/nostalgia/utils/","text":"Module nostalgia.utils View Source import os import sys import re import just import pandas as pd import numpy as np from tok import word_tokenize if sys . argv [ - 1 ] . endswith ( \"ipython\" ): pd . set_option ( \"display.precision\" , 10 ) pd . set_option ( \"precision\" , 10 ) pd . set_option ( 'display.max_columns' , 500 ) pd . set_option ( 'display.max_colwidth' , - 1 ) has_alpha = re . compile ( \"[a-zA-Z]\" ) num = re . compile ( \"[0-9]\" ) class MockCredentials : def __init__ ( self , credential_dict ): for k , v in credential_dict . items (): setattr ( self , k , v ) def get_tokens ( sentence , lower = True ): if not isinstance ( sentence , str ): return [] if sentence is None : return [] return [ x for x in word_tokenize ( sentence , to_lower = lower ) if len ( x ) & gt ; 2 and has_alpha . search ( x ) and not num . search ( x ) ] def get_token_set ( iterable_of_sentences , lower = True ): words = set () for x in iterable_of_sentences : words . update ( get_tokens ( x , lower )) return words def format_latlng ( latlng ): if isinstance ( latlng , str ): latlng = latlng . replace ( \",\" , \" \" ) . split () return \"{:0.7f}, {:0.7f}\" . format ( * map ( float , latlng )) def view ( path ): from requests_viewer import view_html view_html ( just . read ( path )) def haversine ( lat1 , lon1 , lat2 , lon2 ): \"\"\" Calculate the great circle distance between two points on the earth (specified in decimal degrees) currently in meters \"\"\" # Convert decimal degrees to Radians: lon1 = np . radians ( lon1 ) lat1 = np . radians ( lat1 ) lon2 = np . radians ( lon2 ) lat2 = np . radians ( lat2 ) # Implementing Haversine Formula: dlon = np . subtract ( lon2 , lon1 ) dlat = np . subtract ( lat2 , lat1 ) a = np . add ( np . power ( np . sin ( np . divide ( dlat , 2 )), 2 ), np . multiply ( np . cos ( lat1 ), np . multiply ( np . cos ( lat2 ), np . power ( np . sin ( np . divide ( dlon , 2 )), 2 )) ), ) c = np . multiply ( 2 , np . arcsin ( np . sqrt ( a ))) # km r = 6371 # meters r *= 1000 return c * r seps = [ \".\" , \",\" ] def parse_price ( x ): if x is None : return None if isinstance ( x , list ): x = \"\" . join ( x ) if isinstance ( x , ( int , float )): if x != x : return None return x x = x . replace ( \" \" , \"\" ) try : return float ( x ) except ValueError : pass result = np . nan for decimal_sep in seps : for thousand_sep in seps : if decimal_sep == thousand_sep : continue regex = \"^[+-]?\" regex += \"[0-9]{1,3}\" regex += \"(?:[\" + thousand_sep + \"]?[0-9]{3})*\" regex += \"([\" + decimal_sep + \"][0-9]{1,2})?\" regex += \"$\" if re . match ( regex , x ): try : result = float ( x . replace ( thousand_sep , \"\" ) . replace ( decimal_sep , \".\" )) except ValueError : continue return result return result def normalize_name ( name ): return re . sub ( \"([A-Z])\" , r \"_\\1\" , name ) . lstrip ( \"_\" ) . lower () def clean_df ( df , dc ): for k , v in dc . items (): df [ k ] = [ v ( x ) for x in df [ k ]] return df def load_entry (): import sys just . write ( \"\" , \"~/nostalgia_data/__init__.py\" ) sys . path . append ( os . path . expanduser ( \"~/nostalgia_data/\" )) import nostalgia_entry return nostalgia_entry Variables has_alpha num seps Functions clean_df def clean_df ( df , dc ) View Source def clean_df(df, dc): for k, v in dc.items(): df[k] = [v(x) for x in df[k]] return df format_latlng def format_latlng ( latlng ) View Source def format_latlng(latlng): if isinstance(latlng, str): latlng = latlng.replace(\",\", \" \").split() return \"{:0.7f}, {:0.7f}\".format(*map(float, latlng)) get_token_set def get_token_set ( iterable_of_sentences , lower = True ) View Source def get_token_set(iterable_of_sentences, lower=True): words = set() for x in iterable_of_sentences: words.update(get_tokens(x, lower)) return words get_tokens def get_tokens ( sentence , lower = True ) View Source def get_tokens(sentence, lower=True): if not isinstance(sentence, str): return [] if sentence is None: return [] return [ x for x in word_tokenize(sentence, to_lower=lower) if len(x) &gt; 2 and has_alpha.search(x) and not num.search(x) ] haversine def haversine ( lat1 , lon1 , lat2 , lon2 ) Calculate the great circle distance between two points on the earth (specified in decimal degrees) currently in meters View Source def haversine(lat1, lon1, lat2, lon2): \"\"\" Calculate the great circle distance between two points on the earth (specified in decimal degrees) currently in meters \"\"\" # Convert decimal degrees to Radians: lon1 = np.radians(lon1) lat1 = np.radians(lat1) lon2 = np.radians(lon2) lat2 = np.radians(lat2) # Implementing Haversine Formula: dlon = np.subtract(lon2, lon1) dlat = np.subtract(lat2, lat1) a = np.add( np.power(np.sin(np.divide(dlat, 2)), 2), np.multiply( np.cos(lat1), np.multiply(np.cos(lat2), np.power(np.sin(np.divide(dlon, 2)), 2)) ), ) c = np.multiply(2, np.arcsin(np.sqrt(a))) # km r = 6371 # meters r *= 1000 return c * r load_entry def load_entry ( ) View Source def load_entry (): import sys just . write ( \"\" , \"~/nostalgia_data/__init__.py\" ) sys . path . append ( os . path . expanduser ( \"~/nostalgia_data/\" )) import nostalgia_entry return nostalgia_entry normalize_name def normalize_name ( name ) View Source def normalize_name(name): return re.sub(\"([A-Z])\", r\"_\\1\", name).lstrip(\"_\").lower() parse_price def parse_price ( x ) View Source def parse_price(x): if x is None: return None if isinstance(x, list): x = \"\".join(x) if isinstance(x, (int, float)): if x != x: return None return x x = x.replace(\" \", \"\") try: return float(x) except ValueError: pass result = np.nan for decimal_sep in seps: for thousand_sep in seps: if decimal_sep == thousand_sep: continue regex = \"^[+-]?\" regex += \"[0-9]{1,3}\" regex += \"(?:[\" + thousand_sep + \"]?[0-9]{3})*\" regex += \"([\" + decimal_sep + \"][0-9]{1,2})?\" regex += \"$\" if re.match(regex, x): try: result = float(x.replace(thousand_sep, \"\").replace(decimal_sep, \".\")) except ValueError: continue return result return result view def view ( path ) View Source def view ( path ): from requests_viewer import view_html view_html ( just . read ( path )) Classes MockCredentials class MockCredentials ( credential_dict ) View Source class MockCredentials : def __init__ ( self , credential_dict ) : for k , v in credential_dict . items () : setattr ( self , k , v )","title":"Utils"},{"location":"reference/nostalgia/utils/#module-nostalgiautils","text":"View Source import os import sys import re import just import pandas as pd import numpy as np from tok import word_tokenize if sys . argv [ - 1 ] . endswith ( \"ipython\" ): pd . set_option ( \"display.precision\" , 10 ) pd . set_option ( \"precision\" , 10 ) pd . set_option ( 'display.max_columns' , 500 ) pd . set_option ( 'display.max_colwidth' , - 1 ) has_alpha = re . compile ( \"[a-zA-Z]\" ) num = re . compile ( \"[0-9]\" ) class MockCredentials : def __init__ ( self , credential_dict ): for k , v in credential_dict . items (): setattr ( self , k , v ) def get_tokens ( sentence , lower = True ): if not isinstance ( sentence , str ): return [] if sentence is None : return [] return [ x for x in word_tokenize ( sentence , to_lower = lower ) if len ( x ) & gt ; 2 and has_alpha . search ( x ) and not num . search ( x ) ] def get_token_set ( iterable_of_sentences , lower = True ): words = set () for x in iterable_of_sentences : words . update ( get_tokens ( x , lower )) return words def format_latlng ( latlng ): if isinstance ( latlng , str ): latlng = latlng . replace ( \",\" , \" \" ) . split () return \"{:0.7f}, {:0.7f}\" . format ( * map ( float , latlng )) def view ( path ): from requests_viewer import view_html view_html ( just . read ( path )) def haversine ( lat1 , lon1 , lat2 , lon2 ): \"\"\" Calculate the great circle distance between two points on the earth (specified in decimal degrees) currently in meters \"\"\" # Convert decimal degrees to Radians: lon1 = np . radians ( lon1 ) lat1 = np . radians ( lat1 ) lon2 = np . radians ( lon2 ) lat2 = np . radians ( lat2 ) # Implementing Haversine Formula: dlon = np . subtract ( lon2 , lon1 ) dlat = np . subtract ( lat2 , lat1 ) a = np . add ( np . power ( np . sin ( np . divide ( dlat , 2 )), 2 ), np . multiply ( np . cos ( lat1 ), np . multiply ( np . cos ( lat2 ), np . power ( np . sin ( np . divide ( dlon , 2 )), 2 )) ), ) c = np . multiply ( 2 , np . arcsin ( np . sqrt ( a ))) # km r = 6371 # meters r *= 1000 return c * r seps = [ \".\" , \",\" ] def parse_price ( x ): if x is None : return None if isinstance ( x , list ): x = \"\" . join ( x ) if isinstance ( x , ( int , float )): if x != x : return None return x x = x . replace ( \" \" , \"\" ) try : return float ( x ) except ValueError : pass result = np . nan for decimal_sep in seps : for thousand_sep in seps : if decimal_sep == thousand_sep : continue regex = \"^[+-]?\" regex += \"[0-9]{1,3}\" regex += \"(?:[\" + thousand_sep + \"]?[0-9]{3})*\" regex += \"([\" + decimal_sep + \"][0-9]{1,2})?\" regex += \"$\" if re . match ( regex , x ): try : result = float ( x . replace ( thousand_sep , \"\" ) . replace ( decimal_sep , \".\" )) except ValueError : continue return result return result def normalize_name ( name ): return re . sub ( \"([A-Z])\" , r \"_\\1\" , name ) . lstrip ( \"_\" ) . lower () def clean_df ( df , dc ): for k , v in dc . items (): df [ k ] = [ v ( x ) for x in df [ k ]] return df def load_entry (): import sys just . write ( \"\" , \"~/nostalgia_data/__init__.py\" ) sys . path . append ( os . path . expanduser ( \"~/nostalgia_data/\" )) import nostalgia_entry return nostalgia_entry","title":"Module nostalgia.utils"},{"location":"reference/nostalgia/utils/#variables","text":"has_alpha num seps","title":"Variables"},{"location":"reference/nostalgia/utils/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/utils/#clean_df","text":"def clean_df ( df , dc ) View Source def clean_df(df, dc): for k, v in dc.items(): df[k] = [v(x) for x in df[k]] return df","title":"clean_df"},{"location":"reference/nostalgia/utils/#format_latlng","text":"def format_latlng ( latlng ) View Source def format_latlng(latlng): if isinstance(latlng, str): latlng = latlng.replace(\",\", \" \").split() return \"{:0.7f}, {:0.7f}\".format(*map(float, latlng))","title":"format_latlng"},{"location":"reference/nostalgia/utils/#get_token_set","text":"def get_token_set ( iterable_of_sentences , lower = True ) View Source def get_token_set(iterable_of_sentences, lower=True): words = set() for x in iterable_of_sentences: words.update(get_tokens(x, lower)) return words","title":"get_token_set"},{"location":"reference/nostalgia/utils/#get_tokens","text":"def get_tokens ( sentence , lower = True ) View Source def get_tokens(sentence, lower=True): if not isinstance(sentence, str): return [] if sentence is None: return [] return [ x for x in word_tokenize(sentence, to_lower=lower) if len(x) &gt; 2 and has_alpha.search(x) and not num.search(x) ]","title":"get_tokens"},{"location":"reference/nostalgia/utils/#haversine","text":"def haversine ( lat1 , lon1 , lat2 , lon2 ) Calculate the great circle distance between two points on the earth (specified in decimal degrees) currently in meters View Source def haversine(lat1, lon1, lat2, lon2): \"\"\" Calculate the great circle distance between two points on the earth (specified in decimal degrees) currently in meters \"\"\" # Convert decimal degrees to Radians: lon1 = np.radians(lon1) lat1 = np.radians(lat1) lon2 = np.radians(lon2) lat2 = np.radians(lat2) # Implementing Haversine Formula: dlon = np.subtract(lon2, lon1) dlat = np.subtract(lat2, lat1) a = np.add( np.power(np.sin(np.divide(dlat, 2)), 2), np.multiply( np.cos(lat1), np.multiply(np.cos(lat2), np.power(np.sin(np.divide(dlon, 2)), 2)) ), ) c = np.multiply(2, np.arcsin(np.sqrt(a))) # km r = 6371 # meters r *= 1000 return c * r","title":"haversine"},{"location":"reference/nostalgia/utils/#load_entry","text":"def load_entry ( ) View Source def load_entry (): import sys just . write ( \"\" , \"~/nostalgia_data/__init__.py\" ) sys . path . append ( os . path . expanduser ( \"~/nostalgia_data/\" )) import nostalgia_entry return nostalgia_entry","title":"load_entry"},{"location":"reference/nostalgia/utils/#normalize_name","text":"def normalize_name ( name ) View Source def normalize_name(name): return re.sub(\"([A-Z])\", r\"_\\1\", name).lstrip(\"_\").lower()","title":"normalize_name"},{"location":"reference/nostalgia/utils/#parse_price","text":"def parse_price ( x ) View Source def parse_price(x): if x is None: return None if isinstance(x, list): x = \"\".join(x) if isinstance(x, (int, float)): if x != x: return None return x x = x.replace(\" \", \"\") try: return float(x) except ValueError: pass result = np.nan for decimal_sep in seps: for thousand_sep in seps: if decimal_sep == thousand_sep: continue regex = \"^[+-]?\" regex += \"[0-9]{1,3}\" regex += \"(?:[\" + thousand_sep + \"]?[0-9]{3})*\" regex += \"([\" + decimal_sep + \"][0-9]{1,2})?\" regex += \"$\" if re.match(regex, x): try: result = float(x.replace(thousand_sep, \"\").replace(decimal_sep, \".\")) except ValueError: continue return result return result","title":"parse_price"},{"location":"reference/nostalgia/utils/#view","text":"def view ( path ) View Source def view ( path ): from requests_viewer import view_html view_html ( just . read ( path ))","title":"view"},{"location":"reference/nostalgia/utils/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/utils/#mockcredentials","text":"class MockCredentials ( credential_dict ) View Source class MockCredentials : def __init__ ( self , credential_dict ) : for k , v in credential_dict . items () : setattr ( self , k , v )","title":"MockCredentials"},{"location":"reference/nostalgia/interfaces/","text":"Module nostalgia.interfaces Sub-modules nostalgia.interfaces.chat nostalgia.interfaces.places nostalgia.interfaces.post","title":"Index"},{"location":"reference/nostalgia/interfaces/#module-nostalgiainterfaces","text":"","title":"Module nostalgia.interfaces"},{"location":"reference/nostalgia/interfaces/#sub-modules","text":"nostalgia.interfaces.chat nostalgia.interfaces.places nostalgia.interfaces.post","title":"Sub-modules"},{"location":"reference/nostalgia/interfaces/chat/","text":"Module nostalgia.interfaces.chat View Source from nostalgia.ndf import NDF from nostalgia.nlp import nlp class ChatInterface ( NDF ): __name__ = \"\" me = \"\" sender_column = \"\" _sender_updated = False @nlp ( \"filter\" , \"by me\" , \"i\" , \"my\" ) def by_me ( self ): return self [ self . sender == \"me\" ] @nlp ( \"filter\" , \"by the other\" , \"by someone else\" ) def by_other ( self ): return self [ self . sender != \"me\" ] @property def sender ( self ): if not self . _sender_updated : col = self [ self . sender_column ] self . loc [ col == self . me , self . sender_column ] = \"me\" self . _sender_updated = True return self [ self . sender_column ] Classes ChatInterface class ChatInterface ( data ) View Source class ChatInterface ( NDF ) : __name__ = \"\" me = \"\" sender_column = \"\" _sender_updated = False @nlp ( \"filter\" , \"by me\" , \"i\" , \"my\" ) def by_me ( self ) : return self [ self . sender == \"me\" ] @nlp ( \"filter\" , \"by the other\" , \"by someone else\" ) def by_other ( self ) : return self [ self . sender != \"me\" ] @property def sender ( self ) : if not self._sender_updated : col = self [ self . sender_column ] self . loc [ col == self . me , self . sender_column ] = \"me\" self . _sender_updated = True return self [ self . sender_column ] Ancestors (in MRO) nostalgia.ndf.NDF Descendants nostalgia.sources.facebook.messages.FacebookChat nostalgia.sources.whatsapp.WhatsappChat Class variables keywords me nlp_columns nlp_when selected_columns sender_column vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours sender start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self[self.sender == \"me\"] by_other def by_other ( self ) View Source @nlp(\"filter\", \"by the other\", \"by someone else\") def by_other(self): return self[self.sender != \"me\"] col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Chat"},{"location":"reference/nostalgia/interfaces/chat/#module-nostalgiainterfaceschat","text":"View Source from nostalgia.ndf import NDF from nostalgia.nlp import nlp class ChatInterface ( NDF ): __name__ = \"\" me = \"\" sender_column = \"\" _sender_updated = False @nlp ( \"filter\" , \"by me\" , \"i\" , \"my\" ) def by_me ( self ): return self [ self . sender == \"me\" ] @nlp ( \"filter\" , \"by the other\" , \"by someone else\" ) def by_other ( self ): return self [ self . sender != \"me\" ] @property def sender ( self ): if not self . _sender_updated : col = self [ self . sender_column ] self . loc [ col == self . me , self . sender_column ] = \"me\" self . _sender_updated = True return self [ self . sender_column ]","title":"Module nostalgia.interfaces.chat"},{"location":"reference/nostalgia/interfaces/chat/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/interfaces/chat/#chatinterface","text":"class ChatInterface ( data ) View Source class ChatInterface ( NDF ) : __name__ = \"\" me = \"\" sender_column = \"\" _sender_updated = False @nlp ( \"filter\" , \"by me\" , \"i\" , \"my\" ) def by_me ( self ) : return self [ self . sender == \"me\" ] @nlp ( \"filter\" , \"by the other\" , \"by someone else\" ) def by_other ( self ) : return self [ self . sender != \"me\" ] @property def sender ( self ) : if not self._sender_updated : col = self [ self . sender_column ] self . loc [ col == self . me , self . sender_column ] = \"me\" self . _sender_updated = True return self [ self . sender_column ]","title":"ChatInterface"},{"location":"reference/nostalgia/interfaces/chat/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/interfaces/chat/#descendants","text":"nostalgia.sources.facebook.messages.FacebookChat nostalgia.sources.whatsapp.WhatsappChat","title":"Descendants"},{"location":"reference/nostalgia/interfaces/chat/#class-variables","text":"keywords me nlp_columns nlp_when selected_columns sender_column vendor","title":"Class variables"},{"location":"reference/nostalgia/interfaces/chat/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/interfaces/chat/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/interfaces/chat/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/interfaces/chat/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/interfaces/chat/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours sender start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/interfaces/chat/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/interfaces/chat/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/interfaces/chat/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/interfaces/chat/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/interfaces/chat/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/interfaces/chat/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/interfaces/chat/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/interfaces/chat/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/interfaces/chat/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self[self.sender == \"me\"]","title":"by_me"},{"location":"reference/nostalgia/interfaces/chat/#by_other","text":"def by_other ( self ) View Source @nlp(\"filter\", \"by the other\", \"by someone else\") def by_other(self): return self[self.sender != \"me\"]","title":"by_other"},{"location":"reference/nostalgia/interfaces/chat/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/interfaces/chat/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/interfaces/chat/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/interfaces/chat/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/interfaces/chat/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/interfaces/chat/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/interfaces/chat/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/interfaces/chat/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/interfaces/chat/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/interfaces/chat/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/interfaces/chat/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/interfaces/chat/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/interfaces/chat/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/interfaces/chat/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/interfaces/chat/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/interfaces/chat/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/interfaces/chat/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/interfaces/chat/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/interfaces/chat/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/interfaces/chat/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/interfaces/chat/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/interfaces/chat/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/interfaces/chat/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/interfaces/chat/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/interfaces/chat/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/interfaces/chat/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/interfaces/chat/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/interfaces/places/","text":"Module nostalgia.interfaces.places View Source import pandas as pd import numpy as np from nostalgia.ndf import NDF , get_type_from_registry from nostalgia.nlp import nlp from nostalgia.times import parse_date_tz from nostalgia.utils import haversine class Places ( NDF ): keywords = [ \"did i go\" , \"i was in\" , \"i was at\" , \"was i\" , \"what place\" , \"place\" , \"i was at\" , \"visit\" , \"go\" , \"have i been\" , \"stay\" , ] home = [] work = [] hometown = [] @nlp ( \"filter\" , \"at home\" ) def at_home ( self ): return self . __class__ ( self [ self . category == \"Home\" ]) @nlp ( \"filter\" , [ \"near home\" , \"around home\" ]) def near_home ( self , distance = 1000 ): return self . near_ ( distance , get_type_from_registry ( \"places\" ) . at_home ) @nlp ( \"filter\" , \"at work\" ) def at_work ( self ): return self . __class__ ( self [ self . category == \"Work\" ]) @nlp ( \"filter\" , [ \"near work\" , \"around work\" ]) def near_work ( self , distance = 1000 ): return self . near_ ( distance , get_type_from_registry ( \"places\" ) . at_work ) @property def at_hometown ( self ): raise ValueError ( \"Fix me\" ) return self . __class__ ( self [ self . city == \"Hometown\" ]) def near_ ( self , distance_in_meters , other_places = None ): places = get_type_from_registry ( \"places\" ) if other_places is None else other_places nearbies = [] for _ , row in places [[ \"lat\" , \"lon\" ]] . drop_duplicates () . iterrows (): nearbies . append ( haversine ( self . lat , self . lon , * row ) & lt ; distance_in_meters ) return self . __class__ ( self [ np . any ( nearbies , axis = 0 )]) def at ( self , time ): mp = parse_date_tz ( time ) return self [ self . index . overlaps ( pd . Interval ( mp . start_date , mp . end_date ))] @nlp ( \"end\" , \"how much\" ) def sum ( self ): return self . distance . sum () @nlp ( \"end\" , \"how long\" , \"how much time\" ) def length ( self ): return ( self . index . right - self . index . left ) . to_pytimedelta () . sum () @nlp ( \"filter\" , \"travel\" ) def travel ( self ): return self [ self . transporting ] @nlp ( \"filter\" , \"drive\" , \"by car\" , \"travel by car\" , \"travel using the car\" , \"by car\" , \"going by car\" , \"on a car\" , \"driving\" , ) def travel_by_car ( self ): return self . col_contains ( \"Driving\" , \"category\" ) @nlp ( \"filter\" , \"drive by bus\" , \"by bus\" , \"travel by bus\" , \"using the bus\" , \"going by bus\" , \"on a bus\" , ) def travel_by_bus ( self ): return self . col_contains ( \"On a bus\" , \"category\" ) @nlp ( \"filter\" , \"drive by train\" , \"by train\" , \"travel by train\" , \"using the train\" , \"going by train\" , \"on a train\" , ) def travel_by_train ( self ): return self . col_contains ( \"On a train\" , \"category\" ) @nlp ( \"filter\" , \"work days\" , \"work-days\" , \"on working days\" ) def work_days ( self ): return self [ self . time . dt . weekday & lt ; 5 ] @nlp ( \"filter\" , \"during work\" , \"during work hours\" ) def work_hours ( self ): return self [ self . _office_hours ] @nlp ( \"end\" , \"address of\" , \"what is the address\" , \"how to find\" , \"how can i find\" ) def what_address ( self ): return self [ \"address\" ] Classes Places class Places ( data ) View Source class Places ( NDF ) : keywords = [ \"did i go\" , \"i was in\" , \"i was at\" , \"was i\" , \"what place\" , \"place\" , \"i was at\" , \"visit\" , \"go\" , \"have i been\" , \"stay\" , ] home = [] work = [] hometown = [] @nlp ( \"filter\" , \"at home\" ) def at_home ( self ) : return self . __class__ ( self [ self . category == \"Home\" ]) @nlp ( \"filter\" , [ \"near home\" , \"around home\" ]) def near_home ( self , distance = 1000 ) : return self . near_ ( distance , get_type_from_registry ( \"places\" ). at_home ) @nlp ( \"filter\" , \"at work\" ) def at_work ( self ) : return self . __class__ ( self [ self . category == \"Work\" ]) @nlp ( \"filter\" , [ \"near work\" , \"around work\" ]) def near_work ( self , distance = 1000 ) : return self . near_ ( distance , get_type_from_registry ( \"places\" ). at_work ) @property def at_hometown ( self ) : raise ValueError ( \"Fix me\" ) return self . __class__ ( self [ self . city == \"Hometown\" ]) def near_ ( self , distance_in_meters , other_places = None ) : places = get_type_from_registry ( \"places\" ) if other_places is None else other_places nearbies = [] for _ , row in places [[ \"lat\" , \"lon\" ]]. drop_duplicates (). iterrows () : nearbies . append ( haversine ( self . lat , self . lon , * row ) & lt ; distance_in_meters ) return self . __class__ ( self [ np . any ( nearbies , axis = 0 )]) def at ( self , time ) : mp = parse_date_tz ( time ) return self [ self . index . overlaps ( pd . Interval ( mp . start_date , mp . end_date ))] @nlp ( \"end\" , \"how much\" ) def sum ( self ) : return self . distance . sum () @nlp ( \"end\" , \"how long\" , \"how much time\" ) def length ( self ) : return ( self . index . right - self . index . left ). to_pytimedelta (). sum () @nlp ( \"filter\" , \"travel\" ) def travel ( self ) : return self [ self . transporting ] @nlp ( \"filter\" , \"drive\" , \"by car\" , \"travel by car\" , \"travel using the car\" , \"by car\" , \"going by car\" , \"on a car\" , \"driving\" , ) def travel_by_car ( self ) : return self . col_contains ( \"Driving\" , \"category\" ) @nlp ( \"filter\" , \"drive by bus\" , \"by bus\" , \"travel by bus\" , \"using the bus\" , \"going by bus\" , \"on a bus\" , ) def travel_by_bus ( self ) : return self . col_contains ( \"On a bus\" , \"category\" ) @nlp ( \"filter\" , \"drive by train\" , \"by train\" , \"travel by train\" , \"using the train\" , \"going by train\" , \"on a train\" , ) def travel_by_train ( self ) : return self . col_contains ( \"On a train\" , \"category\" ) @nlp ( \"filter\" , \"work days\" , \"work-days\" , \"on working days\" ) def work_days ( self ) : return self [ self . time . dt . weekday & lt ; 5 ] @nlp ( \"filter\" , \"during work\" , \"during work hours\" ) def work_hours ( self ) : return self [ self . _office_hours ] @nlp ( \"end\" , \"address of\" , \"what is the address\" , \"how to find\" , \"how can i find\" ) def what_address ( self ) : return self [ \"address\" ] Ancestors (in MRO) nostalgia.ndf.NDF Descendants nostalgia.sources.google.flycheck_timeline.GooglePlaces nostalgia.sources.google.timeline.GooglePlaces Class variables home hometown keywords nlp_columns nlp_when selected_columns vendor work Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) Instance variables at_hometown df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time ) View Source def at(self, time): mp = parse_date_tz(time) return self[self.index.overlaps(pd.Interval(mp.start_date, mp.end_date))] at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_home def at_home ( self ) View Source @nlp(\"filter\", \"at home\") def at_home(self): return self.__class__(self[self.category == \"Home\"]) at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) at_work def at_work ( self ) View Source @nlp(\"filter\", \"at work\") def at_work(self): return self.__class__(self[self.category == \"Work\"]) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) length def length ( self ) View Source @nlp(\"end\", \"how long\", \"how much time\") def length(self): return (self.index.right - self.index.left).to_pytimedelta().sum() near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) near_ def near_ ( self , distance_in_meters , other_places = None ) View Source def near_(self, distance_in_meters, other_places=None): places = get_type_from_registry(\"places\") if other_places is None else other_places nearbies = [] for _, row in places[[\"lat\", \"lon\"]].drop_duplicates().iterrows(): nearbies.append(haversine(self.lat, self.lon, *row) &lt; distance_in_meters) return self.__class__(self[np.any(nearbies, axis=0)]) near_home def near_home ( self , distance = 1000 ) View Source @nlp(\"filter\", [\"near home\", \"around home\"]) def near_home(self, distance=1000): return self.near_(distance, get_type_from_registry(\"places\").at_home) near_work def near_work ( self , distance = 1000 ) View Source @nlp(\"filter\", [\"near work\", \"around work\"]) def near_work(self, distance=1000): return self.near_(distance, get_type_from_registry(\"places\").at_work) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) sum def sum ( self ) View Source @nlp(\"end\", \"how much\") def sum(self): return self.distance.sum() tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) travel def travel ( self ) View Source @nlp(\"filter\", \"travel\") def travel(self): return self[self.transporting] travel_by_bus def travel_by_bus ( self ) View Source @nlp( \"filter\", \"drive by bus\", \"by bus\", \"travel by bus\", \"using the bus\", \"going by bus\", \"on a bus\", ) def travel_by_bus(self): return self.col_contains(\"On a bus\", \"category\") travel_by_car def travel_by_car ( self ) View Source @nlp( \"filter\", \"drive\", \"by car\", \"travel by car\", \"travel using the car\", \"by car\", \"going by car\", \"on a car\", \"driving\", ) def travel_by_car(self): return self.col_contains(\"Driving\", \"category\") travel_by_train def travel_by_train ( self ) View Source @nlp( \"filter\", \"drive by train\", \"by train\", \"travel by train\", \"using the train\", \"going by train\", \"on a train\", ) def travel_by_train(self): return self.col_contains(\"On a train\", \"category\") view def view ( self , index ) View Source def view(self, index): view(self.path[index]) what_address def what_address ( self ) View Source @nlp(\"end\", \"address of\", \"what is the address\", \"how to find\", \"how can i find\") def what_address(self): return self[\"address\"] when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) work_days def work_days ( self ) View Source @nlp(\"filter\", \"work days\", \"work-days\", \"on working days\") def work_days(self): return self[self.time.dt.weekday &lt; 5] work_hours def work_hours ( self ) View Source @nlp(\"filter\", \"during work\", \"during work hours\") def work_hours(self): return self[self._office_hours]","title":"Places"},{"location":"reference/nostalgia/interfaces/places/#module-nostalgiainterfacesplaces","text":"View Source import pandas as pd import numpy as np from nostalgia.ndf import NDF , get_type_from_registry from nostalgia.nlp import nlp from nostalgia.times import parse_date_tz from nostalgia.utils import haversine class Places ( NDF ): keywords = [ \"did i go\" , \"i was in\" , \"i was at\" , \"was i\" , \"what place\" , \"place\" , \"i was at\" , \"visit\" , \"go\" , \"have i been\" , \"stay\" , ] home = [] work = [] hometown = [] @nlp ( \"filter\" , \"at home\" ) def at_home ( self ): return self . __class__ ( self [ self . category == \"Home\" ]) @nlp ( \"filter\" , [ \"near home\" , \"around home\" ]) def near_home ( self , distance = 1000 ): return self . near_ ( distance , get_type_from_registry ( \"places\" ) . at_home ) @nlp ( \"filter\" , \"at work\" ) def at_work ( self ): return self . __class__ ( self [ self . category == \"Work\" ]) @nlp ( \"filter\" , [ \"near work\" , \"around work\" ]) def near_work ( self , distance = 1000 ): return self . near_ ( distance , get_type_from_registry ( \"places\" ) . at_work ) @property def at_hometown ( self ): raise ValueError ( \"Fix me\" ) return self . __class__ ( self [ self . city == \"Hometown\" ]) def near_ ( self , distance_in_meters , other_places = None ): places = get_type_from_registry ( \"places\" ) if other_places is None else other_places nearbies = [] for _ , row in places [[ \"lat\" , \"lon\" ]] . drop_duplicates () . iterrows (): nearbies . append ( haversine ( self . lat , self . lon , * row ) & lt ; distance_in_meters ) return self . __class__ ( self [ np . any ( nearbies , axis = 0 )]) def at ( self , time ): mp = parse_date_tz ( time ) return self [ self . index . overlaps ( pd . Interval ( mp . start_date , mp . end_date ))] @nlp ( \"end\" , \"how much\" ) def sum ( self ): return self . distance . sum () @nlp ( \"end\" , \"how long\" , \"how much time\" ) def length ( self ): return ( self . index . right - self . index . left ) . to_pytimedelta () . sum () @nlp ( \"filter\" , \"travel\" ) def travel ( self ): return self [ self . transporting ] @nlp ( \"filter\" , \"drive\" , \"by car\" , \"travel by car\" , \"travel using the car\" , \"by car\" , \"going by car\" , \"on a car\" , \"driving\" , ) def travel_by_car ( self ): return self . col_contains ( \"Driving\" , \"category\" ) @nlp ( \"filter\" , \"drive by bus\" , \"by bus\" , \"travel by bus\" , \"using the bus\" , \"going by bus\" , \"on a bus\" , ) def travel_by_bus ( self ): return self . col_contains ( \"On a bus\" , \"category\" ) @nlp ( \"filter\" , \"drive by train\" , \"by train\" , \"travel by train\" , \"using the train\" , \"going by train\" , \"on a train\" , ) def travel_by_train ( self ): return self . col_contains ( \"On a train\" , \"category\" ) @nlp ( \"filter\" , \"work days\" , \"work-days\" , \"on working days\" ) def work_days ( self ): return self [ self . time . dt . weekday & lt ; 5 ] @nlp ( \"filter\" , \"during work\" , \"during work hours\" ) def work_hours ( self ): return self [ self . _office_hours ] @nlp ( \"end\" , \"address of\" , \"what is the address\" , \"how to find\" , \"how can i find\" ) def what_address ( self ): return self [ \"address\" ]","title":"Module nostalgia.interfaces.places"},{"location":"reference/nostalgia/interfaces/places/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/interfaces/places/#places","text":"class Places ( data ) View Source class Places ( NDF ) : keywords = [ \"did i go\" , \"i was in\" , \"i was at\" , \"was i\" , \"what place\" , \"place\" , \"i was at\" , \"visit\" , \"go\" , \"have i been\" , \"stay\" , ] home = [] work = [] hometown = [] @nlp ( \"filter\" , \"at home\" ) def at_home ( self ) : return self . __class__ ( self [ self . category == \"Home\" ]) @nlp ( \"filter\" , [ \"near home\" , \"around home\" ]) def near_home ( self , distance = 1000 ) : return self . near_ ( distance , get_type_from_registry ( \"places\" ). at_home ) @nlp ( \"filter\" , \"at work\" ) def at_work ( self ) : return self . __class__ ( self [ self . category == \"Work\" ]) @nlp ( \"filter\" , [ \"near work\" , \"around work\" ]) def near_work ( self , distance = 1000 ) : return self . near_ ( distance , get_type_from_registry ( \"places\" ). at_work ) @property def at_hometown ( self ) : raise ValueError ( \"Fix me\" ) return self . __class__ ( self [ self . city == \"Hometown\" ]) def near_ ( self , distance_in_meters , other_places = None ) : places = get_type_from_registry ( \"places\" ) if other_places is None else other_places nearbies = [] for _ , row in places [[ \"lat\" , \"lon\" ]]. drop_duplicates (). iterrows () : nearbies . append ( haversine ( self . lat , self . lon , * row ) & lt ; distance_in_meters ) return self . __class__ ( self [ np . any ( nearbies , axis = 0 )]) def at ( self , time ) : mp = parse_date_tz ( time ) return self [ self . index . overlaps ( pd . Interval ( mp . start_date , mp . end_date ))] @nlp ( \"end\" , \"how much\" ) def sum ( self ) : return self . distance . sum () @nlp ( \"end\" , \"how long\" , \"how much time\" ) def length ( self ) : return ( self . index . right - self . index . left ). to_pytimedelta (). sum () @nlp ( \"filter\" , \"travel\" ) def travel ( self ) : return self [ self . transporting ] @nlp ( \"filter\" , \"drive\" , \"by car\" , \"travel by car\" , \"travel using the car\" , \"by car\" , \"going by car\" , \"on a car\" , \"driving\" , ) def travel_by_car ( self ) : return self . col_contains ( \"Driving\" , \"category\" ) @nlp ( \"filter\" , \"drive by bus\" , \"by bus\" , \"travel by bus\" , \"using the bus\" , \"going by bus\" , \"on a bus\" , ) def travel_by_bus ( self ) : return self . col_contains ( \"On a bus\" , \"category\" ) @nlp ( \"filter\" , \"drive by train\" , \"by train\" , \"travel by train\" , \"using the train\" , \"going by train\" , \"on a train\" , ) def travel_by_train ( self ) : return self . col_contains ( \"On a train\" , \"category\" ) @nlp ( \"filter\" , \"work days\" , \"work-days\" , \"on working days\" ) def work_days ( self ) : return self [ self . time . dt . weekday & lt ; 5 ] @nlp ( \"filter\" , \"during work\" , \"during work hours\" ) def work_hours ( self ) : return self [ self . _office_hours ] @nlp ( \"end\" , \"address of\" , \"what is the address\" , \"how to find\" , \"how can i find\" ) def what_address ( self ) : return self [ \"address\" ]","title":"Places"},{"location":"reference/nostalgia/interfaces/places/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/interfaces/places/#descendants","text":"nostalgia.sources.google.flycheck_timeline.GooglePlaces nostalgia.sources.google.timeline.GooglePlaces","title":"Descendants"},{"location":"reference/nostalgia/interfaces/places/#class-variables","text":"home hometown keywords nlp_columns nlp_when selected_columns vendor work","title":"Class variables"},{"location":"reference/nostalgia/interfaces/places/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/interfaces/places/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/interfaces/places/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/interfaces/places/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/interfaces/places/#instance-variables","text":"at_hometown df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/interfaces/places/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/interfaces/places/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/interfaces/places/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/interfaces/places/#at","text":"def at ( self , time ) View Source def at(self, time): mp = parse_date_tz(time) return self[self.index.overlaps(pd.Interval(mp.start_date, mp.end_date))]","title":"at"},{"location":"reference/nostalgia/interfaces/places/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/interfaces/places/#at_home","text":"def at_home ( self ) View Source @nlp(\"filter\", \"at home\") def at_home(self): return self.__class__(self[self.category == \"Home\"])","title":"at_home"},{"location":"reference/nostalgia/interfaces/places/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/interfaces/places/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/interfaces/places/#at_work","text":"def at_work ( self ) View Source @nlp(\"filter\", \"at work\") def at_work(self): return self.__class__(self[self.category == \"Work\"])","title":"at_work"},{"location":"reference/nostalgia/interfaces/places/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/interfaces/places/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/interfaces/places/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/interfaces/places/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/interfaces/places/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/interfaces/places/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/interfaces/places/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/interfaces/places/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/interfaces/places/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/interfaces/places/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/interfaces/places/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/interfaces/places/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/interfaces/places/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/interfaces/places/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/interfaces/places/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/interfaces/places/#length","text":"def length ( self ) View Source @nlp(\"end\", \"how long\", \"how much time\") def length(self): return (self.index.right - self.index.left).to_pytimedelta().sum()","title":"length"},{"location":"reference/nostalgia/interfaces/places/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/interfaces/places/#near_","text":"def near_ ( self , distance_in_meters , other_places = None ) View Source def near_(self, distance_in_meters, other_places=None): places = get_type_from_registry(\"places\") if other_places is None else other_places nearbies = [] for _, row in places[[\"lat\", \"lon\"]].drop_duplicates().iterrows(): nearbies.append(haversine(self.lat, self.lon, *row) &lt; distance_in_meters) return self.__class__(self[np.any(nearbies, axis=0)])","title":"near_"},{"location":"reference/nostalgia/interfaces/places/#near_home","text":"def near_home ( self , distance = 1000 ) View Source @nlp(\"filter\", [\"near home\", \"around home\"]) def near_home(self, distance=1000): return self.near_(distance, get_type_from_registry(\"places\").at_home)","title":"near_home"},{"location":"reference/nostalgia/interfaces/places/#near_work","text":"def near_work ( self , distance = 1000 ) View Source @nlp(\"filter\", [\"near work\", \"around work\"]) def near_work(self, distance=1000): return self.near_(distance, get_type_from_registry(\"places\").at_work)","title":"near_work"},{"location":"reference/nostalgia/interfaces/places/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/interfaces/places/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/interfaces/places/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/interfaces/places/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/interfaces/places/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/interfaces/places/#sum","text":"def sum ( self ) View Source @nlp(\"end\", \"how much\") def sum(self): return self.distance.sum()","title":"sum"},{"location":"reference/nostalgia/interfaces/places/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/interfaces/places/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/interfaces/places/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/interfaces/places/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/interfaces/places/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/interfaces/places/#travel","text":"def travel ( self ) View Source @nlp(\"filter\", \"travel\") def travel(self): return self[self.transporting]","title":"travel"},{"location":"reference/nostalgia/interfaces/places/#travel_by_bus","text":"def travel_by_bus ( self ) View Source @nlp( \"filter\", \"drive by bus\", \"by bus\", \"travel by bus\", \"using the bus\", \"going by bus\", \"on a bus\", ) def travel_by_bus(self): return self.col_contains(\"On a bus\", \"category\")","title":"travel_by_bus"},{"location":"reference/nostalgia/interfaces/places/#travel_by_car","text":"def travel_by_car ( self ) View Source @nlp( \"filter\", \"drive\", \"by car\", \"travel by car\", \"travel using the car\", \"by car\", \"going by car\", \"on a car\", \"driving\", ) def travel_by_car(self): return self.col_contains(\"Driving\", \"category\")","title":"travel_by_car"},{"location":"reference/nostalgia/interfaces/places/#travel_by_train","text":"def travel_by_train ( self ) View Source @nlp( \"filter\", \"drive by train\", \"by train\", \"travel by train\", \"using the train\", \"going by train\", \"on a train\", ) def travel_by_train(self): return self.col_contains(\"On a train\", \"category\")","title":"travel_by_train"},{"location":"reference/nostalgia/interfaces/places/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/interfaces/places/#what_address","text":"def what_address ( self ) View Source @nlp(\"end\", \"address of\", \"what is the address\", \"how to find\", \"how can i find\") def what_address(self): return self[\"address\"]","title":"what_address"},{"location":"reference/nostalgia/interfaces/places/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/interfaces/places/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/interfaces/places/#work_days","text":"def work_days ( self ) View Source @nlp(\"filter\", \"work days\", \"work-days\", \"on working days\") def work_days(self): return self[self.time.dt.weekday &lt; 5]","title":"work_days"},{"location":"reference/nostalgia/interfaces/places/#work_hours","text":"def work_hours ( self ) View Source @nlp(\"filter\", \"during work\", \"during work hours\") def work_hours(self): return self[self._office_hours]","title":"work_hours"},{"location":"reference/nostalgia/interfaces/post/","text":"Module nostalgia.interfaces.post View Source from nostalgia.ndf import NDF from nostalgia.nlp import nlp class PostInterface ( NDF ): __name__ = \"\" me = \"\" nlp_columns = [ \"title\" ] Classes PostInterface class PostInterface ( data ) View Source class PostInterface ( NDF ) : __name__ = \"\" me = \"\" nlp_columns = [ \"title\" ] Ancestors (in MRO) nostalgia.ndf.NDF Descendants nostalgia.sources.facebook.posts.FacebookPosts nostalgia.sources.reddit_posts.RedditPosts Class variables keywords me nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Post"},{"location":"reference/nostalgia/interfaces/post/#module-nostalgiainterfacespost","text":"View Source from nostalgia.ndf import NDF from nostalgia.nlp import nlp class PostInterface ( NDF ): __name__ = \"\" me = \"\" nlp_columns = [ \"title\" ]","title":"Module nostalgia.interfaces.post"},{"location":"reference/nostalgia/interfaces/post/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/interfaces/post/#postinterface","text":"class PostInterface ( data ) View Source class PostInterface ( NDF ) : __name__ = \"\" me = \"\" nlp_columns = [ \"title\" ]","title":"PostInterface"},{"location":"reference/nostalgia/interfaces/post/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/interfaces/post/#descendants","text":"nostalgia.sources.facebook.posts.FacebookPosts nostalgia.sources.reddit_posts.RedditPosts","title":"Descendants"},{"location":"reference/nostalgia/interfaces/post/#class-variables","text":"keywords me nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/interfaces/post/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/interfaces/post/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/interfaces/post/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/interfaces/post/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/interfaces/post/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/interfaces/post/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/interfaces/post/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/interfaces/post/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/interfaces/post/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/interfaces/post/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/interfaces/post/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/interfaces/post/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/interfaces/post/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/interfaces/post/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/interfaces/post/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/interfaces/post/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/interfaces/post/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/interfaces/post/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/interfaces/post/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/interfaces/post/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/interfaces/post/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/interfaces/post/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/interfaces/post/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/interfaces/post/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/interfaces/post/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/interfaces/post/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/interfaces/post/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/interfaces/post/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/interfaces/post/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/interfaces/post/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/interfaces/post/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/interfaces/post/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/interfaces/post/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/interfaces/post/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/interfaces/post/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/interfaces/post/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/interfaces/post/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/interfaces/post/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/interfaces/post/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/interfaces/post/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/interfaces/post/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/","text":"Module nostalgia.sources Sub-modules nostalgia.sources.albert_heijn nostalgia.sources.chrome_history nostalgia.sources.emacs_file_visits nostalgia.sources.facebook nostalgia.sources.fitbit nostalgia.sources.google nostalgia.sources.ing_banking nostalgia.sources.mijn_chipkaart nostalgia.sources.reddit_posts nostalgia.sources.samsung nostalgia.sources.screenshots nostalgia.sources.shazam nostalgia.sources.sleepcycle nostalgia.sources.web nostalgia.sources.whatsapp nostalgia.sources.whereami","title":"Index"},{"location":"reference/nostalgia/sources/#module-nostalgiasources","text":"","title":"Module nostalgia.sources"},{"location":"reference/nostalgia/sources/#sub-modules","text":"nostalgia.sources.albert_heijn nostalgia.sources.chrome_history nostalgia.sources.emacs_file_visits nostalgia.sources.facebook nostalgia.sources.fitbit nostalgia.sources.google nostalgia.sources.ing_banking nostalgia.sources.mijn_chipkaart nostalgia.sources.reddit_posts nostalgia.sources.samsung nostalgia.sources.screenshots nostalgia.sources.shazam nostalgia.sources.sleepcycle nostalgia.sources.web nostalgia.sources.whatsapp nostalgia.sources.whereami","title":"Sub-modules"},{"location":"reference/nostalgia/sources/albert_heijn/","text":"Module nostalgia.sources.albert_heijn View Source # import getpass # from nostalgia.selenium import get_driver # import lxml.html # driver = get_driver( # credentials={\"username\": \"kootenpv@gmail.com\", \"password\": getpass.getpass()}, # login_url=\"https://www.ah.nl/mijn/inloggen\", # ) # url = \"https://www.ah.nl/producten/eerder-gekocht?sorting=last_bought\" # driver.get(url) # tree = lxml.html.fromstring(driver.page_source) # prices = [x.text_content() for x in tree.xpath(\"//div[@class='product-price']\")] # descriptions = [ # x.text_content().replace(\"\\xad\", \"\") # for x in tree.xpath(\"//div[@class='product-description']/h1\") # ] # links = [ # \"https://www.ah.nl\" + x for x in tree.xpath(\"//article[contains(@class, 'product')]/a/@href\") # ]","title":"Albert Heijn"},{"location":"reference/nostalgia/sources/albert_heijn/#module-nostalgiasourcesalbert_heijn","text":"View Source # import getpass # from nostalgia.selenium import get_driver # import lxml.html # driver = get_driver( # credentials={\"username\": \"kootenpv@gmail.com\", \"password\": getpass.getpass()}, # login_url=\"https://www.ah.nl/mijn/inloggen\", # ) # url = \"https://www.ah.nl/producten/eerder-gekocht?sorting=last_bought\" # driver.get(url) # tree = lxml.html.fromstring(driver.page_source) # prices = [x.text_content() for x in tree.xpath(\"//div[@class='product-price']\")] # descriptions = [ # x.text_content().replace(\"\\xad\", \"\") # for x in tree.xpath(\"//div[@class='product-description']/h1\") # ] # links = [ # \"https://www.ah.nl\" + x for x in tree.xpath(\"//article[contains(@class, 'product')]/a/@href\") # ]","title":"Module nostalgia.sources.albert_heijn"},{"location":"reference/nostalgia/sources/chrome_history/","text":"Module nostalgia.sources.chrome_history View Source import re import just import numpy as np import pandas as pd from nostalgia.ndf import NDF from datetime import datetime from nostalgia.times import tz import lxml.html import diskcache from auto_extract import parse_article from nostalgia.nlp import nlp from nostalgia.cache import get_cache from nostalgia.data_loading import read_array_of_dict_from_json CACHE = get_cache ( \"chrome_history\" ) # def destroy_tree(tree): # node_tracker = {tree: [0, None]} # for node in tree.iterdescendants(): # parent = node.getparent() # node_tracker[node] = [node_tracker[parent][0] + 1, parent] # node_tracker = sorted( # [(depth, parent, child) for child, (depth, parent) in node_tracker.items()], # key=lambda x: x[0], # reverse=True, # ) # for _, parent, child in node_tracker: # if parent is None: # break # parent.remove(child) # del tree def get_title ( x ): if not x . get ( \"domain\" ): return \"\" if x [ \"path\" ] in CACHE : return CACHE [ x [ \"path\" ]] # tree = lxml.html.fromstring(just.read(x[\"path\"])) # title = tree.xpath(\"/html/head/title/text()\") or tree.xpath(\"//title/text()\") or [\"\"] # destroy_tree(tree) # title = title[0] match = re . search ( \"&lt;title&gt;([^&lt;]+)&lt;/title\" , just . read ( x [ \"path\" ]), re . MULTILINE ) title = match . groups ()[ 0 ] . strip () if match is not None else \"\" CACHE [ x [ \"path\" ]] = title return title class WebHistory ( NDF ): keywords = [ \"website\" , \"page\" , \"site\" , \"web history\" , \"page history\" , \"visit\" ] nlp_columns = [ \"domain\" , \"domain_and_suffix\" , \"title\" ] # selected_columns = [\"time\", \"name\", \"price\", \"url\"] @nlp ( \"filter\" , \"watch\" , \"see\" , \"view\" ) def shows ( self ): return self [ np . array ( self . title . str . extract ( \"Watch (.+) Full Movie\" ) . notna ())] @nlp ( \"filter\" , \"show\" , \"series\" ) def series ( self ): shows = self . shows () return shows [ shows . title . str . contains ( \"Season\" )] @nlp ( \"filter\" , \"movies\" , \"films\" ) def movies ( self ): shows = self . shows () return shows [ ~ shows . title . str . contains ( \"Season\" )] @classmethod def object_to_row ( cls , x ): import tldextract if x [ \"url\" ]: extract = tldextract . extract ( x [ \"url\" ]) x [ \"domain\" ] = extract . domain x [ \"domain_and_suffix\" ] = extract . domain + \".\" + extract . suffix x [ \"url\" ] = x [ \"url\" ] or \"_\" . join ( x [ \"path\" ] . split ( \"_\" )[ 1 :]) x [ \"title\" ] = get_title ( x ) return x @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ): web_history = cls . load_object_per_newline ( file_path , nrows ) web_history [ \"time\" ] = web_history . time . apply ( lambda x : datetime . fromtimestamp ( float ( x ), tz )) return cls ( web_history ) Variables CACHE Functions get_title def get_title ( x ) View Source def get_title(x): if not x.get(\"domain\"): return \"\" if x[\"path\"] in CACHE: return CACHE[x[\"path\"]] # tree = lxml.html.fromstring(just.read(x[\"path\"])) # title = tree.xpath(\"/html/head/title/text()\") or tree.xpath(\"//title/text()\") or [\"\"] # destroy_tree(tree) # title = title[0] match = re.search(\"&lt;title&gt;([^&lt;]+)&lt;/title\", just.read(x[\"path\"]), re.MULTILINE) title = match.groups()[0].strip() if match is not None else \"\" CACHE[x[\"path\"]] = title return title Classes WebHistory class WebHistory ( data ) View Source class WebHistory ( NDF ): keywords = [ \"website\" , \"page\" , \"site\" , \"web history\" , \"page history\" , \"visit\" ] nlp_columns = [ \"domain\" , \"domain_and_suffix\" , \"title\" ] # selected_columns = [\"time\", \"name\", \"price\", \"url\"] @nlp ( \"filter\" , \"watch\" , \"see\" , \"view\" ) def shows ( self ): return self [ np . array ( self . title . str . extract ( \"Watch (.+) Full Movie\" ) . notna ())] @nlp ( \"filter\" , \"show\" , \"series\" ) def series ( self ): shows = self . shows () return shows [ shows . title . str . contains ( \"Season\" )] @nlp ( \"filter\" , \"movies\" , \"films\" ) def movies ( self ): shows = self . shows () return shows [ ~ shows . title . str . contains ( \"Season\" )] @classmethod def object_to_row ( cls , x ): import tldextract if x [ \"url\" ]: extract = tldextract . extract ( x [ \"url\" ]) x [ \"domain\" ] = extract . domain x [ \"domain_and_suffix\" ] = extract . domain + \".\" + extract . suffix x [ \"url\" ] = x [ \"url\" ] or \"_\" . join ( x [ \"path\" ] . split ( \"_\" )[ 1 :]) x [ \"title\" ] = get_title ( x ) return x @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ): web_history = cls . load_object_per_newline ( file_path , nrows ) web_history [ \"time\" ] = web_history . time . apply ( lambda x : datetime . fromtimestamp ( float ( x ), tz )) return cls ( web_history ) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( file_path = '~/nostalgia_data/meta.jsonl' , nrows = None , ** kwargs ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/meta.jsonl\", nrows=None, **kwargs): web_history = cls.load_object_per_newline(file_path, nrows) web_history[\"time\"] = web_history.time.apply(lambda x: datetime.fromtimestamp(float(x), tz)) return cls(web_history) object_to_row def object_to_row ( x ) View Source @classmethod def object_to_row ( cls , x ): import tldextract if x [ \"url\" ]: extract = tldextract . extract ( x [ \"url\" ]) x [ \"domain\" ] = extract . domain x [ \"domain_and_suffix\" ] = extract . domain + \".\" + extract . suffix x [ \"url\" ] = x [ \"url\" ] or \"_\" . join ( x [ \"path\" ] . split ( \"_\" )[ 1 :]) x [ \"title\" ] = get_title ( x ) return x Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) movies def movies ( self ) View Source @nlp(\"filter\", \"movies\", \"films\") def movies(self): shows = self.shows() return shows[~shows.title.str.contains(\"Season\")] near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) series def series ( self ) View Source @nlp(\"filter\", \"show\", \"series\") def series(self): shows = self.shows() return shows[shows.title.str.contains(\"Season\")] show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) shows def shows ( self ) View Source @nlp(\"filter\", \"watch\", \"see\", \"view\") def shows(self): return self[np.array(self.title.str.extract(\"Watch (.+) Full Movie\").notna())] sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Chrome History"},{"location":"reference/nostalgia/sources/chrome_history/#module-nostalgiasourceschrome_history","text":"View Source import re import just import numpy as np import pandas as pd from nostalgia.ndf import NDF from datetime import datetime from nostalgia.times import tz import lxml.html import diskcache from auto_extract import parse_article from nostalgia.nlp import nlp from nostalgia.cache import get_cache from nostalgia.data_loading import read_array_of_dict_from_json CACHE = get_cache ( \"chrome_history\" ) # def destroy_tree(tree): # node_tracker = {tree: [0, None]} # for node in tree.iterdescendants(): # parent = node.getparent() # node_tracker[node] = [node_tracker[parent][0] + 1, parent] # node_tracker = sorted( # [(depth, parent, child) for child, (depth, parent) in node_tracker.items()], # key=lambda x: x[0], # reverse=True, # ) # for _, parent, child in node_tracker: # if parent is None: # break # parent.remove(child) # del tree def get_title ( x ): if not x . get ( \"domain\" ): return \"\" if x [ \"path\" ] in CACHE : return CACHE [ x [ \"path\" ]] # tree = lxml.html.fromstring(just.read(x[\"path\"])) # title = tree.xpath(\"/html/head/title/text()\") or tree.xpath(\"//title/text()\") or [\"\"] # destroy_tree(tree) # title = title[0] match = re . search ( \"&lt;title&gt;([^&lt;]+)&lt;/title\" , just . read ( x [ \"path\" ]), re . MULTILINE ) title = match . groups ()[ 0 ] . strip () if match is not None else \"\" CACHE [ x [ \"path\" ]] = title return title class WebHistory ( NDF ): keywords = [ \"website\" , \"page\" , \"site\" , \"web history\" , \"page history\" , \"visit\" ] nlp_columns = [ \"domain\" , \"domain_and_suffix\" , \"title\" ] # selected_columns = [\"time\", \"name\", \"price\", \"url\"] @nlp ( \"filter\" , \"watch\" , \"see\" , \"view\" ) def shows ( self ): return self [ np . array ( self . title . str . extract ( \"Watch (.+) Full Movie\" ) . notna ())] @nlp ( \"filter\" , \"show\" , \"series\" ) def series ( self ): shows = self . shows () return shows [ shows . title . str . contains ( \"Season\" )] @nlp ( \"filter\" , \"movies\" , \"films\" ) def movies ( self ): shows = self . shows () return shows [ ~ shows . title . str . contains ( \"Season\" )] @classmethod def object_to_row ( cls , x ): import tldextract if x [ \"url\" ]: extract = tldextract . extract ( x [ \"url\" ]) x [ \"domain\" ] = extract . domain x [ \"domain_and_suffix\" ] = extract . domain + \".\" + extract . suffix x [ \"url\" ] = x [ \"url\" ] or \"_\" . join ( x [ \"path\" ] . split ( \"_\" )[ 1 :]) x [ \"title\" ] = get_title ( x ) return x @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ): web_history = cls . load_object_per_newline ( file_path , nrows ) web_history [ \"time\" ] = web_history . time . apply ( lambda x : datetime . fromtimestamp ( float ( x ), tz )) return cls ( web_history )","title":"Module nostalgia.sources.chrome_history"},{"location":"reference/nostalgia/sources/chrome_history/#variables","text":"CACHE","title":"Variables"},{"location":"reference/nostalgia/sources/chrome_history/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/chrome_history/#get_title","text":"def get_title ( x ) View Source def get_title(x): if not x.get(\"domain\"): return \"\" if x[\"path\"] in CACHE: return CACHE[x[\"path\"]] # tree = lxml.html.fromstring(just.read(x[\"path\"])) # title = tree.xpath(\"/html/head/title/text()\") or tree.xpath(\"//title/text()\") or [\"\"] # destroy_tree(tree) # title = title[0] match = re.search(\"&lt;title&gt;([^&lt;]+)&lt;/title\", just.read(x[\"path\"]), re.MULTILINE) title = match.groups()[0].strip() if match is not None else \"\" CACHE[x[\"path\"]] = title return title","title":"get_title"},{"location":"reference/nostalgia/sources/chrome_history/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/chrome_history/#webhistory","text":"class WebHistory ( data ) View Source class WebHistory ( NDF ): keywords = [ \"website\" , \"page\" , \"site\" , \"web history\" , \"page history\" , \"visit\" ] nlp_columns = [ \"domain\" , \"domain_and_suffix\" , \"title\" ] # selected_columns = [\"time\", \"name\", \"price\", \"url\"] @nlp ( \"filter\" , \"watch\" , \"see\" , \"view\" ) def shows ( self ): return self [ np . array ( self . title . str . extract ( \"Watch (.+) Full Movie\" ) . notna ())] @nlp ( \"filter\" , \"show\" , \"series\" ) def series ( self ): shows = self . shows () return shows [ shows . title . str . contains ( \"Season\" )] @nlp ( \"filter\" , \"movies\" , \"films\" ) def movies ( self ): shows = self . shows () return shows [ ~ shows . title . str . contains ( \"Season\" )] @classmethod def object_to_row ( cls , x ): import tldextract if x [ \"url\" ]: extract = tldextract . extract ( x [ \"url\" ]) x [ \"domain\" ] = extract . domain x [ \"domain_and_suffix\" ] = extract . domain + \".\" + extract . suffix x [ \"url\" ] = x [ \"url\" ] or \"_\" . join ( x [ \"path\" ] . split ( \"_\" )[ 1 :]) x [ \"title\" ] = get_title ( x ) return x @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ): web_history = cls . load_object_per_newline ( file_path , nrows ) web_history [ \"time\" ] = web_history . time . apply ( lambda x : datetime . fromtimestamp ( float ( x ), tz )) return cls ( web_history )","title":"WebHistory"},{"location":"reference/nostalgia/sources/chrome_history/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/chrome_history/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/chrome_history/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/chrome_history/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/chrome_history/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/chrome_history/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/chrome_history/#load","text":"def load ( file_path = '~/nostalgia_data/meta.jsonl' , nrows = None , ** kwargs ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/meta.jsonl\", nrows=None, **kwargs): web_history = cls.load_object_per_newline(file_path, nrows) web_history[\"time\"] = web_history.time.apply(lambda x: datetime.fromtimestamp(float(x), tz)) return cls(web_history)","title":"load"},{"location":"reference/nostalgia/sources/chrome_history/#object_to_row","text":"def object_to_row ( x ) View Source @classmethod def object_to_row ( cls , x ): import tldextract if x [ \"url\" ]: extract = tldextract . extract ( x [ \"url\" ]) x [ \"domain\" ] = extract . domain x [ \"domain_and_suffix\" ] = extract . domain + \".\" + extract . suffix x [ \"url\" ] = x [ \"url\" ] or \"_\" . join ( x [ \"path\" ] . split ( \"_\" )[ 1 :]) x [ \"title\" ] = get_title ( x ) return x","title":"object_to_row"},{"location":"reference/nostalgia/sources/chrome_history/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/chrome_history/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/chrome_history/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/chrome_history/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/chrome_history/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/chrome_history/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/chrome_history/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/chrome_history/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/chrome_history/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/chrome_history/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/chrome_history/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/chrome_history/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/chrome_history/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/chrome_history/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/chrome_history/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/chrome_history/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/chrome_history/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/chrome_history/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/chrome_history/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/chrome_history/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/chrome_history/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/chrome_history/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/chrome_history/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/chrome_history/#movies","text":"def movies ( self ) View Source @nlp(\"filter\", \"movies\", \"films\") def movies(self): shows = self.shows() return shows[~shows.title.str.contains(\"Season\")]","title":"movies"},{"location":"reference/nostalgia/sources/chrome_history/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/chrome_history/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/chrome_history/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/chrome_history/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/chrome_history/#series","text":"def series ( self ) View Source @nlp(\"filter\", \"show\", \"series\") def series(self): shows = self.shows() return shows[shows.title.str.contains(\"Season\")]","title":"series"},{"location":"reference/nostalgia/sources/chrome_history/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/chrome_history/#shows","text":"def shows ( self ) View Source @nlp(\"filter\", \"watch\", \"see\", \"view\") def shows(self): return self[np.array(self.title.str.extract(\"Watch (.+) Full Movie\").notna())]","title":"shows"},{"location":"reference/nostalgia/sources/chrome_history/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/chrome_history/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/chrome_history/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/chrome_history/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/chrome_history/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/chrome_history/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/chrome_history/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/chrome_history/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/chrome_history/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/emacs_file_visits/","text":"Module nostalgia.sources.emacs_file_visits View Source import os import re import pandas as pd from datetime import datetime from nostalgia.times import tz from nostalgia.ndf import NDF # Add this to your emacs startup init: # (require 'f) # (defun log-find-visits () # (when (and buffer-file-name (not (eq last-command \"xah-close-current-buffer\"))) # (f-append-text (concat (int-to-string (float-time)) \",\" buffer-file-name \"\\n\") 'utf-8 \"~/nostalgia_data/input/log-emacs-find-visits.txt\"))) class FileVisits ( NDF ): @classmethod def load ( cls , file_name = \"~/nostalgia_data/input/log-emacs-find-visits.txt\" , nrows = None ): fname = os . path . expanduser ( file_name ) with open ( fname ) as f : results = [] files = [] ds = [] num = 0 for line in f . read () . split ( \" \\n \" ): if not line : continue y , z = line . split ( \",\" , 1 ) if not y : continue files . append ( z ) d = datetime . fromtimestamp ( float ( y ), tz ) ds . append ( d ) for key in [ \"egoroot/\" , \"/ssh:\" , \"gits/\" , \"site-packages/\" , \"Drive/\" , \"Dropbox/\" ]: if key in z : z = re . split ( \"[:/]\" , z . split ( key )[ 1 ])[ 0 ] results . append ( z ) if num == nrows : break num += 1 data = pd . DataFrame ({ \"file\" : files , \"name\" : results , \"time\" : ds }) return cls ( data ) Classes FileVisits class FileVisits ( data ) View Source class FileVisits ( NDF ) : @classmethod def load ( cls , file_name = \"~/nostalgia_data/input/log-emacs-find-visits.txt\" , nrows = None ) : fname = os . path . expanduser ( file_name ) with open ( fname ) as f : results = [] files = [] ds = [] num = 0 for line in f . read (). split ( \"\\n\" ) : if not line : continue y , z = line . split ( \",\" , 1 ) if not y : continue files . append ( z ) d = datetime . fromtimestamp ( float ( y ), tz ) ds . append ( d ) for key in [ \"egoroot/\" , \"/ssh:\" , \"gits/\" , \"site-packages/\" , \"Drive/\" , \"Dropbox/\" ] : if key in z : z = re . split ( \"[:/]\" , z . split ( key )[ 1 ])[ 0 ] results . append ( z ) if num == nrows : break num += 1 data = pd . DataFrame ({ \"file\" : files , \"name\" : results , \"time\" : ds }) return cls ( data ) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( file_name = '~/nostalgia_data/input/log-emacs-find-visits.txt' , nrows = None ) View Source @classmethod def load(cls, file_name=\"~/nostalgia_data/input/log-emacs-find-visits.txt\", nrows=None): fname = os.path.expanduser(file_name) with open(fname) as f: results = [] files = [] ds = [] num = 0 for line in f.read().split(\"\\n\"): if not line: continue y, z = line.split(\",\", 1) if not y: continue files.append(z) d = datetime.fromtimestamp(float(y), tz) ds.append(d) for key in [\"egoroot/\", \"/ssh:\", \"gits/\", \"site-packages/\", \"Drive/\", \"Dropbox/\"]: if key in z: z = re.split(\"[:/]\", z.split(key)[1])[0] results.append(z) if num == nrows: break num += 1 data = pd.DataFrame({\"file\": files, \"name\": results, \"time\": ds}) return cls(data) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Emacs File Visits"},{"location":"reference/nostalgia/sources/emacs_file_visits/#module-nostalgiasourcesemacs_file_visits","text":"View Source import os import re import pandas as pd from datetime import datetime from nostalgia.times import tz from nostalgia.ndf import NDF # Add this to your emacs startup init: # (require 'f) # (defun log-find-visits () # (when (and buffer-file-name (not (eq last-command \"xah-close-current-buffer\"))) # (f-append-text (concat (int-to-string (float-time)) \",\" buffer-file-name \"\\n\") 'utf-8 \"~/nostalgia_data/input/log-emacs-find-visits.txt\"))) class FileVisits ( NDF ): @classmethod def load ( cls , file_name = \"~/nostalgia_data/input/log-emacs-find-visits.txt\" , nrows = None ): fname = os . path . expanduser ( file_name ) with open ( fname ) as f : results = [] files = [] ds = [] num = 0 for line in f . read () . split ( \" \\n \" ): if not line : continue y , z = line . split ( \",\" , 1 ) if not y : continue files . append ( z ) d = datetime . fromtimestamp ( float ( y ), tz ) ds . append ( d ) for key in [ \"egoroot/\" , \"/ssh:\" , \"gits/\" , \"site-packages/\" , \"Drive/\" , \"Dropbox/\" ]: if key in z : z = re . split ( \"[:/]\" , z . split ( key )[ 1 ])[ 0 ] results . append ( z ) if num == nrows : break num += 1 data = pd . DataFrame ({ \"file\" : files , \"name\" : results , \"time\" : ds }) return cls ( data )","title":"Module nostalgia.sources.emacs_file_visits"},{"location":"reference/nostalgia/sources/emacs_file_visits/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/emacs_file_visits/#filevisits","text":"class FileVisits ( data ) View Source class FileVisits ( NDF ) : @classmethod def load ( cls , file_name = \"~/nostalgia_data/input/log-emacs-find-visits.txt\" , nrows = None ) : fname = os . path . expanduser ( file_name ) with open ( fname ) as f : results = [] files = [] ds = [] num = 0 for line in f . read (). split ( \"\\n\" ) : if not line : continue y , z = line . split ( \",\" , 1 ) if not y : continue files . append ( z ) d = datetime . fromtimestamp ( float ( y ), tz ) ds . append ( d ) for key in [ \"egoroot/\" , \"/ssh:\" , \"gits/\" , \"site-packages/\" , \"Drive/\" , \"Dropbox/\" ] : if key in z : z = re . split ( \"[:/]\" , z . split ( key )[ 1 ])[ 0 ] results . append ( z ) if num == nrows : break num += 1 data = pd . DataFrame ({ \"file\" : files , \"name\" : results , \"time\" : ds }) return cls ( data )","title":"FileVisits"},{"location":"reference/nostalgia/sources/emacs_file_visits/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/emacs_file_visits/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/emacs_file_visits/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/emacs_file_visits/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/emacs_file_visits/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/emacs_file_visits/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/emacs_file_visits/#load","text":"def load ( file_name = '~/nostalgia_data/input/log-emacs-find-visits.txt' , nrows = None ) View Source @classmethod def load(cls, file_name=\"~/nostalgia_data/input/log-emacs-find-visits.txt\", nrows=None): fname = os.path.expanduser(file_name) with open(fname) as f: results = [] files = [] ds = [] num = 0 for line in f.read().split(\"\\n\"): if not line: continue y, z = line.split(\",\", 1) if not y: continue files.append(z) d = datetime.fromtimestamp(float(y), tz) ds.append(d) for key in [\"egoroot/\", \"/ssh:\", \"gits/\", \"site-packages/\", \"Drive/\", \"Dropbox/\"]: if key in z: z = re.split(\"[:/]\", z.split(key)[1])[0] results.append(z) if num == nrows: break num += 1 data = pd.DataFrame({\"file\": files, \"name\": results, \"time\": ds}) return cls(data)","title":"load"},{"location":"reference/nostalgia/sources/emacs_file_visits/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/emacs_file_visits/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/emacs_file_visits/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/emacs_file_visits/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/emacs_file_visits/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/emacs_file_visits/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/emacs_file_visits/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/emacs_file_visits/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/emacs_file_visits/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/emacs_file_visits/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/emacs_file_visits/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/emacs_file_visits/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/emacs_file_visits/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/emacs_file_visits/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/emacs_file_visits/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/emacs_file_visits/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/emacs_file_visits/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/emacs_file_visits/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/emacs_file_visits/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/emacs_file_visits/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/emacs_file_visits/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/emacs_file_visits/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/emacs_file_visits/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/emacs_file_visits/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/emacs_file_visits/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/emacs_file_visits/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/emacs_file_visits/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/emacs_file_visits/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/emacs_file_visits/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/emacs_file_visits/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/emacs_file_visits/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/emacs_file_visits/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/emacs_file_visits/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/emacs_file_visits/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/emacs_file_visits/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/emacs_file_visits/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/emacs_file_visits/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/mijn_chipkaart/","text":"Module nostalgia.sources.mijn_chipkaart View Source import just import pandas as pd from nostalgia.ndf import NDF from nostalgia.times import parse_date_tz from nostalgia.nlp import nlp class MijnChipkaart ( NDF ): @property def title ( self ): return [ x + \" - \" + y for x , y in zip ( self . Vertrek , self . Bestemming )] @nlp ( \"end\" , \"how much\" , \"cost\" ) def sum ( self ): return self . Bedrag . sum () @classmethod def load ( cls , nrows = None ): files = just . glob ( \"~/nostalgia_data/input/mijn_chipkaart/*.csv\" ) data = pd . concat ([ pd . read_csv ( x , sep = \";\" , nrows = nrows ) for x in files ]) data [ \"Bedrag\" ] = [ float ( x . replace ( \",\" , \".\" )) for x in data [ \"Bedrag\" ]] data [ \"Datum\" ] = [ parse_date_tz ( x + \" \" + y ) . start_date for x , y in zip ( data . Datum , data [ \"Check-uit\" ]) ] return cls ( data ) Classes MijnChipkaart class MijnChipkaart ( data ) View Source class MijnChipkaart ( NDF ) : @property def title ( self ) : return [ x + \" - \" + y for x , y in zip ( self . Vertrek , self . Bestemming )] @nlp ( \"end\" , \"how much\" , \"cost\" ) def sum ( self ) : return self . Bedrag . sum () @classmethod def load ( cls , nrows = None ) : files = just . glob ( \"~/nostalgia_data/input/mijn_chipkaart/*.csv\" ) data = pd . concat ([ pd . read_csv ( x , sep = \";\" , nrows = nrows ) for x in files ]) data [ \"Bedrag\" ] = [ float ( x . replace ( \",\" , \".\" )) for x in data [ \"Bedrag\" ]] data [ \"Datum\" ] = [ parse_date_tz ( x + \" \" + y ). start_date for x , y in zip ( data . Datum , data [ \"Check-uit\" ]) ] return cls ( data ) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): files = just.glob(\"~/nostalgia_data/input/mijn_chipkaart/*.csv\") data = pd.concat([pd.read_csv(x, sep=\";\", nrows=nrows) for x in files]) data[\"Bedrag\"] = [float(x.replace(\",\", \".\")) for x in data[\"Bedrag\"]] data[\"Datum\"] = [ parse_date_tz(x + \" \" + y).start_date for x, y in zip(data.Datum, data[\"Check-uit\"]) ] return cls(data) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time title when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) sum def sum ( self ) View Source @nlp(\"end\", \"how much\", \"cost\") def sum(self): return self.Bedrag.sum() tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Mijn Chipkaart"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#module-nostalgiasourcesmijn_chipkaart","text":"View Source import just import pandas as pd from nostalgia.ndf import NDF from nostalgia.times import parse_date_tz from nostalgia.nlp import nlp class MijnChipkaart ( NDF ): @property def title ( self ): return [ x + \" - \" + y for x , y in zip ( self . Vertrek , self . Bestemming )] @nlp ( \"end\" , \"how much\" , \"cost\" ) def sum ( self ): return self . Bedrag . sum () @classmethod def load ( cls , nrows = None ): files = just . glob ( \"~/nostalgia_data/input/mijn_chipkaart/*.csv\" ) data = pd . concat ([ pd . read_csv ( x , sep = \";\" , nrows = nrows ) for x in files ]) data [ \"Bedrag\" ] = [ float ( x . replace ( \",\" , \".\" )) for x in data [ \"Bedrag\" ]] data [ \"Datum\" ] = [ parse_date_tz ( x + \" \" + y ) . start_date for x , y in zip ( data . Datum , data [ \"Check-uit\" ]) ] return cls ( data )","title":"Module nostalgia.sources.mijn_chipkaart"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#mijnchipkaart","text":"class MijnChipkaart ( data ) View Source class MijnChipkaart ( NDF ) : @property def title ( self ) : return [ x + \" - \" + y for x , y in zip ( self . Vertrek , self . Bestemming )] @nlp ( \"end\" , \"how much\" , \"cost\" ) def sum ( self ) : return self . Bedrag . sum () @classmethod def load ( cls , nrows = None ) : files = just . glob ( \"~/nostalgia_data/input/mijn_chipkaart/*.csv\" ) data = pd . concat ([ pd . read_csv ( x , sep = \";\" , nrows = nrows ) for x in files ]) data [ \"Bedrag\" ] = [ float ( x . replace ( \",\" , \".\" )) for x in data [ \"Bedrag\" ]] data [ \"Datum\" ] = [ parse_date_tz ( x + \" \" + y ). start_date for x , y in zip ( data . Datum , data [ \"Check-uit\" ]) ] return cls ( data )","title":"MijnChipkaart"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#load","text":"def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): files = just.glob(\"~/nostalgia_data/input/mijn_chipkaart/*.csv\") data = pd.concat([pd.read_csv(x, sep=\";\", nrows=nrows) for x in files]) data[\"Bedrag\"] = [float(x.replace(\",\", \".\")) for x in data[\"Bedrag\"]] data[\"Datum\"] = [ parse_date_tz(x + \" \" + y).start_date for x, y in zip(data.Datum, data[\"Check-uit\"]) ] return cls(data)","title":"load"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time title when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#sum","text":"def sum ( self ) View Source @nlp(\"end\", \"how much\", \"cost\") def sum(self): return self.Bedrag.sum()","title":"sum"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/mijn_chipkaart/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/reddit_posts/","text":"Module nostalgia.sources.reddit_posts View Source import pandas as pd from nostalgia.times import datetime_from_timestamp from nostalgia.file_caching import save_df , load_df from nostalgia.interfaces.post import PostInterface class RedditPosts ( PostInterface ): vendor = \"reddit\" @classmethod def ingest ( cls , author ): from psaw import PushshiftAPI api = PushshiftAPI () posts = [ { \"title\" : x . title , \"time\" : datetime_from_timestamp ( x . created_utc ), \"url\" : x . full_link , \"text\" : x . selftext , } for x in api . search_submissions ( author = author ) ] posts = pd . DataFrame ( posts ) posts [ \"author\" ] = author save_df ( posts , \"reddit_posts\" ) @classmethod def load ( cls , nrows = None ): df = load_df ( \"reddit_posts\" , nrows ) return cls ( df ) Classes RedditPosts class RedditPosts ( data ) View Source class RedditPosts ( PostInterface ): vendor = \"reddit\" @classmethod def ingest ( cls , author ): from psaw import PushshiftAPI api = PushshiftAPI () posts = [ { \"title\" : x . title , \"time\" : datetime_from_timestamp ( x . created_utc ), \"url\" : x . full_link , \"text\" : x . selftext , } for x in api . search_submissions ( author = author ) ] posts = pd . DataFrame ( posts ) posts [ \"author\" ] = author save_df ( posts , \"reddit_posts\" ) @classmethod def load ( cls , nrows = None ): df = load_df ( \"reddit_posts\" , nrows ) return cls ( df ) Ancestors (in MRO) nostalgia.interfaces.post.PostInterface nostalgia.ndf.NDF Class variables keywords me nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( author ) View Source @classmethod def ingest ( cls , author ): from psaw import PushshiftAPI api = PushshiftAPI () posts = [ { \"title\" : x . title , \"time\" : datetime_from_timestamp ( x . created_utc ), \"url\" : x . full_link , \"text\" : x . selftext , } for x in api . search_submissions ( author = author ) ] posts = pd . DataFrame ( posts ) posts [ \"author\" ] = author save_df ( posts , \"reddit_posts\" ) load def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): df = load_df(\"reddit_posts\", nrows) return cls(df) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Reddit Posts"},{"location":"reference/nostalgia/sources/reddit_posts/#module-nostalgiasourcesreddit_posts","text":"View Source import pandas as pd from nostalgia.times import datetime_from_timestamp from nostalgia.file_caching import save_df , load_df from nostalgia.interfaces.post import PostInterface class RedditPosts ( PostInterface ): vendor = \"reddit\" @classmethod def ingest ( cls , author ): from psaw import PushshiftAPI api = PushshiftAPI () posts = [ { \"title\" : x . title , \"time\" : datetime_from_timestamp ( x . created_utc ), \"url\" : x . full_link , \"text\" : x . selftext , } for x in api . search_submissions ( author = author ) ] posts = pd . DataFrame ( posts ) posts [ \"author\" ] = author save_df ( posts , \"reddit_posts\" ) @classmethod def load ( cls , nrows = None ): df = load_df ( \"reddit_posts\" , nrows ) return cls ( df )","title":"Module nostalgia.sources.reddit_posts"},{"location":"reference/nostalgia/sources/reddit_posts/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/reddit_posts/#redditposts","text":"class RedditPosts ( data ) View Source class RedditPosts ( PostInterface ): vendor = \"reddit\" @classmethod def ingest ( cls , author ): from psaw import PushshiftAPI api = PushshiftAPI () posts = [ { \"title\" : x . title , \"time\" : datetime_from_timestamp ( x . created_utc ), \"url\" : x . full_link , \"text\" : x . selftext , } for x in api . search_submissions ( author = author ) ] posts = pd . DataFrame ( posts ) posts [ \"author\" ] = author save_df ( posts , \"reddit_posts\" ) @classmethod def load ( cls , nrows = None ): df = load_df ( \"reddit_posts\" , nrows ) return cls ( df )","title":"RedditPosts"},{"location":"reference/nostalgia/sources/reddit_posts/#ancestors-in-mro","text":"nostalgia.interfaces.post.PostInterface nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/reddit_posts/#class-variables","text":"keywords me nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/reddit_posts/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/reddit_posts/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/reddit_posts/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/reddit_posts/#ingest","text":"def ingest ( author ) View Source @classmethod def ingest ( cls , author ): from psaw import PushshiftAPI api = PushshiftAPI () posts = [ { \"title\" : x . title , \"time\" : datetime_from_timestamp ( x . created_utc ), \"url\" : x . full_link , \"text\" : x . selftext , } for x in api . search_submissions ( author = author ) ] posts = pd . DataFrame ( posts ) posts [ \"author\" ] = author save_df ( posts , \"reddit_posts\" )","title":"ingest"},{"location":"reference/nostalgia/sources/reddit_posts/#load","text":"def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): df = load_df(\"reddit_posts\", nrows) return cls(df)","title":"load"},{"location":"reference/nostalgia/sources/reddit_posts/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/reddit_posts/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/reddit_posts/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/reddit_posts/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/reddit_posts/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/reddit_posts/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/reddit_posts/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/reddit_posts/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/reddit_posts/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/reddit_posts/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/reddit_posts/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/reddit_posts/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/reddit_posts/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/reddit_posts/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/reddit_posts/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/reddit_posts/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/reddit_posts/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/reddit_posts/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/reddit_posts/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/reddit_posts/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/reddit_posts/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/reddit_posts/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/reddit_posts/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/reddit_posts/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/reddit_posts/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/reddit_posts/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/reddit_posts/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/reddit_posts/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/reddit_posts/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/reddit_posts/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/reddit_posts/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/reddit_posts/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/reddit_posts/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/reddit_posts/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/reddit_posts/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/reddit_posts/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/reddit_posts/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/screenshots/","text":"Module nostalgia.sources.screenshots View Source import os from nostalgia.file_caching import save_df , load_df from nostalgia.ndf import NDF class Screenshots ( NDF ): @classmethod def ingest ( cls , file_dir , nrows = None ): \"\"\" Process a picture folder. Takes ~1s per image to OCR. \"\"\" globs = [ os . path . join ( file_dir , \"*.png\" ), os . path . join ( file_dir , \"*.jpg\" )] pic_data = cls . load_image_texts ( globs , nrows = nrows ) save_df ( pic_data , \"screenshots\" ) @classmethod def load ( cls , nrows = None ): df = load_df ( \"screenshots\" , nrows ) return cls ( df ) Classes Screenshots class Screenshots ( data ) View Source class Screenshots ( NDF ) : @classmethod def ingest ( cls , file_dir , nrows = None ) : \"\"\" Process a picture folder. Takes ~1s per image to OCR. \"\"\" globs = [ os . path . join ( file_dir , \"*.png\" ), os . path . join ( file_dir , \"*.jpg\" )] pic_data = cls . load_image_texts ( globs , nrows = nrows ) save_df ( pic_data , \"screenshots\" ) @classmethod def load ( cls , nrows = None ) : df = load_df ( \"screenshots\" , nrows ) return cls ( df ) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( file_dir , nrows = None ) Process a picture folder. Takes ~1s per image to OCR. View Source @classmethod def ingest(cls, file_dir, nrows=None): \"\"\" Process a picture folder. Takes ~1s per image to OCR. \"\"\" globs = [os.path.join(file_dir, \"*.png\"), os.path.join(file_dir, \"*.jpg\")] pic_data = cls.load_image_texts(globs, nrows=nrows) save_df(pic_data, \"screenshots\") load def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): df = load_df(\"screenshots\", nrows) return cls(df) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Screenshots"},{"location":"reference/nostalgia/sources/screenshots/#module-nostalgiasourcesscreenshots","text":"View Source import os from nostalgia.file_caching import save_df , load_df from nostalgia.ndf import NDF class Screenshots ( NDF ): @classmethod def ingest ( cls , file_dir , nrows = None ): \"\"\" Process a picture folder. Takes ~1s per image to OCR. \"\"\" globs = [ os . path . join ( file_dir , \"*.png\" ), os . path . join ( file_dir , \"*.jpg\" )] pic_data = cls . load_image_texts ( globs , nrows = nrows ) save_df ( pic_data , \"screenshots\" ) @classmethod def load ( cls , nrows = None ): df = load_df ( \"screenshots\" , nrows ) return cls ( df )","title":"Module nostalgia.sources.screenshots"},{"location":"reference/nostalgia/sources/screenshots/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/screenshots/#screenshots","text":"class Screenshots ( data ) View Source class Screenshots ( NDF ) : @classmethod def ingest ( cls , file_dir , nrows = None ) : \"\"\" Process a picture folder. Takes ~1s per image to OCR. \"\"\" globs = [ os . path . join ( file_dir , \"*.png\" ), os . path . join ( file_dir , \"*.jpg\" )] pic_data = cls . load_image_texts ( globs , nrows = nrows ) save_df ( pic_data , \"screenshots\" ) @classmethod def load ( cls , nrows = None ) : df = load_df ( \"screenshots\" , nrows ) return cls ( df )","title":"Screenshots"},{"location":"reference/nostalgia/sources/screenshots/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/screenshots/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/screenshots/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/screenshots/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/screenshots/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/screenshots/#ingest","text":"def ingest ( file_dir , nrows = None ) Process a picture folder. Takes ~1s per image to OCR. View Source @classmethod def ingest(cls, file_dir, nrows=None): \"\"\" Process a picture folder. Takes ~1s per image to OCR. \"\"\" globs = [os.path.join(file_dir, \"*.png\"), os.path.join(file_dir, \"*.jpg\")] pic_data = cls.load_image_texts(globs, nrows=nrows) save_df(pic_data, \"screenshots\")","title":"ingest"},{"location":"reference/nostalgia/sources/screenshots/#load","text":"def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): df = load_df(\"screenshots\", nrows) return cls(df)","title":"load"},{"location":"reference/nostalgia/sources/screenshots/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/screenshots/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/screenshots/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/screenshots/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/screenshots/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/screenshots/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/screenshots/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/screenshots/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/screenshots/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/screenshots/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/screenshots/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/screenshots/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/screenshots/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/screenshots/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/screenshots/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/screenshots/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/screenshots/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/screenshots/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/screenshots/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/screenshots/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/screenshots/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/screenshots/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/screenshots/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/screenshots/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/screenshots/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/screenshots/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/screenshots/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/screenshots/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/screenshots/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/screenshots/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/screenshots/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/screenshots/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/screenshots/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/screenshots/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/screenshots/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/screenshots/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/screenshots/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/shazam/","text":"Module nostalgia.sources.shazam View Source # ensure ~/nostalgia_data/input exists (e.g. \"mkdir -p ~/nostalgia_data/input\" on linux) # goto https://shazam.com/myshazam # open network tab # login # search for url containing \"discovery\" # right click and copy as curl and replace limit=20 with limit=2000 # take that curl command and add the following: &gt; ~/nostalgia_data/input/shazam.json and hit return import pandas as pd import just from nostalgia.ndf import NDF from nostalgia.times import datetime_from_timestamp class Shazam ( NDF ): @classmethod def load ( cls , file_path = \"~/nostalgia_data/input/shazam.json\" , nrows = None ): shazam = pd . DataFrame ( [ ( datetime_from_timestamp ( x [ \"timestamp\" ], x [ \"timezone\" ]), x [ \"track\" ][ \"heading\" ][ \"title\" ], x [ \"track\" ][ \"heading\" ][ \"subtitle\" ], ) for x in just . read ( file_path )[ \"tags\" ] ], columns = [ \"time\" , \"title\" , \"artist\" ], ) return cls ( shazam ) Classes Shazam class Shazam ( data ) View Source class Shazam ( NDF ) : @classmethod def load ( cls , file_path = \"~/nostalgia_data/input/shazam.json\" , nrows = None ) : shazam = pd . DataFrame ( [ ( datetime_from_timestamp ( x [ \"timestamp\" ], x [ \"timezone\" ]), x [ \"track\" ][ \"heading\" ][ \"title\" ], x [ \"track\" ][ \"heading\" ][ \"subtitle\" ], ) for x in just . read ( file_path )[ \"tags\" ] ], columns = [ \"time\" , \"title\" , \"artist\" ], ) return cls ( shazam ) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( file_path = '~/nostalgia_data/input/shazam.json' , nrows = None ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/input/shazam.json\", nrows=None): shazam = pd.DataFrame( [ ( datetime_from_timestamp(x[\"timestamp\"], x[\"timezone\"]), x[\"track\"][\"heading\"][\"title\"], x[\"track\"][\"heading\"][\"subtitle\"], ) for x in just.read(file_path)[\"tags\"] ], columns=[\"time\", \"title\", \"artist\"], ) return cls(shazam) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Shazam"},{"location":"reference/nostalgia/sources/shazam/#module-nostalgiasourcesshazam","text":"View Source # ensure ~/nostalgia_data/input exists (e.g. \"mkdir -p ~/nostalgia_data/input\" on linux) # goto https://shazam.com/myshazam # open network tab # login # search for url containing \"discovery\" # right click and copy as curl and replace limit=20 with limit=2000 # take that curl command and add the following: &gt; ~/nostalgia_data/input/shazam.json and hit return import pandas as pd import just from nostalgia.ndf import NDF from nostalgia.times import datetime_from_timestamp class Shazam ( NDF ): @classmethod def load ( cls , file_path = \"~/nostalgia_data/input/shazam.json\" , nrows = None ): shazam = pd . DataFrame ( [ ( datetime_from_timestamp ( x [ \"timestamp\" ], x [ \"timezone\" ]), x [ \"track\" ][ \"heading\" ][ \"title\" ], x [ \"track\" ][ \"heading\" ][ \"subtitle\" ], ) for x in just . read ( file_path )[ \"tags\" ] ], columns = [ \"time\" , \"title\" , \"artist\" ], ) return cls ( shazam )","title":"Module nostalgia.sources.shazam"},{"location":"reference/nostalgia/sources/shazam/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/shazam/#shazam","text":"class Shazam ( data ) View Source class Shazam ( NDF ) : @classmethod def load ( cls , file_path = \"~/nostalgia_data/input/shazam.json\" , nrows = None ) : shazam = pd . DataFrame ( [ ( datetime_from_timestamp ( x [ \"timestamp\" ], x [ \"timezone\" ]), x [ \"track\" ][ \"heading\" ][ \"title\" ], x [ \"track\" ][ \"heading\" ][ \"subtitle\" ], ) for x in just . read ( file_path )[ \"tags\" ] ], columns = [ \"time\" , \"title\" , \"artist\" ], ) return cls ( shazam )","title":"Shazam"},{"location":"reference/nostalgia/sources/shazam/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/shazam/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/shazam/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/shazam/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/shazam/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/shazam/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/shazam/#load","text":"def load ( file_path = '~/nostalgia_data/input/shazam.json' , nrows = None ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/input/shazam.json\", nrows=None): shazam = pd.DataFrame( [ ( datetime_from_timestamp(x[\"timestamp\"], x[\"timezone\"]), x[\"track\"][\"heading\"][\"title\"], x[\"track\"][\"heading\"][\"subtitle\"], ) for x in just.read(file_path)[\"tags\"] ], columns=[\"time\", \"title\", \"artist\"], ) return cls(shazam)","title":"load"},{"location":"reference/nostalgia/sources/shazam/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/shazam/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/shazam/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/shazam/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/shazam/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/shazam/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/shazam/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/shazam/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/shazam/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/shazam/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/shazam/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/shazam/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/shazam/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/shazam/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/shazam/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/shazam/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/shazam/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/shazam/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/shazam/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/shazam/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/shazam/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/shazam/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/shazam/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/shazam/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/shazam/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/shazam/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/shazam/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/shazam/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/shazam/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/shazam/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/shazam/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/shazam/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/shazam/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/shazam/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/shazam/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/shazam/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/shazam/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/sleepcycle/","text":"Module nostalgia.sources.sleepcycle View Source import re import just import json import pandas as pd import getpass from nostalgia.ndf import NDF from nostalgia.file_caching import save_df , load_df from nostalgia.times import tz login_url = \"https://s.sleepcycle.com/site/login\" export_url = 'https://s.sleepcycle.com/export/original' class SleepCycle ( NDF ): @classmethod def ingest ( cls , credentials , ** kwargs ): html = just . get ( login_url ) token = re . findall ( 'name=\"csrftoken\" value=\"([^\"]+)' , html )[ 0 ] data = { \"username\" : credentials . username , \"csrftoken\" : token , \"password\" : credentials . password , } _ = just . post ( login_url , data = data ) str_data = just . get ( 'https://s.sleepcycle.com/export/original' ) start = str_data . index ( \"data_json.txt\" ) + len ( \"data_json.txt\" ) end = str_data [ start :] . index ( \"}]PK\" ) + 2 data = json . loads ( str_data [ start :][: end ]) xs = [] ys = [] nums = [] for num , x in enumerate ( data ): res = [] times = [] if len ( x [ \"events\" ]) & lt ; 15 : continue for y in x [ \"events\" ]: res . append ( y [ - 1 ]) times . append ( y [ 0 ]) nums . append ( num ) xs . extend ( [ pd . Timestamp ( x [ \"start\" ], tz = tz ) + pd . Timedelta ( seconds = int ( y )) for y in pd . Series ( times ) . rolling ( 15 ) . mean () . fillna ( method = \"bfill\" ) ] ) ys . extend ( pd . Series ( res ) . rolling ( 15 ) . mean () . fillna ( method = \"bfill\" )) df = pd . DataFrame ({ \"time\" : xs , \"score\" : ys , \"num\" : nums }) . drop_duplicates () df [ \"score\" ] = df [ \"score\" ] . clip ( 0 , 0.025 ) save_df ( df , \"sleepcycle\" ) return cls ( df ) @classmethod def load ( cls , nrows = None ): return cls ( load_df ( \"sleepcycle\" , nrows )) Variables export_url login_url Classes SleepCycle class SleepCycle ( data ) View Source class SleepCycle ( NDF ) : @classmethod def ingest ( cls , credentials , ** kwargs ) : html = just . get ( login_url ) token = re . findall ( 'name=\"csrftoken\" value=\"([^\"]+)' , html )[ 0 ] data = { \"username\" : credentials . username , \"csrftoken\" : token , \"password\" : credentials . password , } _ = just . post ( login_url , data = data ) str_data = just . get ( 'https://s.sleepcycle.com/export/original' ) start = str_data . index ( \"data_json.txt\" ) + len ( \"data_json.txt\" ) end = str_data [ start : ]. index ( \"}]PK\" ) + 2 data = json . loads ( str_data [ start : ][ : end ]) xs = [] ys = [] nums = [] for num , x in enumerate ( data ) : res = [] times = [] if len ( x [ \"events\" ]) & lt ; 15 : continue for y in x [ \"events\" ] : res . append ( y [ - 1 ]) times . append ( y [ 0 ]) nums . append ( num ) xs . extend ( [ pd . Timestamp ( x [ \"start\" ], tz = tz ) + pd . Timedelta ( seconds = int ( y )) for y in pd . Series ( times ). rolling ( 15 ). mean (). fillna ( method = \"bfill\" ) ] ) ys . extend ( pd . Series ( res ). rolling ( 15 ). mean (). fillna ( method = \"bfill\" )) df = pd . DataFrame ({ \"time\" : xs , \"score\" : ys , \"num\" : nums }). drop_duplicates () df [ \"score\" ] = df [ \"score\" ]. clip ( 0 , 0.025 ) save_df ( df , \"sleepcycle\" ) return cls ( df ) @classmethod def load ( cls , nrows = None ) : return cls ( load_df ( \"sleepcycle\" , nrows )) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( credentials , ** kwargs ) View Source @classmethod def ingest(cls, credentials, **kwargs): html = just.get(login_url) token = re.findall('name=\"csrftoken\" value=\"([^\"]+)', html)[0] data = { \"username\": credentials.username, \"csrftoken\": token, \"password\": credentials.password, } _ = just.post(login_url, data=data) str_data = just.get('https://s.sleepcycle.com/export/original') start = str_data.index(\"data_json.txt\") + len(\"data_json.txt\") end = str_data[start:].index(\"}]PK\") + 2 data = json.loads(str_data[start:][:end]) xs = [] ys = [] nums = [] for num, x in enumerate(data): res = [] times = [] if len(x[\"events\"]) &lt; 15: continue for y in x[\"events\"]: res.append(y[-1]) times.append(y[0]) nums.append(num) xs.extend( [ pd.Timestamp(x[\"start\"], tz=tz) + pd.Timedelta(seconds=int(y)) for y in pd.Series(times).rolling(15).mean().fillna(method=\"bfill\") ] ) ys.extend(pd.Series(res).rolling(15).mean().fillna(method=\"bfill\")) df = pd.DataFrame({\"time\": xs, \"score\": ys, \"num\": nums}).drop_duplicates() df[\"score\"] = df[\"score\"].clip(0, 0.025) save_df(df, \"sleepcycle\") return cls(df) load def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): return cls(load_df(\"sleepcycle\", nrows)) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Sleepcycle"},{"location":"reference/nostalgia/sources/sleepcycle/#module-nostalgiasourcessleepcycle","text":"View Source import re import just import json import pandas as pd import getpass from nostalgia.ndf import NDF from nostalgia.file_caching import save_df , load_df from nostalgia.times import tz login_url = \"https://s.sleepcycle.com/site/login\" export_url = 'https://s.sleepcycle.com/export/original' class SleepCycle ( NDF ): @classmethod def ingest ( cls , credentials , ** kwargs ): html = just . get ( login_url ) token = re . findall ( 'name=\"csrftoken\" value=\"([^\"]+)' , html )[ 0 ] data = { \"username\" : credentials . username , \"csrftoken\" : token , \"password\" : credentials . password , } _ = just . post ( login_url , data = data ) str_data = just . get ( 'https://s.sleepcycle.com/export/original' ) start = str_data . index ( \"data_json.txt\" ) + len ( \"data_json.txt\" ) end = str_data [ start :] . index ( \"}]PK\" ) + 2 data = json . loads ( str_data [ start :][: end ]) xs = [] ys = [] nums = [] for num , x in enumerate ( data ): res = [] times = [] if len ( x [ \"events\" ]) & lt ; 15 : continue for y in x [ \"events\" ]: res . append ( y [ - 1 ]) times . append ( y [ 0 ]) nums . append ( num ) xs . extend ( [ pd . Timestamp ( x [ \"start\" ], tz = tz ) + pd . Timedelta ( seconds = int ( y )) for y in pd . Series ( times ) . rolling ( 15 ) . mean () . fillna ( method = \"bfill\" ) ] ) ys . extend ( pd . Series ( res ) . rolling ( 15 ) . mean () . fillna ( method = \"bfill\" )) df = pd . DataFrame ({ \"time\" : xs , \"score\" : ys , \"num\" : nums }) . drop_duplicates () df [ \"score\" ] = df [ \"score\" ] . clip ( 0 , 0.025 ) save_df ( df , \"sleepcycle\" ) return cls ( df ) @classmethod def load ( cls , nrows = None ): return cls ( load_df ( \"sleepcycle\" , nrows ))","title":"Module nostalgia.sources.sleepcycle"},{"location":"reference/nostalgia/sources/sleepcycle/#variables","text":"export_url login_url","title":"Variables"},{"location":"reference/nostalgia/sources/sleepcycle/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/sleepcycle/#sleepcycle","text":"class SleepCycle ( data ) View Source class SleepCycle ( NDF ) : @classmethod def ingest ( cls , credentials , ** kwargs ) : html = just . get ( login_url ) token = re . findall ( 'name=\"csrftoken\" value=\"([^\"]+)' , html )[ 0 ] data = { \"username\" : credentials . username , \"csrftoken\" : token , \"password\" : credentials . password , } _ = just . post ( login_url , data = data ) str_data = just . get ( 'https://s.sleepcycle.com/export/original' ) start = str_data . index ( \"data_json.txt\" ) + len ( \"data_json.txt\" ) end = str_data [ start : ]. index ( \"}]PK\" ) + 2 data = json . loads ( str_data [ start : ][ : end ]) xs = [] ys = [] nums = [] for num , x in enumerate ( data ) : res = [] times = [] if len ( x [ \"events\" ]) & lt ; 15 : continue for y in x [ \"events\" ] : res . append ( y [ - 1 ]) times . append ( y [ 0 ]) nums . append ( num ) xs . extend ( [ pd . Timestamp ( x [ \"start\" ], tz = tz ) + pd . Timedelta ( seconds = int ( y )) for y in pd . Series ( times ). rolling ( 15 ). mean (). fillna ( method = \"bfill\" ) ] ) ys . extend ( pd . Series ( res ). rolling ( 15 ). mean (). fillna ( method = \"bfill\" )) df = pd . DataFrame ({ \"time\" : xs , \"score\" : ys , \"num\" : nums }). drop_duplicates () df [ \"score\" ] = df [ \"score\" ]. clip ( 0 , 0.025 ) save_df ( df , \"sleepcycle\" ) return cls ( df ) @classmethod def load ( cls , nrows = None ) : return cls ( load_df ( \"sleepcycle\" , nrows ))","title":"SleepCycle"},{"location":"reference/nostalgia/sources/sleepcycle/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/sleepcycle/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/sleepcycle/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/sleepcycle/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/sleepcycle/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/sleepcycle/#ingest","text":"def ingest ( credentials , ** kwargs ) View Source @classmethod def ingest(cls, credentials, **kwargs): html = just.get(login_url) token = re.findall('name=\"csrftoken\" value=\"([^\"]+)', html)[0] data = { \"username\": credentials.username, \"csrftoken\": token, \"password\": credentials.password, } _ = just.post(login_url, data=data) str_data = just.get('https://s.sleepcycle.com/export/original') start = str_data.index(\"data_json.txt\") + len(\"data_json.txt\") end = str_data[start:].index(\"}]PK\") + 2 data = json.loads(str_data[start:][:end]) xs = [] ys = [] nums = [] for num, x in enumerate(data): res = [] times = [] if len(x[\"events\"]) &lt; 15: continue for y in x[\"events\"]: res.append(y[-1]) times.append(y[0]) nums.append(num) xs.extend( [ pd.Timestamp(x[\"start\"], tz=tz) + pd.Timedelta(seconds=int(y)) for y in pd.Series(times).rolling(15).mean().fillna(method=\"bfill\") ] ) ys.extend(pd.Series(res).rolling(15).mean().fillna(method=\"bfill\")) df = pd.DataFrame({\"time\": xs, \"score\": ys, \"num\": nums}).drop_duplicates() df[\"score\"] = df[\"score\"].clip(0, 0.025) save_df(df, \"sleepcycle\") return cls(df)","title":"ingest"},{"location":"reference/nostalgia/sources/sleepcycle/#load","text":"def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): return cls(load_df(\"sleepcycle\", nrows))","title":"load"},{"location":"reference/nostalgia/sources/sleepcycle/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/sleepcycle/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/sleepcycle/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/sleepcycle/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/sleepcycle/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/sleepcycle/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/sleepcycle/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/sleepcycle/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/sleepcycle/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/sleepcycle/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/sleepcycle/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/sleepcycle/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/sleepcycle/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/sleepcycle/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/sleepcycle/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/sleepcycle/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/sleepcycle/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/sleepcycle/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/sleepcycle/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/sleepcycle/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/sleepcycle/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/sleepcycle/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/sleepcycle/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/sleepcycle/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/sleepcycle/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/sleepcycle/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/sleepcycle/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/sleepcycle/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/sleepcycle/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/sleepcycle/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/sleepcycle/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/sleepcycle/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/sleepcycle/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/sleepcycle/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/sleepcycle/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/sleepcycle/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/sleepcycle/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/whatsapp/","text":"Module nostalgia.sources.whatsapp View Source import just import pandas as pd from nostalgia.interfaces.chat import ChatInterface from nostalgia.times import datetime_from_format offset = len ( \"01/10/2019, 20:02 - \" ) class WhatsappChat ( ChatInterface ): \"\"\"# WhatsApp Chat WhatsApp allows you to export a single conversation (.txt) by email using a mobile app. Below official instructions for each platform: - [Android](https://faq.whatsapp.com/en/android/23756533/) - iPhone (does not work) - [Windows Phone](https://faq.whatsapp.com/en/wp/22548236) ### Other method datas = [] for fname in just.glob(\"~/Downloads/*.csv\"): try: with open(fname) as f: chars = f.read(30) if \"Date1;Date2;Time;UserPhone\" not in chars: continue datas.append(pd.read_csv(fname, sep=\";\")) except PermissionError: continue ### Create instance Click below on the `+` sign and fill in the path to the WhatsApp chat file.\"\"\" vendor = \"whatsapp\" me = \"\" sender_column = \"sender\" @classmethod def load ( cls , nrows = None , ** kwargs ): old_text = \"\" results = [] nrows = nrows or float ( \"inf\" ) for file_path in just . glob ( \"~/nostalgia_data/input/whatsapp/*.txt\" ): row = 0 for line in just . iread ( file_path ): try : time = datetime_from_format ( line [: offset ], \" %d /%m/%Y, %H:%M - \" ) except ValueError : old_text += line + \" \\n \" continue line = old_text + line [ offset :] old_text = \"\" try : if line . startswith ( \"Messages to this chat and calls are now secured\" ): continue sender , text = line . split ( \": \" , 1 ) except ValueError : print ( \"ERR\" , line ) continue if line : if row & gt ; nrows : break row += 1 results . append (( time , sender , text )) df = pd . DataFrame ( results , columns = [ \"time\" , \"sender\" , \"text\" ]) # hack \"order\" into minute data same_minute = df . time == df . shift ( 1 ) . time seconds = [] second_prop = 0 for x in same_minute : if x : second_prop += 1 else : second_prop = 0 seconds . append ( pd . Timedelta ( seconds = 60 * second_prop / ( second_prop + 1 ))) df [ \"time\" ] = df [ \"time\" ] + pd . Series ( seconds ) return cls ( df ) Variables offset Classes WhatsappChat class WhatsappChat ( data ) WhatsApp Chat WhatsApp allows you to export a single conversation (.txt) by email using a mobile app. Below official instructions for each platform: Android iPhone (does not work) Windows Phone Other method datas = [] for fname in just.glob(\"~/Downloads/*.csv\"): try: with open(fname) as f: chars = f.read(30) if \"Date1;Date2;Time;UserPhone\" not in chars: continue datas.append(pd.read_csv(fname, sep=\";\")) except PermissionError: continue Create instance Click below on the + sign and fill in the path to the WhatsApp chat file. View Source class WhatsappChat ( ChatInterface ) : \"\"\"# WhatsApp Chat WhatsApp allows you to export a single conversation (.txt) by email using a mobile app. Below official instructions for each platform: - [Android](https://faq.whatsapp.com/en/android/23756533/) - iPhone (does not work) - [Windows Phone](https://faq.whatsapp.com/en/wp/22548236) ### Other method datas = [] for fname in just.glob(\" ~ /Downloads/ * . csv \"): try: with open(fname) as f: chars = f.read(30) if \" Date1 ; Date2 ; Time ; UserPhone \" not in chars: continue datas.append(pd.read_csv(fname, sep=\" ; \")) except PermissionError: continue ### Create instance Click below on the `+` sign and fill in the path to the WhatsApp chat file.\"\"\" vendor = \"whatsapp\" me = \"\" sender_column = \"sender\" @classmethod def load ( cls , nrows = None , ** kwargs ) : old_text = \"\" results = [] nrows = nrows or float ( \"inf\" ) for file_path in just . glob ( \"~/nostalgia_data/input/whatsapp/*.txt\" ) : row = 0 for line in just . iread ( file_path ) : try : time = datetime_from_format ( line [ : offset ], \"%d/%m/%Y, %H:%M - \" ) except ValueError : old_text += line + \"\\n\" continue line = old_text + line [ offset : ] old_text = \"\" try : if line . startswith ( \"Messages to this chat and calls are now secured\" ) : continue sender , text = line . split ( \": \" , 1 ) except ValueError : print ( \"ERR\" , line ) continue if line : if row & gt ; nrows : break row += 1 results . append (( time , sender , text )) df = pd . DataFrame ( results , columns = [ \"time\" , \"sender\" , \"text\" ]) # hack \"order\" into minute data same_minute = df . time == df . shift ( 1 ). time seconds = [] second_prop = 0 for x in same_minute : if x : second_prop += 1 else : second_prop = 0 seconds . append ( pd . Timedelta ( seconds = 60 * second_prop / ( second_prop + 1 ))) df [ \"time\" ] = df [ \"time\" ] + pd . Series ( seconds ) return cls ( df ) Ancestors (in MRO) nostalgia.interfaces.chat.ChatInterface nostalgia.ndf.NDF Class variables keywords me nlp_columns nlp_when selected_columns sender_column vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None , ** kwargs ) View Source @classmethod def load(cls, nrows=None, **kwargs): old_text = \"\" results = [] nrows = nrows or float(\"inf\") for file_path in just.glob(\"~/nostalgia_data/input/whatsapp/*.txt\"): row = 0 for line in just.iread(file_path): try: time = datetime_from_format(line[:offset], \"%d/%m/%Y, %H:%M - \") except ValueError: old_text += line + \"\\n\" continue line = old_text + line[offset:] old_text = \"\" try: if line.startswith(\"Messages to this chat and calls are now secured\"): continue sender, text = line.split(\": \", 1) except ValueError: print(\"ERR\", line) continue if line: if row &gt; nrows: break row += 1 results.append((time, sender, text)) df = pd.DataFrame(results, columns=[\"time\", \"sender\", \"text\"]) # hack \"order\" into minute data same_minute = df.time == df.shift(1).time seconds = [] second_prop = 0 for x in same_minute: if x: second_prop += 1 else: second_prop = 0 seconds.append(pd.Timedelta(seconds=60 * second_prop / (second_prop + 1))) df[\"time\"] = df[\"time\"] + pd.Series(seconds) return cls(df) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours sender start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self[self.sender == \"me\"] by_other def by_other ( self ) View Source @nlp(\"filter\", \"by the other\", \"by someone else\") def by_other(self): return self[self.sender != \"me\"] col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Whatsapp"},{"location":"reference/nostalgia/sources/whatsapp/#module-nostalgiasourceswhatsapp","text":"View Source import just import pandas as pd from nostalgia.interfaces.chat import ChatInterface from nostalgia.times import datetime_from_format offset = len ( \"01/10/2019, 20:02 - \" ) class WhatsappChat ( ChatInterface ): \"\"\"# WhatsApp Chat WhatsApp allows you to export a single conversation (.txt) by email using a mobile app. Below official instructions for each platform: - [Android](https://faq.whatsapp.com/en/android/23756533/) - iPhone (does not work) - [Windows Phone](https://faq.whatsapp.com/en/wp/22548236) ### Other method datas = [] for fname in just.glob(\"~/Downloads/*.csv\"): try: with open(fname) as f: chars = f.read(30) if \"Date1;Date2;Time;UserPhone\" not in chars: continue datas.append(pd.read_csv(fname, sep=\";\")) except PermissionError: continue ### Create instance Click below on the `+` sign and fill in the path to the WhatsApp chat file.\"\"\" vendor = \"whatsapp\" me = \"\" sender_column = \"sender\" @classmethod def load ( cls , nrows = None , ** kwargs ): old_text = \"\" results = [] nrows = nrows or float ( \"inf\" ) for file_path in just . glob ( \"~/nostalgia_data/input/whatsapp/*.txt\" ): row = 0 for line in just . iread ( file_path ): try : time = datetime_from_format ( line [: offset ], \" %d /%m/%Y, %H:%M - \" ) except ValueError : old_text += line + \" \\n \" continue line = old_text + line [ offset :] old_text = \"\" try : if line . startswith ( \"Messages to this chat and calls are now secured\" ): continue sender , text = line . split ( \": \" , 1 ) except ValueError : print ( \"ERR\" , line ) continue if line : if row & gt ; nrows : break row += 1 results . append (( time , sender , text )) df = pd . DataFrame ( results , columns = [ \"time\" , \"sender\" , \"text\" ]) # hack \"order\" into minute data same_minute = df . time == df . shift ( 1 ) . time seconds = [] second_prop = 0 for x in same_minute : if x : second_prop += 1 else : second_prop = 0 seconds . append ( pd . Timedelta ( seconds = 60 * second_prop / ( second_prop + 1 ))) df [ \"time\" ] = df [ \"time\" ] + pd . Series ( seconds ) return cls ( df )","title":"Module nostalgia.sources.whatsapp"},{"location":"reference/nostalgia/sources/whatsapp/#variables","text":"offset","title":"Variables"},{"location":"reference/nostalgia/sources/whatsapp/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/whatsapp/#whatsappchat","text":"class WhatsappChat ( data )","title":"WhatsappChat"},{"location":"reference/nostalgia/sources/whatsapp/#whatsapp-chat","text":"WhatsApp allows you to export a single conversation (.txt) by email using a mobile app. Below official instructions for each platform: Android iPhone (does not work) Windows Phone","title":"WhatsApp Chat"},{"location":"reference/nostalgia/sources/whatsapp/#other-method","text":"datas = [] for fname in just.glob(\"~/Downloads/*.csv\"): try: with open(fname) as f: chars = f.read(30) if \"Date1;Date2;Time;UserPhone\" not in chars: continue datas.append(pd.read_csv(fname, sep=\";\")) except PermissionError: continue","title":"Other method"},{"location":"reference/nostalgia/sources/whatsapp/#create-instance","text":"Click below on the + sign and fill in the path to the WhatsApp chat file. View Source class WhatsappChat ( ChatInterface ) : \"\"\"# WhatsApp Chat WhatsApp allows you to export a single conversation (.txt) by email using a mobile app. Below official instructions for each platform: - [Android](https://faq.whatsapp.com/en/android/23756533/) - iPhone (does not work) - [Windows Phone](https://faq.whatsapp.com/en/wp/22548236) ### Other method datas = [] for fname in just.glob(\" ~ /Downloads/ * . csv \"): try: with open(fname) as f: chars = f.read(30) if \" Date1 ; Date2 ; Time ; UserPhone \" not in chars: continue datas.append(pd.read_csv(fname, sep=\" ; \")) except PermissionError: continue ### Create instance Click below on the `+` sign and fill in the path to the WhatsApp chat file.\"\"\" vendor = \"whatsapp\" me = \"\" sender_column = \"sender\" @classmethod def load ( cls , nrows = None , ** kwargs ) : old_text = \"\" results = [] nrows = nrows or float ( \"inf\" ) for file_path in just . glob ( \"~/nostalgia_data/input/whatsapp/*.txt\" ) : row = 0 for line in just . iread ( file_path ) : try : time = datetime_from_format ( line [ : offset ], \"%d/%m/%Y, %H:%M - \" ) except ValueError : old_text += line + \"\\n\" continue line = old_text + line [ offset : ] old_text = \"\" try : if line . startswith ( \"Messages to this chat and calls are now secured\" ) : continue sender , text = line . split ( \": \" , 1 ) except ValueError : print ( \"ERR\" , line ) continue if line : if row & gt ; nrows : break row += 1 results . append (( time , sender , text )) df = pd . DataFrame ( results , columns = [ \"time\" , \"sender\" , \"text\" ]) # hack \"order\" into minute data same_minute = df . time == df . shift ( 1 ). time seconds = [] second_prop = 0 for x in same_minute : if x : second_prop += 1 else : second_prop = 0 seconds . append ( pd . Timedelta ( seconds = 60 * second_prop / ( second_prop + 1 ))) df [ \"time\" ] = df [ \"time\" ] + pd . Series ( seconds ) return cls ( df )","title":"Create instance"},{"location":"reference/nostalgia/sources/whatsapp/#ancestors-in-mro","text":"nostalgia.interfaces.chat.ChatInterface nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/whatsapp/#class-variables","text":"keywords me nlp_columns nlp_when selected_columns sender_column vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/whatsapp/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/whatsapp/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/whatsapp/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/whatsapp/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/whatsapp/#load","text":"def load ( nrows = None , ** kwargs ) View Source @classmethod def load(cls, nrows=None, **kwargs): old_text = \"\" results = [] nrows = nrows or float(\"inf\") for file_path in just.glob(\"~/nostalgia_data/input/whatsapp/*.txt\"): row = 0 for line in just.iread(file_path): try: time = datetime_from_format(line[:offset], \"%d/%m/%Y, %H:%M - \") except ValueError: old_text += line + \"\\n\" continue line = old_text + line[offset:] old_text = \"\" try: if line.startswith(\"Messages to this chat and calls are now secured\"): continue sender, text = line.split(\": \", 1) except ValueError: print(\"ERR\", line) continue if line: if row &gt; nrows: break row += 1 results.append((time, sender, text)) df = pd.DataFrame(results, columns=[\"time\", \"sender\", \"text\"]) # hack \"order\" into minute data same_minute = df.time == df.shift(1).time seconds = [] second_prop = 0 for x in same_minute: if x: second_prop += 1 else: second_prop = 0 seconds.append(pd.Timedelta(seconds=60 * second_prop / (second_prop + 1))) df[\"time\"] = df[\"time\"] + pd.Series(seconds) return cls(df)","title":"load"},{"location":"reference/nostalgia/sources/whatsapp/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours sender start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/whatsapp/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/whatsapp/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/whatsapp/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/whatsapp/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/whatsapp/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/whatsapp/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/whatsapp/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/whatsapp/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/whatsapp/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self[self.sender == \"me\"]","title":"by_me"},{"location":"reference/nostalgia/sources/whatsapp/#by_other","text":"def by_other ( self ) View Source @nlp(\"filter\", \"by the other\", \"by someone else\") def by_other(self): return self[self.sender != \"me\"]","title":"by_other"},{"location":"reference/nostalgia/sources/whatsapp/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/whatsapp/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/whatsapp/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/whatsapp/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/whatsapp/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/whatsapp/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/whatsapp/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/whatsapp/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/whatsapp/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/whatsapp/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/whatsapp/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/whatsapp/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/whatsapp/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/whatsapp/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/whatsapp/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/whatsapp/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/whatsapp/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/whatsapp/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/whatsapp/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/whatsapp/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/whatsapp/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/whatsapp/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/whatsapp/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/whatsapp/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/whatsapp/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/whatsapp/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/whatsapp/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/facebook/","text":"Module nostalgia.sources.facebook View Source from nostalgia.ndf import NDF class Facebook ( NDF ): vendor = \"facebook\" ingest_settings = { \"ingest_glob\" : \"~/Downloads/facebook-*.zip\" , \"recent_only\" : False , \"delete_existing\" : True , } Sub-modules nostalgia.sources.facebook.messages nostalgia.sources.facebook.posts Classes Facebook class Facebook ( data ) View Source class Facebook ( NDF ) : vendor = \"facebook\" ingest_settings = { \"ingest_glob\" : \"~/Downloads/facebook-*.zip\" , \"recent_only\" : False , \"delete_existing\" : True , } Ancestors (in MRO) nostalgia.ndf.NDF Descendants nostalgia.sources.facebook.messages.FacebookChat nostalgia.sources.facebook.posts.FacebookPosts Class variables ingest_settings keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Index"},{"location":"reference/nostalgia/sources/facebook/#module-nostalgiasourcesfacebook","text":"View Source from nostalgia.ndf import NDF class Facebook ( NDF ): vendor = \"facebook\" ingest_settings = { \"ingest_glob\" : \"~/Downloads/facebook-*.zip\" , \"recent_only\" : False , \"delete_existing\" : True , }","title":"Module nostalgia.sources.facebook"},{"location":"reference/nostalgia/sources/facebook/#sub-modules","text":"nostalgia.sources.facebook.messages nostalgia.sources.facebook.posts","title":"Sub-modules"},{"location":"reference/nostalgia/sources/facebook/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/facebook/#facebook","text":"class Facebook ( data ) View Source class Facebook ( NDF ) : vendor = \"facebook\" ingest_settings = { \"ingest_glob\" : \"~/Downloads/facebook-*.zip\" , \"recent_only\" : False , \"delete_existing\" : True , }","title":"Facebook"},{"location":"reference/nostalgia/sources/facebook/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/facebook/#descendants","text":"nostalgia.sources.facebook.messages.FacebookChat nostalgia.sources.facebook.posts.FacebookPosts","title":"Descendants"},{"location":"reference/nostalgia/sources/facebook/#class-variables","text":"ingest_settings keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/facebook/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/facebook/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/facebook/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/facebook/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/facebook/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/facebook/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/facebook/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/facebook/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/facebook/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/facebook/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/facebook/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/facebook/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/facebook/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/facebook/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/facebook/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/facebook/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/facebook/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/facebook/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/facebook/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/facebook/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/facebook/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/facebook/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/facebook/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/facebook/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/facebook/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/facebook/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/facebook/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/facebook/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/facebook/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/facebook/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/facebook/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/facebook/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/facebook/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/facebook/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/facebook/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/facebook/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/facebook/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/facebook/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/facebook/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/facebook/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/facebook/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/facebook/messages/","text":"Module nostalgia.sources.facebook.messages View Source from nostalgia.times import tz import pandas as pd import just from nostalgia.interfaces.chat import ChatInterface from nostalgia.data_loading import read_array_of_dict_from_json from nostalgia.sources.facebook import Facebook # # # # # class FacebookChat ( Facebook , ChatInterface ): \"\"\"# Facebook Chat Facebook allows you to export all your data. This source is about the chat between two people using Facebook Messenger. ### Obtaining the data Go to https://www.facebook.com/settings?tab=your_facebook_information and click \"Download your information\". Make sure to select at least **Messages**, and choose **JSON** format above. Afterwards, unpack the ZIP-archive and provide the root folder as \"file_path\". For \"user\", fill in a subfolder name (e.g. johnsmith_nx44ludqrh).\"\"\" me = \"\" sender_column = \"sender_name\" @classmethod def load ( cls , nrows = None ): file_path = \"~/nostalgia_data/input/facebook\" chat_paths = just . glob ( f \"{file_path}/messages/inbox/*/message_1.json\" ) face = pd . concat ( [ read_array_of_dict_from_json ( chat_file , \"messages\" , nrows ) for chat_file in chat_paths ] ) face = face . reset_index ( drop = True ) . sort_values ( \"timestamp_ms\" ) face [ \"time\" ] = pd . to_datetime ( face [ \"timestamp_ms\" ], unit = 'ms' , utc = True ) . dt . tz_convert ( tz ) face . drop ( \"timestamp_ms\" , axis = 1 , inplace = True ) face . loc [ ( face [ \"type\" ] != \"Generic\" ) | face [ \"content\" ] . isnull (), \"content\" ] = \"&lt;INTERACTIVE&gt;\" face [ \"path\" ] = \"\" if \"photos\" in face : not_null = face [ \"photos\" ] . notnull () face . loc [ not_null , \"path\" ] = [ file_path + x [ 0 ][ \"uri\" ] for x in face [ not_null ][ \"photos\" ]] # if \"photos\" in face and isinstance(face[\"photos\"]): # face[\"path\"] = [x.get(\"uri\") if x else x for x in face[\"photos\"]] return cls ( face ) Classes FacebookChat class FacebookChat ( data ) Facebook Chat Facebook allows you to export all your data. This source is about the chat between two people using Facebook Messenger. Obtaining the data Go to https://www.facebook.com/settings?tab=your_facebook_information and click \"Download your information\". Make sure to select at least Messages , and choose JSON format above. Afterwards, unpack the ZIP-archive and provide the root folder as \"file_path\". For \"user\", fill in a subfolder name (e.g. johnsmith_nx44ludqrh). View Source class FacebookChat ( Facebook , ChatInterface ) : \"\"\"# Facebook Chat Facebook allows you to export all your data. This source is about the chat between two people using Facebook Messenger. ### Obtaining the data Go to https://www.facebook.com/settings?tab=your_facebook_information and click \" Download your information \". Make sure to select at least **Messages**, and choose **JSON** format above. Afterwards, unpack the ZIP-archive and provide the root folder as \" file_path \". For \" user \", fill in a subfolder name (e.g. johnsmith_nx44ludqrh).\"\"\" me = \"\" sender_column = \"sender_name\" @classmethod def load ( cls , nrows = None ) : file_path = \"~/nostalgia_data/input/facebook\" chat_paths = just . glob ( f \"{file_path}/messages/inbox/*/message_1.json\" ) face = pd . concat ( [ read_array_of_dict_from_json ( chat_file , \"messages\" , nrows ) for chat_file in chat_paths ] ) face = face . reset_index ( drop = True ). sort_values ( \"timestamp_ms\" ) face [ \"time\" ] = pd . to_datetime ( face [ \"timestamp_ms\" ], unit = 'ms' , utc = True ). dt . tz_convert ( tz ) face . drop ( \"timestamp_ms\" , axis = 1 , inplace = True ) face . loc [ ( face [ \"type\" ] != \"Generic\" ) | face [ \"content\" ]. isnull (), \"content\" ] = \"&lt;INTERACTIVE&gt;\" face [ \"path\" ] = \"\" if \"photos\" in face : not_null = face [ \"photos\" ]. notnull () face . loc [ not_null , \"path\" ] = [ file_path + x [ 0 ][ \"uri\" ] for x in face [ not_null ][ \"photos\" ]] # if \"photos\" in face and isinstance ( face [ \"photos\" ]) : # face [ \"path\" ] = [ x . get ( \"uri\" ) if x else x for x in face [ \"photos\" ]] return cls ( face ) Ancestors (in MRO) nostalgia.sources.facebook.Facebook nostalgia.interfaces.chat.ChatInterface nostalgia.ndf.NDF Class variables ingest_settings keywords me nlp_columns nlp_when selected_columns sender_column vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): file_path = \"~/nostalgia_data/input/facebook\" chat_paths = just.glob(f\"{file_path}/messages/inbox/*/message_1.json\") face = pd.concat( [read_array_of_dict_from_json(chat_file, \"messages\", nrows) for chat_file in chat_paths] ) face = face.reset_index(drop=True).sort_values(\"timestamp_ms\") face[\"time\"] = pd.to_datetime(face[\"timestamp_ms\"], unit='ms', utc=True).dt.tz_convert(tz) face.drop(\"timestamp_ms\", axis=1, inplace=True) face.loc[ (face[\"type\"] != \"Generic\") | face[\"content\"].isnull(), \"content\" ] = \"&lt;INTERACTIVE&gt;\" face[\"path\"] = \"\" if \"photos\" in face: not_null = face[\"photos\"].notnull() face.loc[not_null, \"path\"] = [file_path + x[0][\"uri\"] for x in face[not_null][\"photos\"]] # if \"photos\" in face and isinstance(face[\"photos\"]): # face[\"path\"] = [x.get(\"uri\") if x else x for x in face[\"photos\"]] return cls(face) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours sender start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self[self.sender == \"me\"] by_other def by_other ( self ) View Source @nlp(\"filter\", \"by the other\", \"by someone else\") def by_other(self): return self[self.sender != \"me\"] col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Messages"},{"location":"reference/nostalgia/sources/facebook/messages/#module-nostalgiasourcesfacebookmessages","text":"View Source from nostalgia.times import tz import pandas as pd import just from nostalgia.interfaces.chat import ChatInterface from nostalgia.data_loading import read_array_of_dict_from_json from nostalgia.sources.facebook import Facebook # # # # # class FacebookChat ( Facebook , ChatInterface ): \"\"\"# Facebook Chat Facebook allows you to export all your data. This source is about the chat between two people using Facebook Messenger. ### Obtaining the data Go to https://www.facebook.com/settings?tab=your_facebook_information and click \"Download your information\". Make sure to select at least **Messages**, and choose **JSON** format above. Afterwards, unpack the ZIP-archive and provide the root folder as \"file_path\". For \"user\", fill in a subfolder name (e.g. johnsmith_nx44ludqrh).\"\"\" me = \"\" sender_column = \"sender_name\" @classmethod def load ( cls , nrows = None ): file_path = \"~/nostalgia_data/input/facebook\" chat_paths = just . glob ( f \"{file_path}/messages/inbox/*/message_1.json\" ) face = pd . concat ( [ read_array_of_dict_from_json ( chat_file , \"messages\" , nrows ) for chat_file in chat_paths ] ) face = face . reset_index ( drop = True ) . sort_values ( \"timestamp_ms\" ) face [ \"time\" ] = pd . to_datetime ( face [ \"timestamp_ms\" ], unit = 'ms' , utc = True ) . dt . tz_convert ( tz ) face . drop ( \"timestamp_ms\" , axis = 1 , inplace = True ) face . loc [ ( face [ \"type\" ] != \"Generic\" ) | face [ \"content\" ] . isnull (), \"content\" ] = \"&lt;INTERACTIVE&gt;\" face [ \"path\" ] = \"\" if \"photos\" in face : not_null = face [ \"photos\" ] . notnull () face . loc [ not_null , \"path\" ] = [ file_path + x [ 0 ][ \"uri\" ] for x in face [ not_null ][ \"photos\" ]] # if \"photos\" in face and isinstance(face[\"photos\"]): # face[\"path\"] = [x.get(\"uri\") if x else x for x in face[\"photos\"]] return cls ( face )","title":"Module nostalgia.sources.facebook.messages"},{"location":"reference/nostalgia/sources/facebook/messages/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/facebook/messages/#facebookchat","text":"class FacebookChat ( data )","title":"FacebookChat"},{"location":"reference/nostalgia/sources/facebook/messages/#facebook-chat","text":"Facebook allows you to export all your data. This source is about the chat between two people using Facebook Messenger.","title":"Facebook Chat"},{"location":"reference/nostalgia/sources/facebook/messages/#obtaining-the-data","text":"Go to https://www.facebook.com/settings?tab=your_facebook_information and click \"Download your information\". Make sure to select at least Messages , and choose JSON format above. Afterwards, unpack the ZIP-archive and provide the root folder as \"file_path\". For \"user\", fill in a subfolder name (e.g. johnsmith_nx44ludqrh). View Source class FacebookChat ( Facebook , ChatInterface ) : \"\"\"# Facebook Chat Facebook allows you to export all your data. This source is about the chat between two people using Facebook Messenger. ### Obtaining the data Go to https://www.facebook.com/settings?tab=your_facebook_information and click \" Download your information \". Make sure to select at least **Messages**, and choose **JSON** format above. Afterwards, unpack the ZIP-archive and provide the root folder as \" file_path \". For \" user \", fill in a subfolder name (e.g. johnsmith_nx44ludqrh).\"\"\" me = \"\" sender_column = \"sender_name\" @classmethod def load ( cls , nrows = None ) : file_path = \"~/nostalgia_data/input/facebook\" chat_paths = just . glob ( f \"{file_path}/messages/inbox/*/message_1.json\" ) face = pd . concat ( [ read_array_of_dict_from_json ( chat_file , \"messages\" , nrows ) for chat_file in chat_paths ] ) face = face . reset_index ( drop = True ). sort_values ( \"timestamp_ms\" ) face [ \"time\" ] = pd . to_datetime ( face [ \"timestamp_ms\" ], unit = 'ms' , utc = True ). dt . tz_convert ( tz ) face . drop ( \"timestamp_ms\" , axis = 1 , inplace = True ) face . loc [ ( face [ \"type\" ] != \"Generic\" ) | face [ \"content\" ]. isnull (), \"content\" ] = \"&lt;INTERACTIVE&gt;\" face [ \"path\" ] = \"\" if \"photos\" in face : not_null = face [ \"photos\" ]. notnull () face . loc [ not_null , \"path\" ] = [ file_path + x [ 0 ][ \"uri\" ] for x in face [ not_null ][ \"photos\" ]] # if \"photos\" in face and isinstance ( face [ \"photos\" ]) : # face [ \"path\" ] = [ x . get ( \"uri\" ) if x else x for x in face [ \"photos\" ]] return cls ( face )","title":"Obtaining the data"},{"location":"reference/nostalgia/sources/facebook/messages/#ancestors-in-mro","text":"nostalgia.sources.facebook.Facebook nostalgia.interfaces.chat.ChatInterface nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/facebook/messages/#class-variables","text":"ingest_settings keywords me nlp_columns nlp_when selected_columns sender_column vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/facebook/messages/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/facebook/messages/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/facebook/messages/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/facebook/messages/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/facebook/messages/#load","text":"def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): file_path = \"~/nostalgia_data/input/facebook\" chat_paths = just.glob(f\"{file_path}/messages/inbox/*/message_1.json\") face = pd.concat( [read_array_of_dict_from_json(chat_file, \"messages\", nrows) for chat_file in chat_paths] ) face = face.reset_index(drop=True).sort_values(\"timestamp_ms\") face[\"time\"] = pd.to_datetime(face[\"timestamp_ms\"], unit='ms', utc=True).dt.tz_convert(tz) face.drop(\"timestamp_ms\", axis=1, inplace=True) face.loc[ (face[\"type\"] != \"Generic\") | face[\"content\"].isnull(), \"content\" ] = \"&lt;INTERACTIVE&gt;\" face[\"path\"] = \"\" if \"photos\" in face: not_null = face[\"photos\"].notnull() face.loc[not_null, \"path\"] = [file_path + x[0][\"uri\"] for x in face[not_null][\"photos\"]] # if \"photos\" in face and isinstance(face[\"photos\"]): # face[\"path\"] = [x.get(\"uri\") if x else x for x in face[\"photos\"]] return cls(face)","title":"load"},{"location":"reference/nostalgia/sources/facebook/messages/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours sender start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/facebook/messages/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/facebook/messages/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/facebook/messages/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/facebook/messages/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/facebook/messages/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/facebook/messages/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/facebook/messages/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/facebook/messages/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/facebook/messages/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self[self.sender == \"me\"]","title":"by_me"},{"location":"reference/nostalgia/sources/facebook/messages/#by_other","text":"def by_other ( self ) View Source @nlp(\"filter\", \"by the other\", \"by someone else\") def by_other(self): return self[self.sender != \"me\"]","title":"by_other"},{"location":"reference/nostalgia/sources/facebook/messages/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/facebook/messages/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/facebook/messages/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/facebook/messages/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/facebook/messages/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/facebook/messages/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/facebook/messages/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/facebook/messages/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/facebook/messages/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/facebook/messages/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/facebook/messages/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/facebook/messages/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/facebook/messages/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/facebook/messages/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/facebook/messages/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/facebook/messages/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/facebook/messages/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/facebook/messages/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/facebook/messages/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/facebook/messages/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/facebook/messages/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/facebook/messages/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/facebook/messages/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/facebook/messages/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/facebook/messages/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/facebook/messages/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/facebook/messages/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/facebook/posts/","text":"Module nostalgia.sources.facebook.posts View Source import re import just import pandas from nostalgia.times import datetime_from_timestamp from nostalgia.sources.facebook import Facebook from nostalgia.interfaces.post import PostInterface class FacebookPosts ( Facebook , PostInterface ): @classmethod def handle_json ( cls , data ): posts = [] for post in data : if \"data\" not in post or not isinstance ( post [ \"data\" ], list ): continue location = \"self\" title = post . get ( \"title\" , \"\" ) location_res = re . findall ( \"(?:on|to) ([^']+)'s? [tT]imeline|posted in ([^.]+)|was with ([^.]+)[.]$\" , title ) if location_res : location = [ x for x in location_res [ 0 ] if x ][ 0 ] for x in post [ \"data\" ]: if \"post\" not in x : continue row = { \"location\" : location , \"title\" : x [ \"post\" ], \"time\" : datetime_from_timestamp ( post [ 'timestamp' ]), } posts . append ( row ) return posts @classmethod def load ( cls , nrows = None ): data = cls . load_json_file_modified_time ( \"~/nostalgia_data/input/facebook/posts/your_posts_1.json\" ) return cls ( data ) Classes FacebookPosts class FacebookPosts ( data ) View Source class FacebookPosts ( Facebook , PostInterface ) : @classmethod def handle_json ( cls , data ) : posts = [] for post in data : if \"data\" not in post or not isinstance ( post [ \"data\" ], list ) : continue location = \"self\" title = post . get ( \"title\" , \"\" ) location_res = re . findall ( \"(?:on|to) ([^']+)'s? [tT]imeline|posted in ([^.]+)|was with ([^.]+)[.]$\" , title ) if location_res : location = [ x for x in location_res [ 0 ] if x ][ 0 ] for x in post [ \"data\" ] : if \"post\" not in x : continue row = { \"location\" : location , \"title\" : x [ \"post\" ], \"time\" : datetime_from_timestamp ( post [ 'timestamp' ]), } posts . append ( row ) return posts @classmethod def load ( cls , nrows = None ) : data = cls . load_json_file_modified_time ( \"~/nostalgia_data/input/facebook/posts/your_posts_1.json\" ) return cls ( data ) Ancestors (in MRO) nostalgia.sources.facebook.Facebook nostalgia.interfaces.post.PostInterface nostalgia.ndf.NDF Class variables ingest_settings keywords me nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} handle_json def handle_json ( data ) View Source @classmethod def handle_json(cls, data): posts = [] for post in data: if \"data\" not in post or not isinstance(post[\"data\"], list): continue location = \"self\" title = post.get(\"title\", \"\") location_res = re.findall( \"(?:on|to) ([^']+)'s? [tT]imeline|posted in ([^.]+)|was with ([^.]+)[.]$\", title ) if location_res: location = [x for x in location_res[0] if x][0] for x in post[\"data\"]: if \"post\" not in x: continue row = { \"location\": location, \"title\": x[\"post\"], \"time\": datetime_from_timestamp(post['timestamp']), } posts.append(row) return posts ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): data = cls.load_json_file_modified_time( \"~/nostalgia_data/input/facebook/posts/your_posts_1.json\" ) return cls(data) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Posts"},{"location":"reference/nostalgia/sources/facebook/posts/#module-nostalgiasourcesfacebookposts","text":"View Source import re import just import pandas from nostalgia.times import datetime_from_timestamp from nostalgia.sources.facebook import Facebook from nostalgia.interfaces.post import PostInterface class FacebookPosts ( Facebook , PostInterface ): @classmethod def handle_json ( cls , data ): posts = [] for post in data : if \"data\" not in post or not isinstance ( post [ \"data\" ], list ): continue location = \"self\" title = post . get ( \"title\" , \"\" ) location_res = re . findall ( \"(?:on|to) ([^']+)'s? [tT]imeline|posted in ([^.]+)|was with ([^.]+)[.]$\" , title ) if location_res : location = [ x for x in location_res [ 0 ] if x ][ 0 ] for x in post [ \"data\" ]: if \"post\" not in x : continue row = { \"location\" : location , \"title\" : x [ \"post\" ], \"time\" : datetime_from_timestamp ( post [ 'timestamp' ]), } posts . append ( row ) return posts @classmethod def load ( cls , nrows = None ): data = cls . load_json_file_modified_time ( \"~/nostalgia_data/input/facebook/posts/your_posts_1.json\" ) return cls ( data )","title":"Module nostalgia.sources.facebook.posts"},{"location":"reference/nostalgia/sources/facebook/posts/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/facebook/posts/#facebookposts","text":"class FacebookPosts ( data ) View Source class FacebookPosts ( Facebook , PostInterface ) : @classmethod def handle_json ( cls , data ) : posts = [] for post in data : if \"data\" not in post or not isinstance ( post [ \"data\" ], list ) : continue location = \"self\" title = post . get ( \"title\" , \"\" ) location_res = re . findall ( \"(?:on|to) ([^']+)'s? [tT]imeline|posted in ([^.]+)|was with ([^.]+)[.]$\" , title ) if location_res : location = [ x for x in location_res [ 0 ] if x ][ 0 ] for x in post [ \"data\" ] : if \"post\" not in x : continue row = { \"location\" : location , \"title\" : x [ \"post\" ], \"time\" : datetime_from_timestamp ( post [ 'timestamp' ]), } posts . append ( row ) return posts @classmethod def load ( cls , nrows = None ) : data = cls . load_json_file_modified_time ( \"~/nostalgia_data/input/facebook/posts/your_posts_1.json\" ) return cls ( data )","title":"FacebookPosts"},{"location":"reference/nostalgia/sources/facebook/posts/#ancestors-in-mro","text":"nostalgia.sources.facebook.Facebook nostalgia.interfaces.post.PostInterface nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/facebook/posts/#class-variables","text":"ingest_settings keywords me nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/facebook/posts/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/facebook/posts/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/facebook/posts/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/facebook/posts/#handle_json","text":"def handle_json ( data ) View Source @classmethod def handle_json(cls, data): posts = [] for post in data: if \"data\" not in post or not isinstance(post[\"data\"], list): continue location = \"self\" title = post.get(\"title\", \"\") location_res = re.findall( \"(?:on|to) ([^']+)'s? [tT]imeline|posted in ([^.]+)|was with ([^.]+)[.]$\", title ) if location_res: location = [x for x in location_res[0] if x][0] for x in post[\"data\"]: if \"post\" not in x: continue row = { \"location\": location, \"title\": x[\"post\"], \"time\": datetime_from_timestamp(post['timestamp']), } posts.append(row) return posts","title":"handle_json"},{"location":"reference/nostalgia/sources/facebook/posts/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/facebook/posts/#load","text":"def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): data = cls.load_json_file_modified_time( \"~/nostalgia_data/input/facebook/posts/your_posts_1.json\" ) return cls(data)","title":"load"},{"location":"reference/nostalgia/sources/facebook/posts/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/facebook/posts/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/facebook/posts/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/facebook/posts/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/facebook/posts/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/facebook/posts/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/facebook/posts/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/facebook/posts/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/facebook/posts/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/facebook/posts/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/facebook/posts/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/facebook/posts/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/facebook/posts/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/facebook/posts/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/facebook/posts/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/facebook/posts/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/facebook/posts/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/facebook/posts/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/facebook/posts/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/facebook/posts/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/facebook/posts/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/facebook/posts/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/facebook/posts/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/facebook/posts/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/facebook/posts/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/facebook/posts/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/facebook/posts/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/facebook/posts/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/facebook/posts/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/facebook/posts/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/facebook/posts/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/facebook/posts/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/facebook/posts/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/facebook/posts/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/facebook/posts/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/facebook/posts/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/facebook/posts/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/fitbit/","text":"Module nostalgia.sources.fitbit Sub-modules nostalgia.sources.fitbit.heartrate nostalgia.sources.fitbit.sleep","title":"Index"},{"location":"reference/nostalgia/sources/fitbit/#module-nostalgiasourcesfitbit","text":"","title":"Module nostalgia.sources.fitbit"},{"location":"reference/nostalgia/sources/fitbit/#sub-modules","text":"nostalgia.sources.fitbit.heartrate nostalgia.sources.fitbit.sleep","title":"Sub-modules"},{"location":"reference/nostalgia/sources/fitbit/heartrate/","text":"Module nostalgia.sources.fitbit.heartrate View Source import re import os import pandas as pd import just from datetime import datetime from nostalgia . ndf import NDF from nostalgia . times import datetime_from_format def get_day ( fname ) : return \" \" . join ( re . sub ( \".partial_[0-9]+.[0-9]+\" , \"\" , os . path . basename ( fname )). split ( \".\" )[ 1 :- 1 ]) class FitbitHeartrate ( NDF ) : vendor = \"fitbit\" @classmethod def handle_dataframe_per_file ( cls , df , fname ) : if df . empty : return None day = get_day ( fname ) df [ \"time\" ] = [ datetime_from_format ( day + \" \" + x , \"%Y %m %d %H:%M:%S\" ) for x in df . time ] return df @classmethod def download ( cls ) : from nostalgia_fitbit . __ main__ import main main () @classmethod def load ( cls , nrows = None , **kwargs ) : file_glob = \"~/nostalgia_data/input/fitbit/*/heartrate_intraday/**/*.json\" heartrate = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = heartrate . time . iloc [:- 1 ] end = heartrate . time . iloc [ 1 : ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) heartrate = pd . DataFrame ( heartrate . iloc [:- 1 ]) heartrate = heartrate . set_index ( interval_index ) heartrate [ \"end\" ] = heartrate . index . right - pd . Timedelta ( seconds = 1 ) heartrate = heartrate [ heartrate . time ! = heartrate . end ] heartrate = heartrate [ heartrate . time &lt ; = heartrate . end ] # if nrows is not None : # heartrate = heartrate . iloc [ :nrows ] return cls ( heartrate ) Functions get_day def get_day ( fname ) View Source def get_day ( fname ) : return \" \" . join ( re . sub ( \".partial_[0-9]+.[0-9]+\" , \"\" , os . path . basename ( fname )). split ( \".\" )[ 1 :- 1 ]) Classes FitbitHeartrate class FitbitHeartrate ( data ) View Source class FitbitHeartrate ( NDF ) : vendor = \"fitbit\" @classmethod def handle_dataframe_per_file ( cls , df , fname ) : if df . empty : return None day = get_day ( fname ) df [ \"time\" ] = [ datetime_from_format ( day + \" \" + x , \"%Y %m %d %H:%M:%S\" ) for x in df . time ] return df @classmethod def download ( cls ) : from nostalgia_fitbit . __ main__ import main main () @classmethod def load ( cls , nrows = None , **kwargs ) : file_glob = \"~/nostalgia_data/input/fitbit/*/heartrate_intraday/**/*.json\" heartrate = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = heartrate . time . iloc [:- 1 ] end = heartrate . time . iloc [ 1 : ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) heartrate = pd . DataFrame ( heartrate . iloc [:- 1 ]) heartrate = heartrate . set_index ( interval_index ) heartrate [ \"end\" ] = heartrate . index . right - pd . Timedelta ( seconds = 1 ) heartrate = heartrate [ heartrate . time ! = heartrate . end ] heartrate = heartrate [ heartrate . time &lt ; = heartrate . end ] # if nrows is not None : # heartrate = heartrate . iloc [ :nrows ] return cls ( heartrate ) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() download def download ( ) View Source @classmethod def download ( cls ): from nostalgia_fitbit.__main__ import main main () get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} handle_dataframe_per_file def handle_dataframe_per_file ( df , fname ) View Source @classmethod def handle_dataframe_per_file(cls, df, fname): if df.empty: return None day = get_day(fname) df[\"time\"] = [datetime_from_format(day + \" \" + x, \"%Y %m %d %H:%M:%S\") for x in df.time] return df ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None , ** kwargs ) View Source @classmethod def load ( cls , nrows = None , **kwargs ) : file_glob = \"~/nostalgia_data/input/fitbit/*/heartrate_intraday/**/*.json\" heartrate = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = heartrate . time . iloc [:- 1 ] end = heartrate . time . iloc [ 1 : ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) heartrate = pd . DataFrame ( heartrate . iloc [:- 1 ]) heartrate = heartrate . set_index ( interval_index ) heartrate [ \"end\" ] = heartrate . index . right - pd . Timedelta ( seconds = 1 ) heartrate = heartrate [ heartrate . time ! = heartrate . end ] heartrate = heartrate [ heartrate . time &lt ; = heartrate . end ] # if nrows is not None : # heartrate = heartrate . iloc [ :nrows ] return cls ( heartrate ) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Heartrate"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#module-nostalgiasourcesfitbitheartrate","text":"View Source import re import os import pandas as pd import just from datetime import datetime from nostalgia . ndf import NDF from nostalgia . times import datetime_from_format def get_day ( fname ) : return \" \" . join ( re . sub ( \".partial_[0-9]+.[0-9]+\" , \"\" , os . path . basename ( fname )). split ( \".\" )[ 1 :- 1 ]) class FitbitHeartrate ( NDF ) : vendor = \"fitbit\" @classmethod def handle_dataframe_per_file ( cls , df , fname ) : if df . empty : return None day = get_day ( fname ) df [ \"time\" ] = [ datetime_from_format ( day + \" \" + x , \"%Y %m %d %H:%M:%S\" ) for x in df . time ] return df @classmethod def download ( cls ) : from nostalgia_fitbit . __ main__ import main main () @classmethod def load ( cls , nrows = None , **kwargs ) : file_glob = \"~/nostalgia_data/input/fitbit/*/heartrate_intraday/**/*.json\" heartrate = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = heartrate . time . iloc [:- 1 ] end = heartrate . time . iloc [ 1 : ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) heartrate = pd . DataFrame ( heartrate . iloc [:- 1 ]) heartrate = heartrate . set_index ( interval_index ) heartrate [ \"end\" ] = heartrate . index . right - pd . Timedelta ( seconds = 1 ) heartrate = heartrate [ heartrate . time ! = heartrate . end ] heartrate = heartrate [ heartrate . time &lt ; = heartrate . end ] # if nrows is not None : # heartrate = heartrate . iloc [ :nrows ] return cls ( heartrate )","title":"Module nostalgia.sources.fitbit.heartrate"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#get_day","text":"def get_day ( fname ) View Source def get_day ( fname ) : return \" \" . join ( re . sub ( \".partial_[0-9]+.[0-9]+\" , \"\" , os . path . basename ( fname )). split ( \".\" )[ 1 :- 1 ])","title":"get_day"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#fitbitheartrate","text":"class FitbitHeartrate ( data ) View Source class FitbitHeartrate ( NDF ) : vendor = \"fitbit\" @classmethod def handle_dataframe_per_file ( cls , df , fname ) : if df . empty : return None day = get_day ( fname ) df [ \"time\" ] = [ datetime_from_format ( day + \" \" + x , \"%Y %m %d %H:%M:%S\" ) for x in df . time ] return df @classmethod def download ( cls ) : from nostalgia_fitbit . __ main__ import main main () @classmethod def load ( cls , nrows = None , **kwargs ) : file_glob = \"~/nostalgia_data/input/fitbit/*/heartrate_intraday/**/*.json\" heartrate = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = heartrate . time . iloc [:- 1 ] end = heartrate . time . iloc [ 1 : ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) heartrate = pd . DataFrame ( heartrate . iloc [:- 1 ]) heartrate = heartrate . set_index ( interval_index ) heartrate [ \"end\" ] = heartrate . index . right - pd . Timedelta ( seconds = 1 ) heartrate = heartrate [ heartrate . time ! = heartrate . end ] heartrate = heartrate [ heartrate . time &lt ; = heartrate . end ] # if nrows is not None : # heartrate = heartrate . iloc [ :nrows ] return cls ( heartrate )","title":"FitbitHeartrate"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#download","text":"def download ( ) View Source @classmethod def download ( cls ): from nostalgia_fitbit.__main__ import main main ()","title":"download"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#handle_dataframe_per_file","text":"def handle_dataframe_per_file ( df , fname ) View Source @classmethod def handle_dataframe_per_file(cls, df, fname): if df.empty: return None day = get_day(fname) df[\"time\"] = [datetime_from_format(day + \" \" + x, \"%Y %m %d %H:%M:%S\") for x in df.time] return df","title":"handle_dataframe_per_file"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#load","text":"def load ( nrows = None , ** kwargs ) View Source @classmethod def load ( cls , nrows = None , **kwargs ) : file_glob = \"~/nostalgia_data/input/fitbit/*/heartrate_intraday/**/*.json\" heartrate = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = heartrate . time . iloc [:- 1 ] end = heartrate . time . iloc [ 1 : ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) heartrate = pd . DataFrame ( heartrate . iloc [:- 1 ]) heartrate = heartrate . set_index ( interval_index ) heartrate [ \"end\" ] = heartrate . index . right - pd . Timedelta ( seconds = 1 ) heartrate = heartrate [ heartrate . time ! = heartrate . end ] heartrate = heartrate [ heartrate . time &lt ; = heartrate . end ] # if nrows is not None : # heartrate = heartrate . iloc [ :nrows ] return cls ( heartrate )","title":"load"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/fitbit/heartrate/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/fitbit/sleep/","text":"Module nostalgia.sources.fitbit.sleep View Source import os import pandas as pd import just from metadate import parse_date from datetime import datetime from nostalgia . ndf import NDF from nostalgia . times import datetime_from_format class FitbitSleep ( NDF ) : vendor = \"fitbit\" @classmethod def load ( cls , nrows = None ) : file_glob = \"~/nostalgia_data/input/fitbit/*/sleep/*.json\" objects = [] for d in just . multi_read ( file_glob ). values () : if not d : continue for x in d : data = pd . DataFrame ( x [ \"levels\" ][ \"data\" ] + [{ 'dateTime': x [ 'endTime' ], 'level': None , 'seconds': None }] ) data [ \"dateTime\" ] = [ datetime_from_format ( x , \"%Y-%m-%dT%H:%M:%S.%f\" ) for x in data . dateTime ] start = data . dateTime . iloc [:- 1 ] end = data . dateTime . iloc [ 1 : ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) data = pd . DataFrame ( data . iloc [:- 1 ]) data = data . set_index ( interval_index ) data [ \"start\" ] = data . index . left data [ \"end\" ] = data . index . right objects . append ( data ) if nrows is not None and data . shape [ 0 ] &gt ; nrows : break data = pd . concat ( objects ). drop ( \"dateTime\" , axis = 1 ) return cls ( data ) @property def asleep ( self ) : return self . query ( \"level != 'wake'\" ) Classes FitbitSleep class FitbitSleep ( data ) View Source class FitbitSleep ( NDF ) : vendor = \"fitbit\" @classmethod def load ( cls , nrows = None ) : file_glob = \"~/nostalgia_data/input/fitbit/*/sleep/*.json\" objects = [] for d in just . multi_read ( file_glob ). values () : if not d : continue for x in d : data = pd . DataFrame ( x [ \"levels\" ][ \"data\" ] + [{ 'dateTime': x [ 'endTime' ], 'level': None , 'seconds': None }] ) data [ \"dateTime\" ] = [ datetime_from_format ( x , \"%Y-%m-%dT%H:%M:%S.%f\" ) for x in data . dateTime ] start = data . dateTime . iloc [:- 1 ] end = data . dateTime . iloc [ 1 : ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) data = pd . DataFrame ( data . iloc [:- 1 ]) data = data . set_index ( interval_index ) data [ \"start\" ] = data . index . left data [ \"end\" ] = data . index . right objects . append ( data ) if nrows is not None and data . shape [ 0 ] &gt ; nrows : break data = pd . concat ( objects ). drop ( \"dateTime\" , axis = 1 ) return cls ( data ) @property def asleep ( self ) : return self . query ( \"level != 'wake'\" ) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None ) View Source @classmethod def load ( cls , nrows = None ) : file_glob = \"~/nostalgia_data/input/fitbit/*/sleep/*.json\" objects = [] for d in just . multi_read ( file_glob ). values () : if not d : continue for x in d : data = pd . DataFrame ( x [ \"levels\" ][ \"data\" ] + [{ 'dateTime': x [ 'endTime' ], 'level': None , 'seconds': None }] ) data [ \"dateTime\" ] = [ datetime_from_format ( x , \"%Y-%m-%dT%H:%M:%S.%f\" ) for x in data . dateTime ] start = data . dateTime . iloc [:- 1 ] end = data . dateTime . iloc [ 1 : ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) data = pd . DataFrame ( data . iloc [:- 1 ]) data = data . set_index ( interval_index ) data [ \"start\" ] = data . index . left data [ \"end\" ] = data . index . right objects . append ( data ) if nrows is not None and data . shape [ 0 ] &gt ; nrows : break data = pd . concat ( objects ). drop ( \"dateTime\" , axis = 1 ) return cls ( data ) Instance variables asleep at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Sleep"},{"location":"reference/nostalgia/sources/fitbit/sleep/#module-nostalgiasourcesfitbitsleep","text":"View Source import os import pandas as pd import just from metadate import parse_date from datetime import datetime from nostalgia . ndf import NDF from nostalgia . times import datetime_from_format class FitbitSleep ( NDF ) : vendor = \"fitbit\" @classmethod def load ( cls , nrows = None ) : file_glob = \"~/nostalgia_data/input/fitbit/*/sleep/*.json\" objects = [] for d in just . multi_read ( file_glob ). values () : if not d : continue for x in d : data = pd . DataFrame ( x [ \"levels\" ][ \"data\" ] + [{ 'dateTime': x [ 'endTime' ], 'level': None , 'seconds': None }] ) data [ \"dateTime\" ] = [ datetime_from_format ( x , \"%Y-%m-%dT%H:%M:%S.%f\" ) for x in data . dateTime ] start = data . dateTime . iloc [:- 1 ] end = data . dateTime . iloc [ 1 : ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) data = pd . DataFrame ( data . iloc [:- 1 ]) data = data . set_index ( interval_index ) data [ \"start\" ] = data . index . left data [ \"end\" ] = data . index . right objects . append ( data ) if nrows is not None and data . shape [ 0 ] &gt ; nrows : break data = pd . concat ( objects ). drop ( \"dateTime\" , axis = 1 ) return cls ( data ) @property def asleep ( self ) : return self . query ( \"level != 'wake'\" )","title":"Module nostalgia.sources.fitbit.sleep"},{"location":"reference/nostalgia/sources/fitbit/sleep/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/fitbit/sleep/#fitbitsleep","text":"class FitbitSleep ( data ) View Source class FitbitSleep ( NDF ) : vendor = \"fitbit\" @classmethod def load ( cls , nrows = None ) : file_glob = \"~/nostalgia_data/input/fitbit/*/sleep/*.json\" objects = [] for d in just . multi_read ( file_glob ). values () : if not d : continue for x in d : data = pd . DataFrame ( x [ \"levels\" ][ \"data\" ] + [{ 'dateTime': x [ 'endTime' ], 'level': None , 'seconds': None }] ) data [ \"dateTime\" ] = [ datetime_from_format ( x , \"%Y-%m-%dT%H:%M:%S.%f\" ) for x in data . dateTime ] start = data . dateTime . iloc [:- 1 ] end = data . dateTime . iloc [ 1 : ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) data = pd . DataFrame ( data . iloc [:- 1 ]) data = data . set_index ( interval_index ) data [ \"start\" ] = data . index . left data [ \"end\" ] = data . index . right objects . append ( data ) if nrows is not None and data . shape [ 0 ] &gt ; nrows : break data = pd . concat ( objects ). drop ( \"dateTime\" , axis = 1 ) return cls ( data ) @property def asleep ( self ) : return self . query ( \"level != 'wake'\" )","title":"FitbitSleep"},{"location":"reference/nostalgia/sources/fitbit/sleep/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/fitbit/sleep/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/fitbit/sleep/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/fitbit/sleep/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/fitbit/sleep/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/fitbit/sleep/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/fitbit/sleep/#load","text":"def load ( nrows = None ) View Source @classmethod def load ( cls , nrows = None ) : file_glob = \"~/nostalgia_data/input/fitbit/*/sleep/*.json\" objects = [] for d in just . multi_read ( file_glob ). values () : if not d : continue for x in d : data = pd . DataFrame ( x [ \"levels\" ][ \"data\" ] + [{ 'dateTime': x [ 'endTime' ], 'level': None , 'seconds': None }] ) data [ \"dateTime\" ] = [ datetime_from_format ( x , \"%Y-%m-%dT%H:%M:%S.%f\" ) for x in data . dateTime ] start = data . dateTime . iloc [:- 1 ] end = data . dateTime . iloc [ 1 : ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) data = pd . DataFrame ( data . iloc [:- 1 ]) data = data . set_index ( interval_index ) data [ \"start\" ] = data . index . left data [ \"end\" ] = data . index . right objects . append ( data ) if nrows is not None and data . shape [ 0 ] &gt ; nrows : break data = pd . concat ( objects ). drop ( \"dateTime\" , axis = 1 ) return cls ( data )","title":"load"},{"location":"reference/nostalgia/sources/fitbit/sleep/#instance-variables","text":"asleep at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/fitbit/sleep/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/fitbit/sleep/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/fitbit/sleep/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/fitbit/sleep/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/fitbit/sleep/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/fitbit/sleep/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/fitbit/sleep/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/fitbit/sleep/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/fitbit/sleep/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/fitbit/sleep/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/fitbit/sleep/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/fitbit/sleep/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/fitbit/sleep/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/fitbit/sleep/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/fitbit/sleep/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/fitbit/sleep/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/fitbit/sleep/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/fitbit/sleep/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/fitbit/sleep/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/fitbit/sleep/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/fitbit/sleep/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/fitbit/sleep/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/fitbit/sleep/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/fitbit/sleep/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/fitbit/sleep/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/fitbit/sleep/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/fitbit/sleep/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/fitbit/sleep/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/fitbit/sleep/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/fitbit/sleep/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/fitbit/sleep/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/fitbit/sleep/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/fitbit/sleep/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/fitbit/sleep/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/fitbit/sleep/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/fitbit/sleep/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/google/","text":"Module nostalgia.sources.google View Source from nostalgia.ndf import NDF class Google ( NDF ): vendor = \"google\" ingest_settings = { \"ingest_glob\" : \"~/Downloads/takeout-20*-*.zip\" , \"recent_only\" : False , \"delete_existing\" : False , } Sub-modules nostalgia.sources.google.app_usage nostalgia.sources.google.flycheck_timeline nostalgia.sources.google.gmail nostalgia.sources.google.page_visit nostalgia.sources.google.photos nostalgia.sources.google.play_music nostalgia.sources.google.timeline nostalgia.sources.google.timeline_download nostalgia.sources.google.timeline_transform Classes Google class Google ( data ) View Source class Google ( NDF ) : vendor = \"google\" ingest_settings = { \"ingest_glob\" : \"~/Downloads/takeout-20*-*.zip\" , \"recent_only\" : False , \"delete_existing\" : False , } Ancestors (in MRO) nostalgia.ndf.NDF Descendants nostalgia.sources.google.app_usage.AppUsage nostalgia.sources.google.gmail.Gmail nostalgia.sources.google.page_visit.PageVisit nostalgia.sources.google.photos.Photos nostalgia.sources.google.play_music.PlayMusic Class variables ingest_settings keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Index"},{"location":"reference/nostalgia/sources/google/#module-nostalgiasourcesgoogle","text":"View Source from nostalgia.ndf import NDF class Google ( NDF ): vendor = \"google\" ingest_settings = { \"ingest_glob\" : \"~/Downloads/takeout-20*-*.zip\" , \"recent_only\" : False , \"delete_existing\" : False , }","title":"Module nostalgia.sources.google"},{"location":"reference/nostalgia/sources/google/#sub-modules","text":"nostalgia.sources.google.app_usage nostalgia.sources.google.flycheck_timeline nostalgia.sources.google.gmail nostalgia.sources.google.page_visit nostalgia.sources.google.photos nostalgia.sources.google.play_music nostalgia.sources.google.timeline nostalgia.sources.google.timeline_download nostalgia.sources.google.timeline_transform","title":"Sub-modules"},{"location":"reference/nostalgia/sources/google/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/google/#google","text":"class Google ( data ) View Source class Google ( NDF ) : vendor = \"google\" ingest_settings = { \"ingest_glob\" : \"~/Downloads/takeout-20*-*.zip\" , \"recent_only\" : False , \"delete_existing\" : False , }","title":"Google"},{"location":"reference/nostalgia/sources/google/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/google/#descendants","text":"nostalgia.sources.google.app_usage.AppUsage nostalgia.sources.google.gmail.Gmail nostalgia.sources.google.page_visit.PageVisit nostalgia.sources.google.photos.Photos nostalgia.sources.google.play_music.PlayMusic","title":"Descendants"},{"location":"reference/nostalgia/sources/google/#class-variables","text":"ingest_settings keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/google/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/google/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/google/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/google/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/google/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/google/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/google/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/google/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/google/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/google/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/google/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/google/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/google/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/google/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/google/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/google/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/google/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/google/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/google/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/google/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/google/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/google/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/google/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/google/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/google/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/google/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/google/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/google/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/google/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/google/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/google/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/google/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/google/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/google/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/google/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/google/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/google/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/google/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/google/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/google/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/google/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/google/app_usage/","text":"Module nostalgia.sources.google.app_usage View Source from nostalgia.times import datetime_from_format from datetime import timedelta from nostalgia.sources.google import Google def custom_parse ( x ): if not isinstance ( x , str ): return x try : return datetime_from_format ( x , \"%Y-%m- %d T%H:%M:%SZ\" , in_utc = True ) except : return datetime_from_format ( x , \"%Y-%m- %d T%H:%M:%S. %f Z\" , in_utc = True ) class AppUsage ( Google ): @classmethod def handle_dataframe_per_file ( cls , data , file_path ): data [ \"time\" ] = [ custom_parse ( x ) for x in data [ \"time\" ]] data = data . rename ( columns = { \"header\" : \"name\" }) return data [[ \"name\" , \"time\" ]] @classmethod def load ( cls , nrows = None , from_cache = True , ** kwargs ): file_path = \"~/nostalgia_data/input/google/Takeout/My Activity/Android/My Activity.json\" data = cls . load_data_file_modified_time ( file_path , nrows = nrows , from_cache = from_cache ) return cls ( data ) Functions custom_parse def custom_parse ( x ) View Source def custom_parse(x): if not isinstance(x, str): return x try: return datetime_from_format(x, \"%Y-%m-%dT%H:%M:%SZ\", in_utc=True) except: return datetime_from_format(x, \"%Y-%m-%dT%H:%M:%S.%fZ\", in_utc=True) Classes AppUsage class AppUsage ( data ) View Source class AppUsage ( Google ) : @classmethod def handle_dataframe_per_file ( cls , data , file_path ) : data [ \"time\" ] = [ custom_parse ( x ) for x in data [ \"time\" ]] data = data . rename ( columns = { \"header\" : \"name\" }) return data [[ \"name\" , \"time\" ]] @classmethod def load ( cls , nrows = None , from_cache = True , ** kwargs ) : file_path = \"~/nostalgia_data/input/google/Takeout/My Activity/Android/My Activity.json\" data = cls . load_data_file_modified_time ( file_path , nrows = nrows , from_cache = from_cache ) return cls ( data ) Ancestors (in MRO) nostalgia.sources.google.Google nostalgia.ndf.NDF Class variables ingest_settings keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} handle_dataframe_per_file def handle_dataframe_per_file ( data , file_path ) View Source @classmethod def handle_dataframe_per_file(cls, data, file_path): data[\"time\"] = [custom_parse(x) for x in data[\"time\"]] data = data.rename(columns={\"header\": \"name\"}) return data[[\"name\", \"time\"]] ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None , from_cache = True , ** kwargs ) View Source @classmethod def load(cls, nrows=None, from_cache=True, **kwargs): file_path = \"~/nostalgia_data/input/google/Takeout/My Activity/Android/My Activity.json\" data = cls.load_data_file_modified_time(file_path, nrows=nrows, from_cache=from_cache) return cls(data) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"App Usage"},{"location":"reference/nostalgia/sources/google/app_usage/#module-nostalgiasourcesgoogleapp_usage","text":"View Source from nostalgia.times import datetime_from_format from datetime import timedelta from nostalgia.sources.google import Google def custom_parse ( x ): if not isinstance ( x , str ): return x try : return datetime_from_format ( x , \"%Y-%m- %d T%H:%M:%SZ\" , in_utc = True ) except : return datetime_from_format ( x , \"%Y-%m- %d T%H:%M:%S. %f Z\" , in_utc = True ) class AppUsage ( Google ): @classmethod def handle_dataframe_per_file ( cls , data , file_path ): data [ \"time\" ] = [ custom_parse ( x ) for x in data [ \"time\" ]] data = data . rename ( columns = { \"header\" : \"name\" }) return data [[ \"name\" , \"time\" ]] @classmethod def load ( cls , nrows = None , from_cache = True , ** kwargs ): file_path = \"~/nostalgia_data/input/google/Takeout/My Activity/Android/My Activity.json\" data = cls . load_data_file_modified_time ( file_path , nrows = nrows , from_cache = from_cache ) return cls ( data )","title":"Module nostalgia.sources.google.app_usage"},{"location":"reference/nostalgia/sources/google/app_usage/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/google/app_usage/#custom_parse","text":"def custom_parse ( x ) View Source def custom_parse(x): if not isinstance(x, str): return x try: return datetime_from_format(x, \"%Y-%m-%dT%H:%M:%SZ\", in_utc=True) except: return datetime_from_format(x, \"%Y-%m-%dT%H:%M:%S.%fZ\", in_utc=True)","title":"custom_parse"},{"location":"reference/nostalgia/sources/google/app_usage/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/google/app_usage/#appusage","text":"class AppUsage ( data ) View Source class AppUsage ( Google ) : @classmethod def handle_dataframe_per_file ( cls , data , file_path ) : data [ \"time\" ] = [ custom_parse ( x ) for x in data [ \"time\" ]] data = data . rename ( columns = { \"header\" : \"name\" }) return data [[ \"name\" , \"time\" ]] @classmethod def load ( cls , nrows = None , from_cache = True , ** kwargs ) : file_path = \"~/nostalgia_data/input/google/Takeout/My Activity/Android/My Activity.json\" data = cls . load_data_file_modified_time ( file_path , nrows = nrows , from_cache = from_cache ) return cls ( data )","title":"AppUsage"},{"location":"reference/nostalgia/sources/google/app_usage/#ancestors-in-mro","text":"nostalgia.sources.google.Google nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/google/app_usage/#class-variables","text":"ingest_settings keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/google/app_usage/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/google/app_usage/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/google/app_usage/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/google/app_usage/#handle_dataframe_per_file","text":"def handle_dataframe_per_file ( data , file_path ) View Source @classmethod def handle_dataframe_per_file(cls, data, file_path): data[\"time\"] = [custom_parse(x) for x in data[\"time\"]] data = data.rename(columns={\"header\": \"name\"}) return data[[\"name\", \"time\"]]","title":"handle_dataframe_per_file"},{"location":"reference/nostalgia/sources/google/app_usage/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/google/app_usage/#load","text":"def load ( nrows = None , from_cache = True , ** kwargs ) View Source @classmethod def load(cls, nrows=None, from_cache=True, **kwargs): file_path = \"~/nostalgia_data/input/google/Takeout/My Activity/Android/My Activity.json\" data = cls.load_data_file_modified_time(file_path, nrows=nrows, from_cache=from_cache) return cls(data)","title":"load"},{"location":"reference/nostalgia/sources/google/app_usage/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/google/app_usage/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/google/app_usage/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/google/app_usage/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/google/app_usage/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/google/app_usage/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/google/app_usage/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/google/app_usage/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/google/app_usage/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/google/app_usage/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/google/app_usage/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/google/app_usage/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/google/app_usage/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/google/app_usage/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/google/app_usage/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/google/app_usage/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/google/app_usage/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/google/app_usage/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/google/app_usage/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/google/app_usage/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/google/app_usage/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/google/app_usage/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/google/app_usage/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/google/app_usage/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/google/app_usage/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/google/app_usage/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/google/app_usage/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/google/app_usage/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/google/app_usage/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/google/app_usage/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/google/app_usage/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/google/app_usage/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/google/app_usage/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/google/app_usage/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/google/app_usage/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/google/app_usage/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/google/app_usage/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/","text":"Module nostalgia.sources.google.flycheck_timeline View Source from datetime import datetime import os import requests import diskcache import dotenv import hashlib import pandas as pd from pytz import timezone import just from nostalgia.times import tz from nostalgia.utils import format_latlng from nostalgia.cache import get_cache from nostalgia.interfaces.places import Places def api_call ( url , params ): key = get_hash ( url , params ) status = CACHE . get ( key , {}) . get ( \"status\" ) if status == \"ZERO_RESULTS\" : return None if status == \"OK\" : return CACHE [ key ] jdata = s . get ( url , params = params ) . json () jdata [ \"meta\" ] = { \"time\" : str ( datetime . utcnow ()), \"url\" : url , \"params\" : { k : v for k , v in params . items () if k != \"key\" }, } if jdata . get ( \"status\" ) == \"ZERO_RESULTS\" : CACHE [ key ] = jdata return None if jdata . get ( \"status\" ) != \"OK\" : print ( \"status\" , url , params , key , jdata . get ( \"status\" )) raise ValueError ( \"not ok\" ) CACHE [ key ] = jdata return jdata def get_nearby ( latlng , name , excluded_transport_names ): latlng = format_latlng ( latlng ) if latlng == \"nan, nan\" : return None params = { \"location\" : latlng , \"radius\" : 100 , \"key\" : KEY } if name not in excluded_transport_names : params [ \"query\" ] = name return api_call ( NEARBY_URL , params ) def get_nearby_results ( latlng , name , excluded_transport_names ): near = get_nearby ( latlng , name , excluded_transport_names ) if near is None : return None res = [ x for x in near [ \"results\" ] if name in excluded_transport_names or x [ \"name\" ] == name ] for x in res : if \"opening_hours\" in x : return x if res : return res [ 0 ] if not near [ \"results\" ]: return None return near [ \"results\" ][ 0 ] def get_details ( place_id ): params = { 'placeid' : place_id , 'sensor' : 'false' , 'key' : KEY } return api_call ( DETAILS_URL , params ) . get ( \"result\" , {}) def get_ ( details , * types ): for tp in types : for addr in details . get ( 'address_components' , []): if tp in addr [ \"types\" ]: return addr [ \"long_name\" ] def get_hash ( url , params ): data = { k : v for k , v in params . items () if k != \"key\" } data [ \"url\" ] = url data = sorted ( data . items ()) return hashlib . sha256 ( str ( data ) . encode ( \"utf8\" )) . hexdigest () GEOCODE_URL = \"https://maps.googleapis.com/maps/api/geocode/json\" def get_address ( json_response ): for res in json_response . get ( \"results\" , []): addr = res . get ( 'formatted_address' ) if addr : return addr def geo_get_ ( json_response , * types ): for res in json_response . get ( \"results\" , []): for tp in types : for addr in res . get ( 'address_components' , []): if tp in addr [ \"types\" ]: return addr [ \"long_name\" ] def geo_get_info ( latlng ): latlng = format_latlng ( latlng ) if latlng == \"nan, nan\" : return None params = { \"method\" : \"reverse\" , \"latlng\" : latlng , \"key\" : KEY } json_response = api_call ( GEOCODE_URL , params = params ) city = geo_get_ ( json_response , \"locality\" , \"postal_town\" ) # place_id = json_response.get(\"results\", [{}])[0].get(\"place_id\") country = geo_get_ ( json_response , \"country\" ) address = get_address ( json_response ) return { \"city\" : city , \"country\" : country , \"formatted_address\" : address } dotenv . load_dotenv ( \"google/.env\" ) dotenv . load_dotenv ( \".env\" ) PYTHON_ENV = os . environ . get ( \"PYTHON_ENV\" , \"dev\" ) if PYTHON_ENV != \"prod\" : KEY = None else : KEY = os . environ . get ( \"GOOGLE_API_KEY\" , None ) CACHE = get_cache ( \"google_timeline\" ) DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\" NEARBY_URL = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json' s = requests . Session () # {'Boating', 'Cycling', 'Driving', 'Flying', 'In transit', 'Moving', 'On a bus', 'On a ferry', 'On a train', 'On a tram', 'On the subway', 'Running', 'Walking'} def get_results ( latlng , name , excluded_transport_names ): if name in excluded_transport_names : return geo_get_info ( latlng ) near_result = get_nearby_results ( latlng , name , excluded_transport_names ) if near_result is None : return None details = get_details ( near_result [ \"place_id\" ]) if details is None : return None return { \"city\" : get_ ( details , \"locality\" , \"postal_town\" ), \"country\" : get_ ( details , \"country\" ), \"rating\" : details , \"details_name\" : details . get ( \"name\" ), \"formatted_address\" : details . get ( \"formatted_address\" ), 'international_phone_number' : details . get ( 'international_phone_number' ), 'opening_hours' : details . get ( 'opening_hours' ), 'user_ratings_total' : details . get ( 'user_ratings_total' ), 'rating' : details . get ( 'rating' ), 'website' : details . get ( 'website' ), } def process ( df , excluded_transport_names , home_regex , work_regex , hometown_regex ): df [ \"start\" ] = pd . to_datetime ( df . start ) df [ \"end\" ] = pd . to_datetime ( df . end ) df [ \"around_noon\" ] = ( df . start . dt . hour & lt ; 12 ) & amp ; ( df . end . dt . hour & gt ; 12 ) df [ \"week\" ] = df . start . dt . weekday & lt ; 5 df [ \"city\" ] = df . city . fillna ( \"\" ) df [ \"transporting\" ] = df . category . isin ( excluded_transport_names ) if work_regex : df . loc [ df . name . str . contains ( work_regex , regex = True , na = False ), \"category\" ] = \"Work\" if home_regex : df . loc [ df . name . str . contains ( home_regex , regex = True , na = False ), \"category\" ] = \"Home\" # if hometown_regex: # df.loc[ # df.name.str.contains(hometown_regex, regex=True, na=False), \"category\" # ] = \"Hometown\" df = df . sort_values ( \"start\" ) . reset_index ( drop = True ) if \"Unnamed: 0\" in df . columns : del df [ \"Unnamed: 0\" ] df [ \"start\" ] = df [ \"start\" ] . dt . tz_convert ( tz ) df [ \"end\" ] = df [ \"end\" ] . dt . tz_convert ( tz ) df . index = pd . IntervalIndex . from_arrays ( df [ 'start' ], df [ 'end' ]) return df class GooglePlaces ( Places ): nlp_columns = [ \"name\" , \"city\" , \"country\" , \"category\" , \"website\" ] selected_columns = [ \"date\" , \"name\" , \"city\" , \"_office_hours\" ] @classmethod def load ( cls , file_glob = \"~/Downloads/timeline_data-*\" , nrows = None ): df = pd . read_csv ( max ( just . glob ( file_glob )), nrows = nrows ) unique_locs = set ([(( y , z ), x ) for x , y , z in zip ( df . name , df . lat , df . lon ) if y != \"nan\" ]) excluded_transport_names = set ( df [ df . name == df . category ] . name ) details_data = [] for ( lat , lon ), name in unique_locs : d = get_results (( lat , lon ), name , excluded_transport_names ) if d is None : continue d [ \"lat\" ] = lat d [ \"lon\" ] = lon d [ \"name\" ] = name details_data . append ( d ) details_data = pd . DataFrame ( details_data ) # all_loc = df.merge(details_data, on=[\"name\", \"lat\", \"lon\"], how=\"outer\") places = df . merge ( details_data , on = [ \"name\" , \"lat\" , \"lon\" ], how = \"inner\" ) # all_loc = process(all_loc, excluded_transport_names) home_regex = \"|\" . join ( cls . home ) work_regex = \"|\" . join ( cls . work ) hometown_regex = \"|\" . join ( cls . hometown ) places = process ( places , excluded_transport_names , home_regex , work_regex , hometown_regex ) return cls ( places ) GooglePlaces . work = [ \"Jibes\" , \"Sleepboot\" , \"De Meerpaal\" , \"Papendorp\" , \"Vektis\" , \"Work \" ] # at_work = places.at_work() # # places.travel_by_car()\\ # .duration_longer_than(minutes=30)\\ # .at_day(at_work) # .at_time(\"november 2019\")\\ # .during_weekdays\\ # cat_str = \", \".join([\"{}: {}\".format(i, x) for i, x in enumerate(places.category)]) # [places.iloc[[int(y) for y in x]] for x in re.findall(\", \".join([\"([0-9]+): {}\".format(x) for x in pats]), cat_str)] Variables CACHE DETAILS_URL GEOCODE_URL KEY NEARBY_URL PYTHON_ENV s Functions api_call def api_call ( url , params ) View Source def api_call(url, params): key = get_hash(url, params) status = CACHE.get(key, {}).get(\"status\") if status == \"ZERO_RESULTS\": return None if status == \"OK\": return CACHE[key] jdata = s.get(url, params=params).json() jdata[\"meta\"] = { \"time\": str(datetime.utcnow()), \"url\": url, \"params\": {k: v for k, v in params.items() if k != \"key\"}, } if jdata.get(\"status\") == \"ZERO_RESULTS\": CACHE[key] = jdata return None if jdata.get(\"status\") != \"OK\": print(\"status\", url, params, key, jdata.get(\"status\")) raise ValueError(\"not ok\") CACHE[key] = jdata return jdata geo_get_ def geo_get_ ( json_response , * types ) View Source def geo_get_(json_response, *types): for res in json_response.get(\"results\", []): for tp in types: for addr in res.get('address_components', []): if tp in addr[\"types\"]: return addr[\"long_name\"] geo_get_info def geo_get_info ( latlng ) View Source def geo_get_info(latlng): latlng = format_latlng(latlng) if latlng == \"nan, nan\": return None params = {\"method\": \"reverse\", \"latlng\": latlng, \"key\": KEY} json_response = api_call(GEOCODE_URL, params=params) city = geo_get_(json_response, \"locality\", \"postal_town\") # place_id = json_response.get(\"results\", [{}])[0].get(\"place_id\") country = geo_get_(json_response, \"country\") address = get_address(json_response) return {\"city\": city, \"country\": country, \"formatted_address\": address} get_ def get_ ( details , * types ) View Source def get_(details, *types): for tp in types: for addr in details.get('address_components', []): if tp in addr[\"types\"]: return addr[\"long_name\"] get_address def get_address ( json_response ) View Source def get_address(json_response): for res in json_response.get(\"results\", []): addr = res.get('formatted_address') if addr: return addr get_details def get_details ( place_id ) View Source def get_details(place_id): params = {'placeid': place_id, 'sensor': 'false', 'key': KEY} return api_call(DETAILS_URL, params).get(\"result\", {}) get_hash def get_hash ( url , params ) View Source def get_hash(url, params): data = {k: v for k, v in params.items() if k != \"key\"} data[\"url\"] = url data = sorted(data.items()) return hashlib.sha256(str(data).encode(\"utf8\")).hexdigest() get_nearby def get_nearby ( latlng , name , excluded_transport_names ) View Source def get_nearby(latlng, name, excluded_transport_names): latlng = format_latlng(latlng) if latlng == \"nan, nan\": return None params = {\"location\": latlng, \"radius\": 100, \"key\": KEY} if name not in excluded_transport_names: params[\"query\"] = name return api_call(NEARBY_URL, params) get_nearby_results def get_nearby_results ( latlng , name , excluded_transport_names ) View Source def get_nearby_results(latlng, name, excluded_transport_names): near = get_nearby(latlng, name, excluded_transport_names) if near is None: return None res = [x for x in near[\"results\"] if name in excluded_transport_names or x[\"name\"] == name] for x in res: if \"opening_hours\" in x: return x if res: return res[0] if not near[\"results\"]: return None return near[\"results\"][0] get_results def get_results ( latlng , name , excluded_transport_names ) View Source def get_results(latlng, name, excluded_transport_names): if name in excluded_transport_names: return geo_get_info(latlng) near_result = get_nearby_results(latlng, name, excluded_transport_names) if near_result is None: return None details = get_details(near_result[\"place_id\"]) if details is None: return None return { \"city\": get_(details, \"locality\", \"postal_town\"), \"country\": get_(details, \"country\"), \"rating\": details, \"details_name\": details.get(\"name\"), \"formatted_address\": details.get(\"formatted_address\"), 'international_phone_number': details.get('international_phone_number'), 'opening_hours': details.get('opening_hours'), 'user_ratings_total': details.get('user_ratings_total'), 'rating': details.get('rating'), 'website': details.get('website'), } process def process ( df , excluded_transport_names , home_regex , work_regex , hometown_regex ) View Source def process ( df , excluded_transport_names , home_regex , work_regex , hometown_regex ): df [ \"start\" ] = pd . to_datetime ( df . start ) df [ \"end\" ] = pd . to_datetime ( df . end ) df [ \"around_noon\" ] = ( df . start . dt . hour &lt ; 12 ) &amp ; ( df . end . dt . hour &gt ; 12 ) df [ \"week\" ] = df . start . dt . weekday &lt ; 5 df [ \"city\" ] = df . city . fillna ( \"\" ) df [ \"transporting\" ] = df . category . isin ( excluded_transport_names ) if work_regex: df . loc [ df . name . str . contains ( work_regex , regex = True , na = False ), \"category\" ] = \"Work\" if home_regex: df . loc [ df . name . str . contains ( home_regex , regex = True , na = False ), \"category\" ] = \"Home\" # if hometown_regex: # df.loc[ # df.name.str.contains(hometown_regex, regex=True, na=False), \"category\" # ] = \"Hometown\" df = df . sort_values ( \"start\" ). reset_index ( drop = True ) if \"Unnamed: 0\" in df . columns: del df [ \"Unnamed: 0\" ] df [ \"start\" ] = df [ \"start\" ]. dt . tz_convert ( tz ) df [ \"end\" ] = df [ \"end\" ]. dt . tz_convert ( tz ) df . index = pd . IntervalIndex . from_arrays ( df [ 'start' ], df [ 'end' ]) return df Classes GooglePlaces class GooglePlaces ( data ) View Source class GooglePlaces ( Places ) : nlp_columns = [ \"name\" , \"city\" , \"country\" , \"category\" , \"website\" ] selected_columns = [ \"date\" , \"name\" , \"city\" , \"_office_hours\" ] @classmethod def load ( cls , file_glob = \"~/Downloads/timeline_data-*\" , nrows = None ) : df = pd . read_csv ( max ( just . glob ( file_glob )), nrows = nrows ) unique_locs = set ([(( y , z ), x ) for x , y , z in zip ( df . name , df . lat , df . lon ) if y != \"nan\" ]) excluded_transport_names = set ( df [ df . name == df . category ]. name ) details_data = [] for ( lat , lon ), name in unique_locs : d = get_results (( lat , lon ), name , excluded_transport_names ) if d is None : continue d [ \"lat\" ] = lat d [ \"lon\" ] = lon d [ \"name\" ] = name details_data . append ( d ) details_data = pd . DataFrame ( details_data ) # all_loc = df . merge ( details_data , on = [ \"name\" , \"lat\" , \"lon\" ], how = \"outer\" ) places = df . merge ( details_data , on = [ \"name\" , \"lat\" , \"lon\" ], how = \"inner\" ) # all_loc = process ( all_loc , excluded_transport_names ) home_regex = \"|\" . join ( cls . home ) work_regex = \"|\" . join ( cls . work ) hometown_regex = \"|\" . join ( cls . hometown ) places = process ( places , excluded_transport_names , home_regex , work_regex , hometown_regex ) return cls ( places ) Ancestors (in MRO) nostalgia.interfaces.places.Places nostalgia.ndf.NDF Class variables home hometown keywords nlp_columns nlp_when selected_columns vendor work Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( file_glob = '~/Downloads/timeline_data-*' , nrows = None ) View Source @classmethod def load(cls, file_glob=\"~/Downloads/timeline_data-*\", nrows=None): df = pd.read_csv(max(just.glob(file_glob)), nrows=nrows) unique_locs = set([((y, z), x) for x, y, z in zip(df.name, df.lat, df.lon) if y != \"nan\"]) excluded_transport_names = set(df[df.name == df.category].name) details_data = [] for (lat, lon), name in unique_locs: d = get_results((lat, lon), name, excluded_transport_names) if d is None: continue d[\"lat\"] = lat d[\"lon\"] = lon d[\"name\"] = name details_data.append(d) details_data = pd.DataFrame(details_data) # all_loc = df.merge(details_data, on=[\"name\", \"lat\", \"lon\"], how=\"outer\") places = df.merge(details_data, on=[\"name\", \"lat\", \"lon\"], how=\"inner\") # all_loc = process(all_loc, excluded_transport_names) home_regex = \"|\".join(cls.home) work_regex = \"|\".join(cls.work) hometown_regex = \"|\".join(cls.hometown) places = process(places, excluded_transport_names, home_regex, work_regex, hometown_regex) return cls(places) Instance variables at_hometown df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time ) View Source def at(self, time): mp = parse_date_tz(time) return self[self.index.overlaps(pd.Interval(mp.start_date, mp.end_date))] at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_home def at_home ( self ) View Source @nlp(\"filter\", \"at home\") def at_home(self): return self.__class__(self[self.category == \"Home\"]) at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) at_work def at_work ( self ) View Source @nlp(\"filter\", \"at work\") def at_work(self): return self.__class__(self[self.category == \"Work\"]) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) length def length ( self ) View Source @nlp(\"end\", \"how long\", \"how much time\") def length(self): return (self.index.right - self.index.left).to_pytimedelta().sum() near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) near_ def near_ ( self , distance_in_meters , other_places = None ) View Source def near_(self, distance_in_meters, other_places=None): places = get_type_from_registry(\"places\") if other_places is None else other_places nearbies = [] for _, row in places[[\"lat\", \"lon\"]].drop_duplicates().iterrows(): nearbies.append(haversine(self.lat, self.lon, *row) &lt; distance_in_meters) return self.__class__(self[np.any(nearbies, axis=0)]) near_home def near_home ( self , distance = 1000 ) View Source @nlp(\"filter\", [\"near home\", \"around home\"]) def near_home(self, distance=1000): return self.near_(distance, get_type_from_registry(\"places\").at_home) near_work def near_work ( self , distance = 1000 ) View Source @nlp(\"filter\", [\"near work\", \"around work\"]) def near_work(self, distance=1000): return self.near_(distance, get_type_from_registry(\"places\").at_work) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) sum def sum ( self ) View Source @nlp(\"end\", \"how much\") def sum(self): return self.distance.sum() tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) travel def travel ( self ) View Source @nlp(\"filter\", \"travel\") def travel(self): return self[self.transporting] travel_by_bus def travel_by_bus ( self ) View Source @nlp( \"filter\", \"drive by bus\", \"by bus\", \"travel by bus\", \"using the bus\", \"going by bus\", \"on a bus\", ) def travel_by_bus(self): return self.col_contains(\"On a bus\", \"category\") travel_by_car def travel_by_car ( self ) View Source @nlp( \"filter\", \"drive\", \"by car\", \"travel by car\", \"travel using the car\", \"by car\", \"going by car\", \"on a car\", \"driving\", ) def travel_by_car(self): return self.col_contains(\"Driving\", \"category\") travel_by_train def travel_by_train ( self ) View Source @nlp( \"filter\", \"drive by train\", \"by train\", \"travel by train\", \"using the train\", \"going by train\", \"on a train\", ) def travel_by_train(self): return self.col_contains(\"On a train\", \"category\") view def view ( self , index ) View Source def view(self, index): view(self.path[index]) what_address def what_address ( self ) View Source @nlp(\"end\", \"address of\", \"what is the address\", \"how to find\", \"how can i find\") def what_address(self): return self[\"address\"] when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) work_days def work_days ( self ) View Source @nlp(\"filter\", \"work days\", \"work-days\", \"on working days\") def work_days(self): return self[self.time.dt.weekday &lt; 5] work_hours def work_hours ( self ) View Source @nlp(\"filter\", \"during work\", \"during work hours\") def work_hours(self): return self[self._office_hours]","title":"Flycheck Timeline"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#module-nostalgiasourcesgoogleflycheck_timeline","text":"View Source from datetime import datetime import os import requests import diskcache import dotenv import hashlib import pandas as pd from pytz import timezone import just from nostalgia.times import tz from nostalgia.utils import format_latlng from nostalgia.cache import get_cache from nostalgia.interfaces.places import Places def api_call ( url , params ): key = get_hash ( url , params ) status = CACHE . get ( key , {}) . get ( \"status\" ) if status == \"ZERO_RESULTS\" : return None if status == \"OK\" : return CACHE [ key ] jdata = s . get ( url , params = params ) . json () jdata [ \"meta\" ] = { \"time\" : str ( datetime . utcnow ()), \"url\" : url , \"params\" : { k : v for k , v in params . items () if k != \"key\" }, } if jdata . get ( \"status\" ) == \"ZERO_RESULTS\" : CACHE [ key ] = jdata return None if jdata . get ( \"status\" ) != \"OK\" : print ( \"status\" , url , params , key , jdata . get ( \"status\" )) raise ValueError ( \"not ok\" ) CACHE [ key ] = jdata return jdata def get_nearby ( latlng , name , excluded_transport_names ): latlng = format_latlng ( latlng ) if latlng == \"nan, nan\" : return None params = { \"location\" : latlng , \"radius\" : 100 , \"key\" : KEY } if name not in excluded_transport_names : params [ \"query\" ] = name return api_call ( NEARBY_URL , params ) def get_nearby_results ( latlng , name , excluded_transport_names ): near = get_nearby ( latlng , name , excluded_transport_names ) if near is None : return None res = [ x for x in near [ \"results\" ] if name in excluded_transport_names or x [ \"name\" ] == name ] for x in res : if \"opening_hours\" in x : return x if res : return res [ 0 ] if not near [ \"results\" ]: return None return near [ \"results\" ][ 0 ] def get_details ( place_id ): params = { 'placeid' : place_id , 'sensor' : 'false' , 'key' : KEY } return api_call ( DETAILS_URL , params ) . get ( \"result\" , {}) def get_ ( details , * types ): for tp in types : for addr in details . get ( 'address_components' , []): if tp in addr [ \"types\" ]: return addr [ \"long_name\" ] def get_hash ( url , params ): data = { k : v for k , v in params . items () if k != \"key\" } data [ \"url\" ] = url data = sorted ( data . items ()) return hashlib . sha256 ( str ( data ) . encode ( \"utf8\" )) . hexdigest () GEOCODE_URL = \"https://maps.googleapis.com/maps/api/geocode/json\" def get_address ( json_response ): for res in json_response . get ( \"results\" , []): addr = res . get ( 'formatted_address' ) if addr : return addr def geo_get_ ( json_response , * types ): for res in json_response . get ( \"results\" , []): for tp in types : for addr in res . get ( 'address_components' , []): if tp in addr [ \"types\" ]: return addr [ \"long_name\" ] def geo_get_info ( latlng ): latlng = format_latlng ( latlng ) if latlng == \"nan, nan\" : return None params = { \"method\" : \"reverse\" , \"latlng\" : latlng , \"key\" : KEY } json_response = api_call ( GEOCODE_URL , params = params ) city = geo_get_ ( json_response , \"locality\" , \"postal_town\" ) # place_id = json_response.get(\"results\", [{}])[0].get(\"place_id\") country = geo_get_ ( json_response , \"country\" ) address = get_address ( json_response ) return { \"city\" : city , \"country\" : country , \"formatted_address\" : address } dotenv . load_dotenv ( \"google/.env\" ) dotenv . load_dotenv ( \".env\" ) PYTHON_ENV = os . environ . get ( \"PYTHON_ENV\" , \"dev\" ) if PYTHON_ENV != \"prod\" : KEY = None else : KEY = os . environ . get ( \"GOOGLE_API_KEY\" , None ) CACHE = get_cache ( \"google_timeline\" ) DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\" NEARBY_URL = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json' s = requests . Session () # {'Boating', 'Cycling', 'Driving', 'Flying', 'In transit', 'Moving', 'On a bus', 'On a ferry', 'On a train', 'On a tram', 'On the subway', 'Running', 'Walking'} def get_results ( latlng , name , excluded_transport_names ): if name in excluded_transport_names : return geo_get_info ( latlng ) near_result = get_nearby_results ( latlng , name , excluded_transport_names ) if near_result is None : return None details = get_details ( near_result [ \"place_id\" ]) if details is None : return None return { \"city\" : get_ ( details , \"locality\" , \"postal_town\" ), \"country\" : get_ ( details , \"country\" ), \"rating\" : details , \"details_name\" : details . get ( \"name\" ), \"formatted_address\" : details . get ( \"formatted_address\" ), 'international_phone_number' : details . get ( 'international_phone_number' ), 'opening_hours' : details . get ( 'opening_hours' ), 'user_ratings_total' : details . get ( 'user_ratings_total' ), 'rating' : details . get ( 'rating' ), 'website' : details . get ( 'website' ), } def process ( df , excluded_transport_names , home_regex , work_regex , hometown_regex ): df [ \"start\" ] = pd . to_datetime ( df . start ) df [ \"end\" ] = pd . to_datetime ( df . end ) df [ \"around_noon\" ] = ( df . start . dt . hour & lt ; 12 ) & amp ; ( df . end . dt . hour & gt ; 12 ) df [ \"week\" ] = df . start . dt . weekday & lt ; 5 df [ \"city\" ] = df . city . fillna ( \"\" ) df [ \"transporting\" ] = df . category . isin ( excluded_transport_names ) if work_regex : df . loc [ df . name . str . contains ( work_regex , regex = True , na = False ), \"category\" ] = \"Work\" if home_regex : df . loc [ df . name . str . contains ( home_regex , regex = True , na = False ), \"category\" ] = \"Home\" # if hometown_regex: # df.loc[ # df.name.str.contains(hometown_regex, regex=True, na=False), \"category\" # ] = \"Hometown\" df = df . sort_values ( \"start\" ) . reset_index ( drop = True ) if \"Unnamed: 0\" in df . columns : del df [ \"Unnamed: 0\" ] df [ \"start\" ] = df [ \"start\" ] . dt . tz_convert ( tz ) df [ \"end\" ] = df [ \"end\" ] . dt . tz_convert ( tz ) df . index = pd . IntervalIndex . from_arrays ( df [ 'start' ], df [ 'end' ]) return df class GooglePlaces ( Places ): nlp_columns = [ \"name\" , \"city\" , \"country\" , \"category\" , \"website\" ] selected_columns = [ \"date\" , \"name\" , \"city\" , \"_office_hours\" ] @classmethod def load ( cls , file_glob = \"~/Downloads/timeline_data-*\" , nrows = None ): df = pd . read_csv ( max ( just . glob ( file_glob )), nrows = nrows ) unique_locs = set ([(( y , z ), x ) for x , y , z in zip ( df . name , df . lat , df . lon ) if y != \"nan\" ]) excluded_transport_names = set ( df [ df . name == df . category ] . name ) details_data = [] for ( lat , lon ), name in unique_locs : d = get_results (( lat , lon ), name , excluded_transport_names ) if d is None : continue d [ \"lat\" ] = lat d [ \"lon\" ] = lon d [ \"name\" ] = name details_data . append ( d ) details_data = pd . DataFrame ( details_data ) # all_loc = df.merge(details_data, on=[\"name\", \"lat\", \"lon\"], how=\"outer\") places = df . merge ( details_data , on = [ \"name\" , \"lat\" , \"lon\" ], how = \"inner\" ) # all_loc = process(all_loc, excluded_transport_names) home_regex = \"|\" . join ( cls . home ) work_regex = \"|\" . join ( cls . work ) hometown_regex = \"|\" . join ( cls . hometown ) places = process ( places , excluded_transport_names , home_regex , work_regex , hometown_regex ) return cls ( places ) GooglePlaces . work = [ \"Jibes\" , \"Sleepboot\" , \"De Meerpaal\" , \"Papendorp\" , \"Vektis\" , \"Work \" ] # at_work = places.at_work() # # places.travel_by_car()\\ # .duration_longer_than(minutes=30)\\ # .at_day(at_work) # .at_time(\"november 2019\")\\ # .during_weekdays\\ # cat_str = \", \".join([\"{}: {}\".format(i, x) for i, x in enumerate(places.category)]) # [places.iloc[[int(y) for y in x]] for x in re.findall(\", \".join([\"([0-9]+): {}\".format(x) for x in pats]), cat_str)]","title":"Module nostalgia.sources.google.flycheck_timeline"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#variables","text":"CACHE DETAILS_URL GEOCODE_URL KEY NEARBY_URL PYTHON_ENV s","title":"Variables"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#api_call","text":"def api_call ( url , params ) View Source def api_call(url, params): key = get_hash(url, params) status = CACHE.get(key, {}).get(\"status\") if status == \"ZERO_RESULTS\": return None if status == \"OK\": return CACHE[key] jdata = s.get(url, params=params).json() jdata[\"meta\"] = { \"time\": str(datetime.utcnow()), \"url\": url, \"params\": {k: v for k, v in params.items() if k != \"key\"}, } if jdata.get(\"status\") == \"ZERO_RESULTS\": CACHE[key] = jdata return None if jdata.get(\"status\") != \"OK\": print(\"status\", url, params, key, jdata.get(\"status\")) raise ValueError(\"not ok\") CACHE[key] = jdata return jdata","title":"api_call"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#geo_get_","text":"def geo_get_ ( json_response , * types ) View Source def geo_get_(json_response, *types): for res in json_response.get(\"results\", []): for tp in types: for addr in res.get('address_components', []): if tp in addr[\"types\"]: return addr[\"long_name\"]","title":"geo_get_"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#geo_get_info","text":"def geo_get_info ( latlng ) View Source def geo_get_info(latlng): latlng = format_latlng(latlng) if latlng == \"nan, nan\": return None params = {\"method\": \"reverse\", \"latlng\": latlng, \"key\": KEY} json_response = api_call(GEOCODE_URL, params=params) city = geo_get_(json_response, \"locality\", \"postal_town\") # place_id = json_response.get(\"results\", [{}])[0].get(\"place_id\") country = geo_get_(json_response, \"country\") address = get_address(json_response) return {\"city\": city, \"country\": country, \"formatted_address\": address}","title":"geo_get_info"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#get_","text":"def get_ ( details , * types ) View Source def get_(details, *types): for tp in types: for addr in details.get('address_components', []): if tp in addr[\"types\"]: return addr[\"long_name\"]","title":"get_"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#get_address","text":"def get_address ( json_response ) View Source def get_address(json_response): for res in json_response.get(\"results\", []): addr = res.get('formatted_address') if addr: return addr","title":"get_address"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#get_details","text":"def get_details ( place_id ) View Source def get_details(place_id): params = {'placeid': place_id, 'sensor': 'false', 'key': KEY} return api_call(DETAILS_URL, params).get(\"result\", {})","title":"get_details"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#get_hash","text":"def get_hash ( url , params ) View Source def get_hash(url, params): data = {k: v for k, v in params.items() if k != \"key\"} data[\"url\"] = url data = sorted(data.items()) return hashlib.sha256(str(data).encode(\"utf8\")).hexdigest()","title":"get_hash"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#get_nearby","text":"def get_nearby ( latlng , name , excluded_transport_names ) View Source def get_nearby(latlng, name, excluded_transport_names): latlng = format_latlng(latlng) if latlng == \"nan, nan\": return None params = {\"location\": latlng, \"radius\": 100, \"key\": KEY} if name not in excluded_transport_names: params[\"query\"] = name return api_call(NEARBY_URL, params)","title":"get_nearby"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#get_nearby_results","text":"def get_nearby_results ( latlng , name , excluded_transport_names ) View Source def get_nearby_results(latlng, name, excluded_transport_names): near = get_nearby(latlng, name, excluded_transport_names) if near is None: return None res = [x for x in near[\"results\"] if name in excluded_transport_names or x[\"name\"] == name] for x in res: if \"opening_hours\" in x: return x if res: return res[0] if not near[\"results\"]: return None return near[\"results\"][0]","title":"get_nearby_results"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#get_results","text":"def get_results ( latlng , name , excluded_transport_names ) View Source def get_results(latlng, name, excluded_transport_names): if name in excluded_transport_names: return geo_get_info(latlng) near_result = get_nearby_results(latlng, name, excluded_transport_names) if near_result is None: return None details = get_details(near_result[\"place_id\"]) if details is None: return None return { \"city\": get_(details, \"locality\", \"postal_town\"), \"country\": get_(details, \"country\"), \"rating\": details, \"details_name\": details.get(\"name\"), \"formatted_address\": details.get(\"formatted_address\"), 'international_phone_number': details.get('international_phone_number'), 'opening_hours': details.get('opening_hours'), 'user_ratings_total': details.get('user_ratings_total'), 'rating': details.get('rating'), 'website': details.get('website'), }","title":"get_results"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#process","text":"def process ( df , excluded_transport_names , home_regex , work_regex , hometown_regex ) View Source def process ( df , excluded_transport_names , home_regex , work_regex , hometown_regex ): df [ \"start\" ] = pd . to_datetime ( df . start ) df [ \"end\" ] = pd . to_datetime ( df . end ) df [ \"around_noon\" ] = ( df . start . dt . hour &lt ; 12 ) &amp ; ( df . end . dt . hour &gt ; 12 ) df [ \"week\" ] = df . start . dt . weekday &lt ; 5 df [ \"city\" ] = df . city . fillna ( \"\" ) df [ \"transporting\" ] = df . category . isin ( excluded_transport_names ) if work_regex: df . loc [ df . name . str . contains ( work_regex , regex = True , na = False ), \"category\" ] = \"Work\" if home_regex: df . loc [ df . name . str . contains ( home_regex , regex = True , na = False ), \"category\" ] = \"Home\" # if hometown_regex: # df.loc[ # df.name.str.contains(hometown_regex, regex=True, na=False), \"category\" # ] = \"Hometown\" df = df . sort_values ( \"start\" ). reset_index ( drop = True ) if \"Unnamed: 0\" in df . columns: del df [ \"Unnamed: 0\" ] df [ \"start\" ] = df [ \"start\" ]. dt . tz_convert ( tz ) df [ \"end\" ] = df [ \"end\" ]. dt . tz_convert ( tz ) df . index = pd . IntervalIndex . from_arrays ( df [ 'start' ], df [ 'end' ]) return df","title":"process"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#googleplaces","text":"class GooglePlaces ( data ) View Source class GooglePlaces ( Places ) : nlp_columns = [ \"name\" , \"city\" , \"country\" , \"category\" , \"website\" ] selected_columns = [ \"date\" , \"name\" , \"city\" , \"_office_hours\" ] @classmethod def load ( cls , file_glob = \"~/Downloads/timeline_data-*\" , nrows = None ) : df = pd . read_csv ( max ( just . glob ( file_glob )), nrows = nrows ) unique_locs = set ([(( y , z ), x ) for x , y , z in zip ( df . name , df . lat , df . lon ) if y != \"nan\" ]) excluded_transport_names = set ( df [ df . name == df . category ]. name ) details_data = [] for ( lat , lon ), name in unique_locs : d = get_results (( lat , lon ), name , excluded_transport_names ) if d is None : continue d [ \"lat\" ] = lat d [ \"lon\" ] = lon d [ \"name\" ] = name details_data . append ( d ) details_data = pd . DataFrame ( details_data ) # all_loc = df . merge ( details_data , on = [ \"name\" , \"lat\" , \"lon\" ], how = \"outer\" ) places = df . merge ( details_data , on = [ \"name\" , \"lat\" , \"lon\" ], how = \"inner\" ) # all_loc = process ( all_loc , excluded_transport_names ) home_regex = \"|\" . join ( cls . home ) work_regex = \"|\" . join ( cls . work ) hometown_regex = \"|\" . join ( cls . hometown ) places = process ( places , excluded_transport_names , home_regex , work_regex , hometown_regex ) return cls ( places )","title":"GooglePlaces"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#ancestors-in-mro","text":"nostalgia.interfaces.places.Places nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#class-variables","text":"home hometown keywords nlp_columns nlp_when selected_columns vendor work","title":"Class variables"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#load","text":"def load ( file_glob = '~/Downloads/timeline_data-*' , nrows = None ) View Source @classmethod def load(cls, file_glob=\"~/Downloads/timeline_data-*\", nrows=None): df = pd.read_csv(max(just.glob(file_glob)), nrows=nrows) unique_locs = set([((y, z), x) for x, y, z in zip(df.name, df.lat, df.lon) if y != \"nan\"]) excluded_transport_names = set(df[df.name == df.category].name) details_data = [] for (lat, lon), name in unique_locs: d = get_results((lat, lon), name, excluded_transport_names) if d is None: continue d[\"lat\"] = lat d[\"lon\"] = lon d[\"name\"] = name details_data.append(d) details_data = pd.DataFrame(details_data) # all_loc = df.merge(details_data, on=[\"name\", \"lat\", \"lon\"], how=\"outer\") places = df.merge(details_data, on=[\"name\", \"lat\", \"lon\"], how=\"inner\") # all_loc = process(all_loc, excluded_transport_names) home_regex = \"|\".join(cls.home) work_regex = \"|\".join(cls.work) hometown_regex = \"|\".join(cls.hometown) places = process(places, excluded_transport_names, home_regex, work_regex, hometown_regex) return cls(places)","title":"load"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#instance-variables","text":"at_hometown df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#at","text":"def at ( self , time ) View Source def at(self, time): mp = parse_date_tz(time) return self[self.index.overlaps(pd.Interval(mp.start_date, mp.end_date))]","title":"at"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#at_home","text":"def at_home ( self ) View Source @nlp(\"filter\", \"at home\") def at_home(self): return self.__class__(self[self.category == \"Home\"])","title":"at_home"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#at_work","text":"def at_work ( self ) View Source @nlp(\"filter\", \"at work\") def at_work(self): return self.__class__(self[self.category == \"Work\"])","title":"at_work"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#length","text":"def length ( self ) View Source @nlp(\"end\", \"how long\", \"how much time\") def length(self): return (self.index.right - self.index.left).to_pytimedelta().sum()","title":"length"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#near_","text":"def near_ ( self , distance_in_meters , other_places = None ) View Source def near_(self, distance_in_meters, other_places=None): places = get_type_from_registry(\"places\") if other_places is None else other_places nearbies = [] for _, row in places[[\"lat\", \"lon\"]].drop_duplicates().iterrows(): nearbies.append(haversine(self.lat, self.lon, *row) &lt; distance_in_meters) return self.__class__(self[np.any(nearbies, axis=0)])","title":"near_"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#near_home","text":"def near_home ( self , distance = 1000 ) View Source @nlp(\"filter\", [\"near home\", \"around home\"]) def near_home(self, distance=1000): return self.near_(distance, get_type_from_registry(\"places\").at_home)","title":"near_home"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#near_work","text":"def near_work ( self , distance = 1000 ) View Source @nlp(\"filter\", [\"near work\", \"around work\"]) def near_work(self, distance=1000): return self.near_(distance, get_type_from_registry(\"places\").at_work)","title":"near_work"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#sum","text":"def sum ( self ) View Source @nlp(\"end\", \"how much\") def sum(self): return self.distance.sum()","title":"sum"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#travel","text":"def travel ( self ) View Source @nlp(\"filter\", \"travel\") def travel(self): return self[self.transporting]","title":"travel"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#travel_by_bus","text":"def travel_by_bus ( self ) View Source @nlp( \"filter\", \"drive by bus\", \"by bus\", \"travel by bus\", \"using the bus\", \"going by bus\", \"on a bus\", ) def travel_by_bus(self): return self.col_contains(\"On a bus\", \"category\")","title":"travel_by_bus"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#travel_by_car","text":"def travel_by_car ( self ) View Source @nlp( \"filter\", \"drive\", \"by car\", \"travel by car\", \"travel using the car\", \"by car\", \"going by car\", \"on a car\", \"driving\", ) def travel_by_car(self): return self.col_contains(\"Driving\", \"category\")","title":"travel_by_car"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#travel_by_train","text":"def travel_by_train ( self ) View Source @nlp( \"filter\", \"drive by train\", \"by train\", \"travel by train\", \"using the train\", \"going by train\", \"on a train\", ) def travel_by_train(self): return self.col_contains(\"On a train\", \"category\")","title":"travel_by_train"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#what_address","text":"def what_address ( self ) View Source @nlp(\"end\", \"address of\", \"what is the address\", \"how to find\", \"how can i find\") def what_address(self): return self[\"address\"]","title":"what_address"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#work_days","text":"def work_days ( self ) View Source @nlp(\"filter\", \"work days\", \"work-days\", \"on working days\") def work_days(self): return self[self.time.dt.weekday &lt; 5]","title":"work_days"},{"location":"reference/nostalgia/sources/google/flycheck_timeline/#work_hours","text":"def work_hours ( self ) View Source @nlp(\"filter\", \"during work\", \"during work hours\") def work_hours(self): return self[self._office_hours]","title":"work_hours"},{"location":"reference/nostalgia/sources/google/gmail/","text":"Module nostalgia.sources.google.gmail View Source import just import pandas as pd from datetime import datetime from nostalgia.times import tz , parse from nostalgia.data_loading import read_array_of_dict_from_json from nostalgia.sources.google import Google def try_parse ( x ): try : if x . endswith ( \"PST\" ): x = x . replace ( \"PST\" , \"-0800 (PST)\" ) elif x . endswith ( \"PDT\" ): x = x . replace ( \"PDT\" , \"-0700 (PDT)\" ) d = parse ( x ) if d . tzinfo is None : d = d . replace ( tzinfo = tz ) return d except : return datetime ( 1970 , 1 , 1 , 0 , 0 , 0 , tzinfo = tz ) class Gmail ( Google ): me = [] @classmethod def handle_dataframe_per_file ( cls , data , fname ): data [ \"subject\" ] = data [ \"subject\" ] . astype ( str ) data [ \"to\" ] = data [ \"to\" ] . astype ( str ) data [ \"sender\" ] = data [ \"from\" ] . str . extract ( \"&lt;([^&gt;]+)&gt;\" ) data . loc [ data [ \"sender\" ] . isnull (), \"sender\" ] = data [ data [ \"sender\" ] . isnull ()][ \"from\" ] . str . strip ( '\"' ) data [ \"sent\" ] = data . sender . str . contains ( \"|\" . join ( cls . me ), na = False ) data [ \"receiver\" ] = data [ \"to\" ] . str . extract ( \"&lt;([^&gt;]+)&gt;\" ) data . loc [ data [ \"receiver\" ] . isnull (), \"receiver\" ] = data . loc [ data [ \"receiver\" ] . isnull (), \"to\" ] data [ \"received\" ] = data . receiver . str . contains ( \"|\" . join ( cls . me ), na = False ) data [ \"timestamp\" ] = pd . to_datetime ([ try_parse ( x ) for x in data . date ], utc = True ) . tz_convert ( tz ) data . drop ( \"date\" , axis = 1 , inplace = True ) return data # refactor to use \"google_takeout\" as target @classmethod def load ( cls , fname , nrows = None , from_cache = True ): emails = cls . load_data_file_modified_time ( fname , nrows = nrows , from_cache = from_cache ) return cls ( emails ) def sent_by ( self , name = None , email = None , case = False ): if name is not None and email is not None : a = self . sender . str . contains ( name , case = case , na = False ) b = self . sender . str . contains ( email , case = case , na = False ) res = self [ a | b ] elif name is not None : res = self [ self . sender . str . contains ( name , case = case , na = False )] elif email is not None : res = self [ self . sender . str . contains ( email , case = case , na = False )] return self . __class__ ( res ) def received_by ( self , name = None , email = None , case = False ): if name is not None and email is not None : a = self . receiver . str . contains ( name , case = case , na = False ) b = self . receiver . str . contains ( email , case = case , na = False ) res = self [ a | b ] elif name is not None : res = self [ self . receiver . str . contains ( name , case = case , na = False )] elif email is not None : res = self [ self . receiver . str . contains ( email , case = case , na = False )] return self . __class__ ( res ) Functions try_parse def try_parse ( x ) View Source def try_parse(x): try: if x.endswith(\"PST\"): x = x.replace(\"PST\", \"-0800 (PST)\") elif x.endswith(\"PDT\"): x = x.replace(\"PDT\", \"-0700 (PDT)\") d = parse(x) if d.tzinfo is None: d = d.replace(tzinfo=tz) return d except: return datetime(1970, 1, 1, 0, 0, 0, tzinfo=tz) Classes Gmail class Gmail ( data ) View Source class Gmail ( Google ) : me = [] @classmethod def handle_dataframe_per_file ( cls , data , fname ) : data [ \"subject\" ] = data [ \"subject\" ]. astype ( str ) data [ \"to\" ] = data [ \"to\" ]. astype ( str ) data [ \"sender\" ] = data [ \"from\" ]. str . extract ( \"&lt;([^&gt;]+)&gt;\" ) data . loc [ data [ \"sender\" ]. isnull (), \"sender\" ] = data [ data [ \"sender\" ]. isnull ()][ \"from\" ]. str . strip ( '\"' ) data [ \"sent\" ] = data . sender . str . contains ( \"|\" . join ( cls . me ), na = False ) data [ \"receiver\" ] = data [ \"to\" ]. str . extract ( \"&lt;([^&gt;]+)&gt;\" ) data . loc [ data [ \"receiver\" ]. isnull (), \"receiver\" ] = data . loc [ data [ \"receiver\" ]. isnull (), \"to\" ] data [ \"received\" ] = data . receiver . str . contains ( \"|\" . join ( cls . me ), na = False ) data [ \"timestamp\" ] = pd . to_datetime ([ try_parse ( x ) for x in data . date ], utc = True ). tz_convert ( tz ) data . drop ( \"date\" , axis = 1 , inplace = True ) return data # refactor to use \"google_takeout\" as target @classmethod def load ( cls , fname , nrows = None , from_cache = True ) : emails = cls . load_data_file_modified_time ( fname , nrows = nrows , from_cache = from_cache ) return cls ( emails ) def sent_by ( self , name = None , email = None , case = False ) : if name is not None and email is not None : a = self . sender . str . contains ( name , case = case , na = False ) b = self . sender . str . contains ( email , case = case , na = False ) res = self [ a | b ] elif name is not None : res = self [ self . sender . str . contains ( name , case = case , na = False )] elif email is not None : res = self [ self . sender . str . contains ( email , case = case , na = False )] return self . __class__ ( res ) def received_by ( self , name = None , email = None , case = False ) : if name is not None and email is not None : a = self . receiver . str . contains ( name , case = case , na = False ) b = self . receiver . str . contains ( email , case = case , na = False ) res = self [ a | b ] elif name is not None : res = self [ self . receiver . str . contains ( name , case = case , na = False )] elif email is not None : res = self [ self . receiver . str . contains ( email , case = case , na = False )] return self . __class__ ( res ) Ancestors (in MRO) nostalgia.sources.google.Google nostalgia.ndf.NDF Class variables ingest_settings keywords me nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} handle_dataframe_per_file def handle_dataframe_per_file ( data , fname ) View Source @classmethod def handle_dataframe_per_file(cls, data, fname): data[\"subject\"] = data[\"subject\"].astype(str) data[\"to\"] = data[\"to\"].astype(str) data[\"sender\"] = data[\"from\"].str.extract(\"&lt;([^&gt;]+)&gt;\") data.loc[data[\"sender\"].isnull(), \"sender\"] = data[data[\"sender\"].isnull()][ \"from\" ].str.strip('\"') data[\"sent\"] = data.sender.str.contains(\"|\".join(cls.me), na=False) data[\"receiver\"] = data[\"to\"].str.extract(\"&lt;([^&gt;]+)&gt;\") data.loc[data[\"receiver\"].isnull(), \"receiver\"] = data.loc[data[\"receiver\"].isnull(), \"to\"] data[\"received\"] = data.receiver.str.contains(\"|\".join(cls.me), na=False) data[\"timestamp\"] = pd.to_datetime([try_parse(x) for x in data.date], utc=True).tz_convert( tz ) data.drop(\"date\", axis=1, inplace=True) return data ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( fname , nrows = None , from_cache = True ) View Source @classmethod def load(cls, fname, nrows=None, from_cache=True): emails = cls.load_data_file_modified_time(fname, nrows=nrows, from_cache=from_cache) return cls(emails) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) received_by def received_by ( self , name = None , email = None , case = False ) View Source def received_by(self, name=None, email=None, case=False): if name is not None and email is not None: a = self.receiver.str.contains(name, case=case, na=False) b = self.receiver.str.contains(email, case=case, na=False) res = self[a | b] elif name is not None: res = self[self.receiver.str.contains(name, case=case, na=False)] elif email is not None: res = self[self.receiver.str.contains(email, case=case, na=False)] return self.__class__(res) sent_by def sent_by ( self , name = None , email = None , case = False ) View Source def sent_by(self, name=None, email=None, case=False): if name is not None and email is not None: a = self.sender.str.contains(name, case=case, na=False) b = self.sender.str.contains(email, case=case, na=False) res = self[a | b] elif name is not None: res = self[self.sender.str.contains(name, case=case, na=False)] elif email is not None: res = self[self.sender.str.contains(email, case=case, na=False)] return self.__class__(res) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Gmail"},{"location":"reference/nostalgia/sources/google/gmail/#module-nostalgiasourcesgooglegmail","text":"View Source import just import pandas as pd from datetime import datetime from nostalgia.times import tz , parse from nostalgia.data_loading import read_array_of_dict_from_json from nostalgia.sources.google import Google def try_parse ( x ): try : if x . endswith ( \"PST\" ): x = x . replace ( \"PST\" , \"-0800 (PST)\" ) elif x . endswith ( \"PDT\" ): x = x . replace ( \"PDT\" , \"-0700 (PDT)\" ) d = parse ( x ) if d . tzinfo is None : d = d . replace ( tzinfo = tz ) return d except : return datetime ( 1970 , 1 , 1 , 0 , 0 , 0 , tzinfo = tz ) class Gmail ( Google ): me = [] @classmethod def handle_dataframe_per_file ( cls , data , fname ): data [ \"subject\" ] = data [ \"subject\" ] . astype ( str ) data [ \"to\" ] = data [ \"to\" ] . astype ( str ) data [ \"sender\" ] = data [ \"from\" ] . str . extract ( \"&lt;([^&gt;]+)&gt;\" ) data . loc [ data [ \"sender\" ] . isnull (), \"sender\" ] = data [ data [ \"sender\" ] . isnull ()][ \"from\" ] . str . strip ( '\"' ) data [ \"sent\" ] = data . sender . str . contains ( \"|\" . join ( cls . me ), na = False ) data [ \"receiver\" ] = data [ \"to\" ] . str . extract ( \"&lt;([^&gt;]+)&gt;\" ) data . loc [ data [ \"receiver\" ] . isnull (), \"receiver\" ] = data . loc [ data [ \"receiver\" ] . isnull (), \"to\" ] data [ \"received\" ] = data . receiver . str . contains ( \"|\" . join ( cls . me ), na = False ) data [ \"timestamp\" ] = pd . to_datetime ([ try_parse ( x ) for x in data . date ], utc = True ) . tz_convert ( tz ) data . drop ( \"date\" , axis = 1 , inplace = True ) return data # refactor to use \"google_takeout\" as target @classmethod def load ( cls , fname , nrows = None , from_cache = True ): emails = cls . load_data_file_modified_time ( fname , nrows = nrows , from_cache = from_cache ) return cls ( emails ) def sent_by ( self , name = None , email = None , case = False ): if name is not None and email is not None : a = self . sender . str . contains ( name , case = case , na = False ) b = self . sender . str . contains ( email , case = case , na = False ) res = self [ a | b ] elif name is not None : res = self [ self . sender . str . contains ( name , case = case , na = False )] elif email is not None : res = self [ self . sender . str . contains ( email , case = case , na = False )] return self . __class__ ( res ) def received_by ( self , name = None , email = None , case = False ): if name is not None and email is not None : a = self . receiver . str . contains ( name , case = case , na = False ) b = self . receiver . str . contains ( email , case = case , na = False ) res = self [ a | b ] elif name is not None : res = self [ self . receiver . str . contains ( name , case = case , na = False )] elif email is not None : res = self [ self . receiver . str . contains ( email , case = case , na = False )] return self . __class__ ( res )","title":"Module nostalgia.sources.google.gmail"},{"location":"reference/nostalgia/sources/google/gmail/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/google/gmail/#try_parse","text":"def try_parse ( x ) View Source def try_parse(x): try: if x.endswith(\"PST\"): x = x.replace(\"PST\", \"-0800 (PST)\") elif x.endswith(\"PDT\"): x = x.replace(\"PDT\", \"-0700 (PDT)\") d = parse(x) if d.tzinfo is None: d = d.replace(tzinfo=tz) return d except: return datetime(1970, 1, 1, 0, 0, 0, tzinfo=tz)","title":"try_parse"},{"location":"reference/nostalgia/sources/google/gmail/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/google/gmail/#gmail","text":"class Gmail ( data ) View Source class Gmail ( Google ) : me = [] @classmethod def handle_dataframe_per_file ( cls , data , fname ) : data [ \"subject\" ] = data [ \"subject\" ]. astype ( str ) data [ \"to\" ] = data [ \"to\" ]. astype ( str ) data [ \"sender\" ] = data [ \"from\" ]. str . extract ( \"&lt;([^&gt;]+)&gt;\" ) data . loc [ data [ \"sender\" ]. isnull (), \"sender\" ] = data [ data [ \"sender\" ]. isnull ()][ \"from\" ]. str . strip ( '\"' ) data [ \"sent\" ] = data . sender . str . contains ( \"|\" . join ( cls . me ), na = False ) data [ \"receiver\" ] = data [ \"to\" ]. str . extract ( \"&lt;([^&gt;]+)&gt;\" ) data . loc [ data [ \"receiver\" ]. isnull (), \"receiver\" ] = data . loc [ data [ \"receiver\" ]. isnull (), \"to\" ] data [ \"received\" ] = data . receiver . str . contains ( \"|\" . join ( cls . me ), na = False ) data [ \"timestamp\" ] = pd . to_datetime ([ try_parse ( x ) for x in data . date ], utc = True ). tz_convert ( tz ) data . drop ( \"date\" , axis = 1 , inplace = True ) return data # refactor to use \"google_takeout\" as target @classmethod def load ( cls , fname , nrows = None , from_cache = True ) : emails = cls . load_data_file_modified_time ( fname , nrows = nrows , from_cache = from_cache ) return cls ( emails ) def sent_by ( self , name = None , email = None , case = False ) : if name is not None and email is not None : a = self . sender . str . contains ( name , case = case , na = False ) b = self . sender . str . contains ( email , case = case , na = False ) res = self [ a | b ] elif name is not None : res = self [ self . sender . str . contains ( name , case = case , na = False )] elif email is not None : res = self [ self . sender . str . contains ( email , case = case , na = False )] return self . __class__ ( res ) def received_by ( self , name = None , email = None , case = False ) : if name is not None and email is not None : a = self . receiver . str . contains ( name , case = case , na = False ) b = self . receiver . str . contains ( email , case = case , na = False ) res = self [ a | b ] elif name is not None : res = self [ self . receiver . str . contains ( name , case = case , na = False )] elif email is not None : res = self [ self . receiver . str . contains ( email , case = case , na = False )] return self . __class__ ( res )","title":"Gmail"},{"location":"reference/nostalgia/sources/google/gmail/#ancestors-in-mro","text":"nostalgia.sources.google.Google nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/google/gmail/#class-variables","text":"ingest_settings keywords me nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/google/gmail/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/google/gmail/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/google/gmail/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/google/gmail/#handle_dataframe_per_file","text":"def handle_dataframe_per_file ( data , fname ) View Source @classmethod def handle_dataframe_per_file(cls, data, fname): data[\"subject\"] = data[\"subject\"].astype(str) data[\"to\"] = data[\"to\"].astype(str) data[\"sender\"] = data[\"from\"].str.extract(\"&lt;([^&gt;]+)&gt;\") data.loc[data[\"sender\"].isnull(), \"sender\"] = data[data[\"sender\"].isnull()][ \"from\" ].str.strip('\"') data[\"sent\"] = data.sender.str.contains(\"|\".join(cls.me), na=False) data[\"receiver\"] = data[\"to\"].str.extract(\"&lt;([^&gt;]+)&gt;\") data.loc[data[\"receiver\"].isnull(), \"receiver\"] = data.loc[data[\"receiver\"].isnull(), \"to\"] data[\"received\"] = data.receiver.str.contains(\"|\".join(cls.me), na=False) data[\"timestamp\"] = pd.to_datetime([try_parse(x) for x in data.date], utc=True).tz_convert( tz ) data.drop(\"date\", axis=1, inplace=True) return data","title":"handle_dataframe_per_file"},{"location":"reference/nostalgia/sources/google/gmail/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/google/gmail/#load","text":"def load ( fname , nrows = None , from_cache = True ) View Source @classmethod def load(cls, fname, nrows=None, from_cache=True): emails = cls.load_data_file_modified_time(fname, nrows=nrows, from_cache=from_cache) return cls(emails)","title":"load"},{"location":"reference/nostalgia/sources/google/gmail/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/google/gmail/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/google/gmail/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/google/gmail/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/google/gmail/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/google/gmail/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/google/gmail/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/google/gmail/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/google/gmail/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/google/gmail/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/google/gmail/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/google/gmail/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/google/gmail/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/google/gmail/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/google/gmail/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/google/gmail/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/google/gmail/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/google/gmail/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/google/gmail/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/google/gmail/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/google/gmail/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/google/gmail/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/google/gmail/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/google/gmail/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/google/gmail/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/google/gmail/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/google/gmail/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/google/gmail/#received_by","text":"def received_by ( self , name = None , email = None , case = False ) View Source def received_by(self, name=None, email=None, case=False): if name is not None and email is not None: a = self.receiver.str.contains(name, case=case, na=False) b = self.receiver.str.contains(email, case=case, na=False) res = self[a | b] elif name is not None: res = self[self.receiver.str.contains(name, case=case, na=False)] elif email is not None: res = self[self.receiver.str.contains(email, case=case, na=False)] return self.__class__(res)","title":"received_by"},{"location":"reference/nostalgia/sources/google/gmail/#sent_by","text":"def sent_by ( self , name = None , email = None , case = False ) View Source def sent_by(self, name=None, email=None, case=False): if name is not None and email is not None: a = self.sender.str.contains(name, case=case, na=False) b = self.sender.str.contains(email, case=case, na=False) res = self[a | b] elif name is not None: res = self[self.sender.str.contains(name, case=case, na=False)] elif email is not None: res = self[self.sender.str.contains(email, case=case, na=False)] return self.__class__(res)","title":"sent_by"},{"location":"reference/nostalgia/sources/google/gmail/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/google/gmail/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/google/gmail/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/google/gmail/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/google/gmail/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/google/gmail/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/google/gmail/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/google/gmail/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/google/gmail/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/google/gmail/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/google/page_visit/","text":"Module nostalgia.sources.google.page_visit View Source import pandas as pd from nostalgia.sources.google import Google from nostalgia.times import datetime_from_format from nostalgia.times import tz class PageVisit ( Google ): @classmethod def handle_dataframe_per_file ( cls , data , file_path ): data [ \"time\" ] = pd . to_datetime ( data . time_usec , utc = True , unit = \"us\" ) . dt . tz_convert ( tz ) del data [ \"time_usec\" ] return data # refactor to use \"google_takeout\" as target @classmethod def load ( cls , nrows = None , from_cache = True , ** kwargs ): file_path = \"~/nostalgia_data/input/google/Takeout/Chrome/BrowserHistory.json\" page_visit = cls . load_data_file_modified_time ( file_path , \"Browser History\" , nrows = nrows , from_cache = from_cache ) return cls ( page_visit ) Classes PageVisit class PageVisit ( data ) View Source class PageVisit ( Google ) : @classmethod def handle_dataframe_per_file ( cls , data , file_path ) : data [ \"time\" ] = pd . to_datetime ( data . time_usec , utc = True , unit = \"us\" ). dt . tz_convert ( tz ) del data [ \"time_usec\" ] return data # refactor to use \"google_takeout\" as target @classmethod def load ( cls , nrows = None , from_cache = True , ** kwargs ) : file_path = \"~/nostalgia_data/input/google/Takeout/Chrome/BrowserHistory.json\" page_visit = cls . load_data_file_modified_time ( file_path , \"Browser History\" , nrows = nrows , from_cache = from_cache ) return cls ( page_visit ) Ancestors (in MRO) nostalgia.sources.google.Google nostalgia.ndf.NDF Class variables ingest_settings keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} handle_dataframe_per_file def handle_dataframe_per_file ( data , file_path ) View Source @classmethod def handle_dataframe_per_file(cls, data, file_path): data[\"time\"] = pd.to_datetime(data.time_usec, utc=True, unit=\"us\").dt.tz_convert(tz) del data[\"time_usec\"] return data ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None , from_cache = True , ** kwargs ) View Source @classmethod def load(cls, nrows=None, from_cache=True, **kwargs): file_path = \"~/nostalgia_data/input/google/Takeout/Chrome/BrowserHistory.json\" page_visit = cls.load_data_file_modified_time( file_path, \"Browser History\", nrows=nrows, from_cache=from_cache ) return cls(page_visit) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Page Visit"},{"location":"reference/nostalgia/sources/google/page_visit/#module-nostalgiasourcesgooglepage_visit","text":"View Source import pandas as pd from nostalgia.sources.google import Google from nostalgia.times import datetime_from_format from nostalgia.times import tz class PageVisit ( Google ): @classmethod def handle_dataframe_per_file ( cls , data , file_path ): data [ \"time\" ] = pd . to_datetime ( data . time_usec , utc = True , unit = \"us\" ) . dt . tz_convert ( tz ) del data [ \"time_usec\" ] return data # refactor to use \"google_takeout\" as target @classmethod def load ( cls , nrows = None , from_cache = True , ** kwargs ): file_path = \"~/nostalgia_data/input/google/Takeout/Chrome/BrowserHistory.json\" page_visit = cls . load_data_file_modified_time ( file_path , \"Browser History\" , nrows = nrows , from_cache = from_cache ) return cls ( page_visit )","title":"Module nostalgia.sources.google.page_visit"},{"location":"reference/nostalgia/sources/google/page_visit/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/google/page_visit/#pagevisit","text":"class PageVisit ( data ) View Source class PageVisit ( Google ) : @classmethod def handle_dataframe_per_file ( cls , data , file_path ) : data [ \"time\" ] = pd . to_datetime ( data . time_usec , utc = True , unit = \"us\" ). dt . tz_convert ( tz ) del data [ \"time_usec\" ] return data # refactor to use \"google_takeout\" as target @classmethod def load ( cls , nrows = None , from_cache = True , ** kwargs ) : file_path = \"~/nostalgia_data/input/google/Takeout/Chrome/BrowserHistory.json\" page_visit = cls . load_data_file_modified_time ( file_path , \"Browser History\" , nrows = nrows , from_cache = from_cache ) return cls ( page_visit )","title":"PageVisit"},{"location":"reference/nostalgia/sources/google/page_visit/#ancestors-in-mro","text":"nostalgia.sources.google.Google nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/google/page_visit/#class-variables","text":"ingest_settings keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/google/page_visit/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/google/page_visit/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/google/page_visit/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/google/page_visit/#handle_dataframe_per_file","text":"def handle_dataframe_per_file ( data , file_path ) View Source @classmethod def handle_dataframe_per_file(cls, data, file_path): data[\"time\"] = pd.to_datetime(data.time_usec, utc=True, unit=\"us\").dt.tz_convert(tz) del data[\"time_usec\"] return data","title":"handle_dataframe_per_file"},{"location":"reference/nostalgia/sources/google/page_visit/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/google/page_visit/#load","text":"def load ( nrows = None , from_cache = True , ** kwargs ) View Source @classmethod def load(cls, nrows=None, from_cache=True, **kwargs): file_path = \"~/nostalgia_data/input/google/Takeout/Chrome/BrowserHistory.json\" page_visit = cls.load_data_file_modified_time( file_path, \"Browser History\", nrows=nrows, from_cache=from_cache ) return cls(page_visit)","title":"load"},{"location":"reference/nostalgia/sources/google/page_visit/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/google/page_visit/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/google/page_visit/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/google/page_visit/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/google/page_visit/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/google/page_visit/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/google/page_visit/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/google/page_visit/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/google/page_visit/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/google/page_visit/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/google/page_visit/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/google/page_visit/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/google/page_visit/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/google/page_visit/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/google/page_visit/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/google/page_visit/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/google/page_visit/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/google/page_visit/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/google/page_visit/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/google/page_visit/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/google/page_visit/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/google/page_visit/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/google/page_visit/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/google/page_visit/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/google/page_visit/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/google/page_visit/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/google/page_visit/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/google/page_visit/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/google/page_visit/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/google/page_visit/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/google/page_visit/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/google/page_visit/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/google/page_visit/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/google/page_visit/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/google/page_visit/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/google/page_visit/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/google/page_visit/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/google/photos/","text":"Module nostalgia.sources.google.photos View Source import os import just from datetime import datetime import pandas as pd from nostalgia.times import tz from nostalgia.utils import format_latlng from nostalgia.sources.google import Google class Photos ( Google ): @classmethod def load ( cls , nrows = None ): photo_glob = \"~/nostalgia_data/input/google/Takeout/Google Photos/*/*\" pics = [] nrows = nrows or float ( \"inf\" ) rows = 0 for fname in just . glob ( photo_glob ): if fname . endswith ( \".json\" ): continue try : meta = just . read ( fname + \".json\" ) except FileNotFoundError : continue if rows == nrows : break date = datetime . fromtimestamp ( int ( meta [ 'photoTakenTime' ][ 'timestamp' ]), tz ) lat , lon = format_latlng ( ( meta [ 'geoData' ][ 'latitude' ], meta [ 'geoData' ][ 'longitude' ]) ) . split ( \", \" ) title = meta [ \"title\" ] pics . append ( { \"path\" : \"file://\" + fname , \"lat\" : lat , \"lon\" : lon , \"title\" : title , \"time\" : date } ) rows += 1 pics = pd . DataFrame ( pics ) return cls ( pics ) Classes Photos class Photos ( data ) View Source class Photos ( Google ) : @classmethod def load ( cls , nrows = None ) : photo_glob = \"~/nostalgia_data/input/google/Takeout/Google Photos/*/*\" pics = [] nrows = nrows or float ( \"inf\" ) rows = 0 for fname in just . glob ( photo_glob ) : if fname . endswith ( \".json\" ) : continue try : meta = just . read ( fname + \".json\" ) except FileNotFoundError : continue if rows == nrows : break date = datetime . fromtimestamp ( int ( meta [ 'photoTakenTime' ][ 'timestamp' ]), tz ) lat , lon = format_latlng ( ( meta [ 'geoData' ][ 'latitude' ], meta [ 'geoData' ][ 'longitude' ]) ). split ( \", \" ) title = meta [ \"title\" ] pics . append ( { \"path\" : \"file://\" + fname , \"lat\" : lat , \"lon\" : lon , \"title\" : title , \"time\" : date } ) rows += 1 pics = pd . DataFrame ( pics ) return cls ( pics ) Ancestors (in MRO) nostalgia.sources.google.Google nostalgia.ndf.NDF Class variables ingest_settings keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): photo_glob = \"~/nostalgia_data/input/google/Takeout/Google Photos/*/*\" pics = [] nrows = nrows or float(\"inf\") rows = 0 for fname in just.glob(photo_glob): if fname.endswith(\".json\"): continue try: meta = just.read(fname + \".json\") except FileNotFoundError: continue if rows == nrows: break date = datetime.fromtimestamp(int(meta['photoTakenTime']['timestamp']), tz) lat, lon = format_latlng( (meta['geoData']['latitude'], meta['geoData']['longitude']) ).split(\", \") title = meta[\"title\"] pics.append( {\"path\": \"file://\" + fname, \"lat\": lat, \"lon\": lon, \"title\": title, \"time\": date} ) rows += 1 pics = pd.DataFrame(pics) return cls(pics) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Photos"},{"location":"reference/nostalgia/sources/google/photos/#module-nostalgiasourcesgooglephotos","text":"View Source import os import just from datetime import datetime import pandas as pd from nostalgia.times import tz from nostalgia.utils import format_latlng from nostalgia.sources.google import Google class Photos ( Google ): @classmethod def load ( cls , nrows = None ): photo_glob = \"~/nostalgia_data/input/google/Takeout/Google Photos/*/*\" pics = [] nrows = nrows or float ( \"inf\" ) rows = 0 for fname in just . glob ( photo_glob ): if fname . endswith ( \".json\" ): continue try : meta = just . read ( fname + \".json\" ) except FileNotFoundError : continue if rows == nrows : break date = datetime . fromtimestamp ( int ( meta [ 'photoTakenTime' ][ 'timestamp' ]), tz ) lat , lon = format_latlng ( ( meta [ 'geoData' ][ 'latitude' ], meta [ 'geoData' ][ 'longitude' ]) ) . split ( \", \" ) title = meta [ \"title\" ] pics . append ( { \"path\" : \"file://\" + fname , \"lat\" : lat , \"lon\" : lon , \"title\" : title , \"time\" : date } ) rows += 1 pics = pd . DataFrame ( pics ) return cls ( pics )","title":"Module nostalgia.sources.google.photos"},{"location":"reference/nostalgia/sources/google/photos/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/google/photos/#photos","text":"class Photos ( data ) View Source class Photos ( Google ) : @classmethod def load ( cls , nrows = None ) : photo_glob = \"~/nostalgia_data/input/google/Takeout/Google Photos/*/*\" pics = [] nrows = nrows or float ( \"inf\" ) rows = 0 for fname in just . glob ( photo_glob ) : if fname . endswith ( \".json\" ) : continue try : meta = just . read ( fname + \".json\" ) except FileNotFoundError : continue if rows == nrows : break date = datetime . fromtimestamp ( int ( meta [ 'photoTakenTime' ][ 'timestamp' ]), tz ) lat , lon = format_latlng ( ( meta [ 'geoData' ][ 'latitude' ], meta [ 'geoData' ][ 'longitude' ]) ). split ( \", \" ) title = meta [ \"title\" ] pics . append ( { \"path\" : \"file://\" + fname , \"lat\" : lat , \"lon\" : lon , \"title\" : title , \"time\" : date } ) rows += 1 pics = pd . DataFrame ( pics ) return cls ( pics )","title":"Photos"},{"location":"reference/nostalgia/sources/google/photos/#ancestors-in-mro","text":"nostalgia.sources.google.Google nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/google/photos/#class-variables","text":"ingest_settings keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/google/photos/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/google/photos/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/google/photos/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/google/photos/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/google/photos/#load","text":"def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): photo_glob = \"~/nostalgia_data/input/google/Takeout/Google Photos/*/*\" pics = [] nrows = nrows or float(\"inf\") rows = 0 for fname in just.glob(photo_glob): if fname.endswith(\".json\"): continue try: meta = just.read(fname + \".json\") except FileNotFoundError: continue if rows == nrows: break date = datetime.fromtimestamp(int(meta['photoTakenTime']['timestamp']), tz) lat, lon = format_latlng( (meta['geoData']['latitude'], meta['geoData']['longitude']) ).split(\", \") title = meta[\"title\"] pics.append( {\"path\": \"file://\" + fname, \"lat\": lat, \"lon\": lon, \"title\": title, \"time\": date} ) rows += 1 pics = pd.DataFrame(pics) return cls(pics)","title":"load"},{"location":"reference/nostalgia/sources/google/photos/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/google/photos/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/google/photos/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/google/photos/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/google/photos/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/google/photos/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/google/photos/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/google/photos/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/google/photos/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/google/photos/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/google/photos/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/google/photos/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/google/photos/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/google/photos/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/google/photos/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/google/photos/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/google/photos/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/google/photos/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/google/photos/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/google/photos/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/google/photos/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/google/photos/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/google/photos/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/google/photos/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/google/photos/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/google/photos/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/google/photos/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/google/photos/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/google/photos/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/google/photos/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/google/photos/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/google/photos/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/google/photos/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/google/photos/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/google/photos/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/google/photos/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/google/photos/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/google/play_music/","text":"Module nostalgia.sources.google.play_music View Source import numpy as np from nostalgia.times import datetime_from_format from datetime import timedelta from nostalgia.sources.google import Google def custom_parse ( x ): if not isinstance ( x , str ): return x try : return datetime_from_format ( x , \"%Y-%m- %d T%H:%M:%SZ\" , in_utc = True ) except : return datetime_from_format ( x , \"%Y-%m- %d T%H:%M:%S. %f Z\" , in_utc = True ) class PlayMusic ( Google ): @classmethod def handle_dataframe_per_file ( cls , data , file_path ): data [ \"_start\" ] = [ custom_parse ( x ) for x in data [ \"time\" ]] data = data . rename ( columns = { \"description\" : \"artist\" }) data = data [ ~ data . title . str . startswith ( \"Searched for \" )] data . loc [:, \"skip\" ] = data . title . str . startswith ( \"Skipped \" ) data . loc [:, \"listen\" ] = data . title . str . startswith ( \"Listened to \" ) data = data . sort_values ( \"_start\" ) data [ \"skipping_this\" ] = data . shift ( - 1 ) . skip data [ \"_end\" ] = data . shift ( - 1 ) . _start data [ \"fake\" ] = data . skipping_this & amp ; ( data . _end - data . _start & lt ; timedelta ( seconds = 15 )) data [ \"long\" ] = data . _end - data . _start & gt ; timedelta ( minutes = 10 ) data . loc [ data . long , \"_end\" ] = data [ data . long ][ \"_start\" ] + timedelta ( minutes = 10 ) data = data [ ~ data . fake & amp ; data . listen ] # stripping \"Listened to \" data [ \"title\" ] = data . title . str . slice ( 12 , None ) data [ \"duration\" ] = data . _end - data . _start # clean up for d in [ \"products\" , \"subtitles\" , \"header\" , \"titleUrl\" , \"skipping_this\" , \"skip\" , \"listen\" , \"fake\" , ]: del data [ d ] for name , group in data . groupby ([ \"artist\" , \"title\" ]): if not group . long . any () or group . long . all (): continue max_duration = group . duration . max () group . loc [ group . long , \"duration\" ] = max_duration group . loc [ group . long , \"_end\" ] = group . _start + max_duration group . loc [:, \"long\" ] = False # duration cannot be written to parquet, so do it in post process del data [ \"duration\" ] return data @classmethod def load ( cls , nrows = None , from_cache = True , ** kwargs ): file_path = ( \"~/nostalgia_data/input/google/Takeout/My Activity/Google Play Music/My Activity.json\" ) data = cls . load_data_file_modified_time ( file_path , nrows = nrows , from_cache = from_cache ) data [ \"duration\" ] = data [ \"_end\" ] - data [ \"_start\" ] return cls ( data ) Functions custom_parse def custom_parse ( x ) View Source def custom_parse(x): if not isinstance(x, str): return x try: return datetime_from_format(x, \"%Y-%m-%dT%H:%M:%SZ\", in_utc=True) except: return datetime_from_format(x, \"%Y-%m-%dT%H:%M:%S.%fZ\", in_utc=True) Classes PlayMusic class PlayMusic ( data ) View Source class PlayMusic ( Google ) : @classmethod def handle_dataframe_per_file ( cls , data , file_path ) : data [ \"_start\" ] = [ custom_parse ( x ) for x in data [ \"time\" ]] data = data . rename ( columns = { \"description\" : \"artist\" }) data = data [ ~ data . title . str . startswith ( \"Searched for \" )] data . loc [ : , \"skip\" ] = data . title . str . startswith ( \"Skipped \" ) data . loc [ : , \"listen\" ] = data . title . str . startswith ( \"Listened to \" ) data = data . sort_values ( \"_start\" ) data [ \"skipping_this\" ] = data . shift ( - 1 ). skip data [ \"_end\" ] = data . shift ( - 1 ). _start data [ \"fake\" ] = data . skipping_this & amp ; ( data . _end - data . _start & lt ; timedelta ( seconds = 15 )) data [ \"long\" ] = data . _end - data . _start & gt ; timedelta ( minutes = 10 ) data . loc [ data . long , \"_end\" ] = data [ data . long ][ \"_start\" ] + timedelta ( minutes = 10 ) data = data [ ~ data . fake & amp ; data . listen ] # stripping \"Listened to \" data [ \"title\" ] = data . title . str . slice ( 12 , None ) data [ \"duration\" ] = data . _end - data . _start # clean up for d in [ \"products\" , \"subtitles\" , \"header\" , \"titleUrl\" , \"skipping_this\" , \"skip\" , \"listen\" , \"fake\" , ] : del data [ d ] for name , group in data . groupby ([ \"artist\" , \"title\" ]) : if not group . long . any () or group . long . all () : continue max_duration = group . duration . max () group . loc [ group . long , \"duration\" ] = max_duration group . loc [ group . long , \"_end\" ] = group . _start + max_duration group . loc [ : , \"long\" ] = False # duration cannot be written to parquet , so do it in post process del data [ \"duration\" ] return data @classmethod def load ( cls , nrows = None , from_cache = True , ** kwargs ) : file_path = ( \"~/nostalgia_data/input/google/Takeout/My Activity/Google Play Music/My Activity.json\" ) data = cls . load_data_file_modified_time ( file_path , nrows = nrows , from_cache = from_cache ) data [ \"duration\" ] = data [ \"_end\" ] - data [ \"_start\" ] return cls ( data ) Ancestors (in MRO) nostalgia.sources.google.Google nostalgia.ndf.NDF Class variables ingest_settings keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} handle_dataframe_per_file def handle_dataframe_per_file ( data , file_path ) View Source @classmethod def handle_dataframe_per_file(cls, data, file_path): data[\"_start\"] = [custom_parse(x) for x in data[\"time\"]] data = data.rename(columns={\"description\": \"artist\"}) data = data[~data.title.str.startswith(\"Searched for \")] data.loc[:, \"skip\"] = data.title.str.startswith(\"Skipped \") data.loc[:, \"listen\"] = data.title.str.startswith(\"Listened to \") data = data.sort_values(\"_start\") data[\"skipping_this\"] = data.shift(-1).skip data[\"_end\"] = data.shift(-1)._start data[\"fake\"] = data.skipping_this &amp; (data._end - data._start &lt; timedelta(seconds=15)) data[\"long\"] = data._end - data._start &gt; timedelta(minutes=10) data.loc[data.long, \"_end\"] = data[data.long][\"_start\"] + timedelta(minutes=10) data = data[~data.fake &amp; data.listen] # stripping \"Listened to \" data[\"title\"] = data.title.str.slice(12, None) data[\"duration\"] = data._end - data._start # clean up for d in [ \"products\", \"subtitles\", \"header\", \"titleUrl\", \"skipping_this\", \"skip\", \"listen\", \"fake\", ]: del data[d] for name, group in data.groupby([\"artist\", \"title\"]): if not group.long.any() or group.long.all(): continue max_duration = group.duration.max() group.loc[group.long, \"duration\"] = max_duration group.loc[group.long, \"_end\"] = group._start + max_duration group.loc[:, \"long\"] = False # duration cannot be written to parquet, so do it in post process del data[\"duration\"] return data ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None , from_cache = True , ** kwargs ) View Source @classmethod def load(cls, nrows=None, from_cache=True, **kwargs): file_path = ( \"~/nostalgia_data/input/google/Takeout/My Activity/Google Play Music/My Activity.json\" ) data = cls.load_data_file_modified_time(file_path, nrows=nrows, from_cache=from_cache) data[\"duration\"] = data[\"_end\"] - data[\"_start\"] return cls(data) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Play Music"},{"location":"reference/nostalgia/sources/google/play_music/#module-nostalgiasourcesgoogleplay_music","text":"View Source import numpy as np from nostalgia.times import datetime_from_format from datetime import timedelta from nostalgia.sources.google import Google def custom_parse ( x ): if not isinstance ( x , str ): return x try : return datetime_from_format ( x , \"%Y-%m- %d T%H:%M:%SZ\" , in_utc = True ) except : return datetime_from_format ( x , \"%Y-%m- %d T%H:%M:%S. %f Z\" , in_utc = True ) class PlayMusic ( Google ): @classmethod def handle_dataframe_per_file ( cls , data , file_path ): data [ \"_start\" ] = [ custom_parse ( x ) for x in data [ \"time\" ]] data = data . rename ( columns = { \"description\" : \"artist\" }) data = data [ ~ data . title . str . startswith ( \"Searched for \" )] data . loc [:, \"skip\" ] = data . title . str . startswith ( \"Skipped \" ) data . loc [:, \"listen\" ] = data . title . str . startswith ( \"Listened to \" ) data = data . sort_values ( \"_start\" ) data [ \"skipping_this\" ] = data . shift ( - 1 ) . skip data [ \"_end\" ] = data . shift ( - 1 ) . _start data [ \"fake\" ] = data . skipping_this & amp ; ( data . _end - data . _start & lt ; timedelta ( seconds = 15 )) data [ \"long\" ] = data . _end - data . _start & gt ; timedelta ( minutes = 10 ) data . loc [ data . long , \"_end\" ] = data [ data . long ][ \"_start\" ] + timedelta ( minutes = 10 ) data = data [ ~ data . fake & amp ; data . listen ] # stripping \"Listened to \" data [ \"title\" ] = data . title . str . slice ( 12 , None ) data [ \"duration\" ] = data . _end - data . _start # clean up for d in [ \"products\" , \"subtitles\" , \"header\" , \"titleUrl\" , \"skipping_this\" , \"skip\" , \"listen\" , \"fake\" , ]: del data [ d ] for name , group in data . groupby ([ \"artist\" , \"title\" ]): if not group . long . any () or group . long . all (): continue max_duration = group . duration . max () group . loc [ group . long , \"duration\" ] = max_duration group . loc [ group . long , \"_end\" ] = group . _start + max_duration group . loc [:, \"long\" ] = False # duration cannot be written to parquet, so do it in post process del data [ \"duration\" ] return data @classmethod def load ( cls , nrows = None , from_cache = True , ** kwargs ): file_path = ( \"~/nostalgia_data/input/google/Takeout/My Activity/Google Play Music/My Activity.json\" ) data = cls . load_data_file_modified_time ( file_path , nrows = nrows , from_cache = from_cache ) data [ \"duration\" ] = data [ \"_end\" ] - data [ \"_start\" ] return cls ( data )","title":"Module nostalgia.sources.google.play_music"},{"location":"reference/nostalgia/sources/google/play_music/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/google/play_music/#custom_parse","text":"def custom_parse ( x ) View Source def custom_parse(x): if not isinstance(x, str): return x try: return datetime_from_format(x, \"%Y-%m-%dT%H:%M:%SZ\", in_utc=True) except: return datetime_from_format(x, \"%Y-%m-%dT%H:%M:%S.%fZ\", in_utc=True)","title":"custom_parse"},{"location":"reference/nostalgia/sources/google/play_music/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/google/play_music/#playmusic","text":"class PlayMusic ( data ) View Source class PlayMusic ( Google ) : @classmethod def handle_dataframe_per_file ( cls , data , file_path ) : data [ \"_start\" ] = [ custom_parse ( x ) for x in data [ \"time\" ]] data = data . rename ( columns = { \"description\" : \"artist\" }) data = data [ ~ data . title . str . startswith ( \"Searched for \" )] data . loc [ : , \"skip\" ] = data . title . str . startswith ( \"Skipped \" ) data . loc [ : , \"listen\" ] = data . title . str . startswith ( \"Listened to \" ) data = data . sort_values ( \"_start\" ) data [ \"skipping_this\" ] = data . shift ( - 1 ). skip data [ \"_end\" ] = data . shift ( - 1 ). _start data [ \"fake\" ] = data . skipping_this & amp ; ( data . _end - data . _start & lt ; timedelta ( seconds = 15 )) data [ \"long\" ] = data . _end - data . _start & gt ; timedelta ( minutes = 10 ) data . loc [ data . long , \"_end\" ] = data [ data . long ][ \"_start\" ] + timedelta ( minutes = 10 ) data = data [ ~ data . fake & amp ; data . listen ] # stripping \"Listened to \" data [ \"title\" ] = data . title . str . slice ( 12 , None ) data [ \"duration\" ] = data . _end - data . _start # clean up for d in [ \"products\" , \"subtitles\" , \"header\" , \"titleUrl\" , \"skipping_this\" , \"skip\" , \"listen\" , \"fake\" , ] : del data [ d ] for name , group in data . groupby ([ \"artist\" , \"title\" ]) : if not group . long . any () or group . long . all () : continue max_duration = group . duration . max () group . loc [ group . long , \"duration\" ] = max_duration group . loc [ group . long , \"_end\" ] = group . _start + max_duration group . loc [ : , \"long\" ] = False # duration cannot be written to parquet , so do it in post process del data [ \"duration\" ] return data @classmethod def load ( cls , nrows = None , from_cache = True , ** kwargs ) : file_path = ( \"~/nostalgia_data/input/google/Takeout/My Activity/Google Play Music/My Activity.json\" ) data = cls . load_data_file_modified_time ( file_path , nrows = nrows , from_cache = from_cache ) data [ \"duration\" ] = data [ \"_end\" ] - data [ \"_start\" ] return cls ( data )","title":"PlayMusic"},{"location":"reference/nostalgia/sources/google/play_music/#ancestors-in-mro","text":"nostalgia.sources.google.Google nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/google/play_music/#class-variables","text":"ingest_settings keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/google/play_music/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/google/play_music/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/google/play_music/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/google/play_music/#handle_dataframe_per_file","text":"def handle_dataframe_per_file ( data , file_path ) View Source @classmethod def handle_dataframe_per_file(cls, data, file_path): data[\"_start\"] = [custom_parse(x) for x in data[\"time\"]] data = data.rename(columns={\"description\": \"artist\"}) data = data[~data.title.str.startswith(\"Searched for \")] data.loc[:, \"skip\"] = data.title.str.startswith(\"Skipped \") data.loc[:, \"listen\"] = data.title.str.startswith(\"Listened to \") data = data.sort_values(\"_start\") data[\"skipping_this\"] = data.shift(-1).skip data[\"_end\"] = data.shift(-1)._start data[\"fake\"] = data.skipping_this &amp; (data._end - data._start &lt; timedelta(seconds=15)) data[\"long\"] = data._end - data._start &gt; timedelta(minutes=10) data.loc[data.long, \"_end\"] = data[data.long][\"_start\"] + timedelta(minutes=10) data = data[~data.fake &amp; data.listen] # stripping \"Listened to \" data[\"title\"] = data.title.str.slice(12, None) data[\"duration\"] = data._end - data._start # clean up for d in [ \"products\", \"subtitles\", \"header\", \"titleUrl\", \"skipping_this\", \"skip\", \"listen\", \"fake\", ]: del data[d] for name, group in data.groupby([\"artist\", \"title\"]): if not group.long.any() or group.long.all(): continue max_duration = group.duration.max() group.loc[group.long, \"duration\"] = max_duration group.loc[group.long, \"_end\"] = group._start + max_duration group.loc[:, \"long\"] = False # duration cannot be written to parquet, so do it in post process del data[\"duration\"] return data","title":"handle_dataframe_per_file"},{"location":"reference/nostalgia/sources/google/play_music/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/google/play_music/#load","text":"def load ( nrows = None , from_cache = True , ** kwargs ) View Source @classmethod def load(cls, nrows=None, from_cache=True, **kwargs): file_path = ( \"~/nostalgia_data/input/google/Takeout/My Activity/Google Play Music/My Activity.json\" ) data = cls.load_data_file_modified_time(file_path, nrows=nrows, from_cache=from_cache) data[\"duration\"] = data[\"_end\"] - data[\"_start\"] return cls(data)","title":"load"},{"location":"reference/nostalgia/sources/google/play_music/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/google/play_music/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/google/play_music/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/google/play_music/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/google/play_music/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/google/play_music/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/google/play_music/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/google/play_music/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/google/play_music/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/google/play_music/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/google/play_music/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/google/play_music/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/google/play_music/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/google/play_music/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/google/play_music/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/google/play_music/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/google/play_music/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/google/play_music/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/google/play_music/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/google/play_music/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/google/play_music/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/google/play_music/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/google/play_music/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/google/play_music/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/google/play_music/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/google/play_music/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/google/play_music/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/google/play_music/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/google/play_music/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/google/play_music/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/google/play_music/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/google/play_music/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/google/play_music/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/google/play_music/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/google/play_music/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/google/play_music/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/google/play_music/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/google/timeline/","text":"Module nostalgia.sources.google.timeline View Source from datetime import datetime import os import requests import diskcache import dotenv import hashlib import pandas as pd from pytz import timezone import just from nostalgia.times import tz from nostalgia.utils import format_latlng from nostalgia.cache import get_cache from nostalgia.interfaces.places import Places def api_call ( url , params ): key = get_hash ( url , params ) status = CACHE . get ( key , {}) . get ( \"status\" ) if status == \"ZERO_RESULTS\" : return None if status == \"OK\" : return CACHE [ key ] jdata = s . get ( url , params = params ) . json () jdata [ \"meta\" ] = { \"time\" : str ( datetime . utcnow ()), \"url\" : url , \"params\" : { k : v for k , v in params . items () if k != \"key\" }, } if jdata . get ( \"status\" ) == \"ZERO_RESULTS\" : CACHE [ key ] = jdata return None if jdata . get ( \"status\" ) != \"OK\" : print ( \"status\" , url , params , key , jdata . get ( \"status\" )) raise ValueError ( \"not ok\" ) CACHE [ key ] = jdata return jdata def get_nearby ( latlng , name , excluded_transport_names ): latlng = format_latlng ( latlng ) if latlng == \"nan, nan\" : return None params = { \"location\" : latlng , \"radius\" : 100 , \"key\" : KEY } if name not in excluded_transport_names : params [ \"query\" ] = name return api_call ( NEARBY_URL , params ) def get_nearby_results ( latlng , name , excluded_transport_names ): near = get_nearby ( latlng , name , excluded_transport_names ) if near is None : return None res = [ x for x in near [ \"results\" ] if name in excluded_transport_names or x [ \"name\" ] == name ] for x in res : if \"opening_hours\" in x : return x if res : return res [ 0 ] if not near [ \"results\" ]: return None return near [ \"results\" ][ 0 ] def get_details ( place_id ): params = { 'placeid' : place_id , 'sensor' : 'false' , 'key' : KEY } return api_call ( DETAILS_URL , params ) . get ( \"result\" , {}) def get_ ( details , * types ): for tp in types : for addr in details . get ( 'address_components' , []): if tp in addr [ \"types\" ]: return addr [ \"long_name\" ] def get_hash ( url , params ): data = { k : v for k , v in params . items () if k != \"key\" } data [ \"url\" ] = url data = sorted ( data . items ()) return hashlib . sha256 ( str ( data ) . encode ( \"utf8\" )) . hexdigest () GEOCODE_URL = \"https://maps.googleapis.com/maps/api/geocode/json\" def get_address ( json_response ): for res in json_response . get ( \"results\" , []): addr = res . get ( 'formatted_address' ) if addr : return addr def geo_get_ ( json_response , * types ): for res in json_response . get ( \"results\" , []): for tp in types : for addr in res . get ( 'address_components' , []): if tp in addr [ \"types\" ]: return addr [ \"long_name\" ] def geo_get_info ( latlng ): latlng = format_latlng ( latlng ) if latlng == \"nan, nan\" : return None params = { \"method\" : \"reverse\" , \"latlng\" : latlng , \"key\" : KEY } json_response = api_call ( GEOCODE_URL , params = params ) city = geo_get_ ( json_response , \"locality\" , \"postal_town\" ) # place_id = json_response.get(\"results\", [{}])[0].get(\"place_id\") country = geo_get_ ( json_response , \"country\" ) address = get_address ( json_response ) return { \"city\" : city , \"country\" : country , \"formatted_address\" : address } dotenv . load_dotenv ( \"google/.env\" ) dotenv . load_dotenv ( \".env\" ) PYTHON_ENV = os . environ . get ( \"PYTHON_ENV\" , \"dev\" ) if PYTHON_ENV != \"prod\" : KEY = None else : KEY = os . environ . get ( \"GOOGLE_API_KEY\" , None ) CACHE = get_cache ( \"google_timeline\" ) DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\" NEARBY_URL = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json' s = requests . Session () # {'Boating', 'Cycling', 'Driving', 'Flying', 'In transit', 'Moving', 'On a bus', 'On a ferry', 'On a train', 'On a tram', 'On the subway', 'Running', 'Walking'} def get_results ( latlng , name , excluded_transport_names ): if name in excluded_transport_names : return geo_get_info ( latlng ) near_result = get_nearby_results ( latlng , name , excluded_transport_names ) if near_result is None : return None details = get_details ( near_result [ \"place_id\" ]) if details is None : return None return { \"city\" : get_ ( details , \"locality\" , \"postal_town\" ), \"country\" : get_ ( details , \"country\" ), \"rating\" : details , \"details_name\" : details . get ( \"name\" ), \"formatted_address\" : details . get ( \"formatted_address\" ), 'international_phone_number' : details . get ( 'international_phone_number' ), 'opening_hours' : details . get ( 'opening_hours' ), 'user_ratings_total' : details . get ( 'user_ratings_total' ), 'rating' : details . get ( 'rating' ), 'website' : details . get ( 'website' ), } def process ( df , excluded_transport_names , home_regex , work_regex , hometown_regex ): df [ \"start\" ] = pd . to_datetime ( df . start ) df [ \"end\" ] = pd . to_datetime ( df . end ) df [ \"around_noon\" ] = ( df . start . dt . hour & lt ; 12 ) & amp ; ( df . end . dt . hour & gt ; 12 ) df [ \"week\" ] = df . start . dt . weekday & lt ; 5 df [ \"city\" ] = df . city . fillna ( \"\" ) df [ \"transporting\" ] = df . category . isin ( excluded_transport_names ) if work_regex : df . loc [ df . name . str . contains ( work_regex , regex = True , na = False ), \"category\" ] = \"Work\" if home_regex : df . loc [ df . name . str . contains ( home_regex , regex = True , na = False ), \"category\" ] = \"Home\" # if hometown_regex: # df.loc[ # df.name.str.contains(hometown_regex, regex=True, na=False), \"category\" # ] = \"Hometown\" df = df . sort_values ( \"start\" ) . reset_index ( drop = True ) if \"Unnamed: 0\" in df . columns : del df [ \"Unnamed: 0\" ] df [ \"start\" ] = df [ \"start\" ] . dt . tz_convert ( tz ) df [ \"end\" ] = df [ \"end\" ] . dt . tz_convert ( tz ) df . index = pd . IntervalIndex . from_arrays ( df [ 'start' ], df [ 'end' ]) return df class GooglePlaces ( Places ): nlp_columns = [ \"name\" , \"city\" , \"country\" , \"category\" , \"website\" ] selected_columns = [ \"date\" , \"name\" , \"city\" , \"_office_hours\" ] @classmethod def load ( cls , file_glob = \"~/Downloads/timeline_data-*\" , nrows = None ): df = pd . read_csv ( max ( just . glob ( file_glob )), nrows = nrows ) unique_locs = set ([(( y , z ), x ) for x , y , z in zip ( df . name , df . lat , df . lon ) if y != \"nan\" ]) excluded_transport_names = set ( df [ df . name == df . category ] . name ) details_data = [] for ( lat , lon ), name in unique_locs : d = get_results (( lat , lon ), name , excluded_transport_names ) if d is None : continue d [ \"lat\" ] = lat d [ \"lon\" ] = lon d [ \"name\" ] = name details_data . append ( d ) details_data = pd . DataFrame ( details_data ) # all_loc = df.merge(details_data, on=[\"name\", \"lat\", \"lon\"], how=\"outer\") places = df . merge ( details_data , on = [ \"name\" , \"lat\" , \"lon\" ], how = \"inner\" ) # all_loc = process(all_loc, excluded_transport_names) home_regex = \"|\" . join ( cls . home ) work_regex = \"|\" . join ( cls . work ) hometown_regex = \"|\" . join ( cls . hometown ) places = process ( places , excluded_transport_names , home_regex , work_regex , hometown_regex ) return cls ( places ) GooglePlaces . work = [ \"Jibes\" , \"Sleepboot\" , \"De Meerpaal\" , \"Papendorp\" , \"Vektis\" , \"Work \" ] # at_work = places.at_work() # # places.travel_by_car()\\ # .duration_longer_than(minutes=30)\\ # .at_day(at_work) # .at_time(\"november 2019\")\\ # .during_weekdays\\ # cat_str = \", \".join([\"{}: {}\".format(i, x) for i, x in enumerate(places.category)]) # [places.iloc[[int(y) for y in x]] for x in re.findall(\", \".join([\"([0-9]+): {}\".format(x) for x in pats]), cat_str)] Variables CACHE DETAILS_URL GEOCODE_URL KEY NEARBY_URL PYTHON_ENV s Functions api_call def api_call ( url , params ) View Source def api_call(url, params): key = get_hash(url, params) status = CACHE.get(key, {}).get(\"status\") if status == \"ZERO_RESULTS\": return None if status == \"OK\": return CACHE[key] jdata = s.get(url, params=params).json() jdata[\"meta\"] = { \"time\": str(datetime.utcnow()), \"url\": url, \"params\": {k: v for k, v in params.items() if k != \"key\"}, } if jdata.get(\"status\") == \"ZERO_RESULTS\": CACHE[key] = jdata return None if jdata.get(\"status\") != \"OK\": print(\"status\", url, params, key, jdata.get(\"status\")) raise ValueError(\"not ok\") CACHE[key] = jdata return jdata geo_get_ def geo_get_ ( json_response , * types ) View Source def geo_get_(json_response, *types): for res in json_response.get(\"results\", []): for tp in types: for addr in res.get('address_components', []): if tp in addr[\"types\"]: return addr[\"long_name\"] geo_get_info def geo_get_info ( latlng ) View Source def geo_get_info(latlng): latlng = format_latlng(latlng) if latlng == \"nan, nan\": return None params = {\"method\": \"reverse\", \"latlng\": latlng, \"key\": KEY} json_response = api_call(GEOCODE_URL, params=params) city = geo_get_(json_response, \"locality\", \"postal_town\") # place_id = json_response.get(\"results\", [{}])[0].get(\"place_id\") country = geo_get_(json_response, \"country\") address = get_address(json_response) return {\"city\": city, \"country\": country, \"formatted_address\": address} get_ def get_ ( details , * types ) View Source def get_(details, *types): for tp in types: for addr in details.get('address_components', []): if tp in addr[\"types\"]: return addr[\"long_name\"] get_address def get_address ( json_response ) View Source def get_address(json_response): for res in json_response.get(\"results\", []): addr = res.get('formatted_address') if addr: return addr get_details def get_details ( place_id ) View Source def get_details(place_id): params = {'placeid': place_id, 'sensor': 'false', 'key': KEY} return api_call(DETAILS_URL, params).get(\"result\", {}) get_hash def get_hash ( url , params ) View Source def get_hash(url, params): data = {k: v for k, v in params.items() if k != \"key\"} data[\"url\"] = url data = sorted(data.items()) return hashlib.sha256(str(data).encode(\"utf8\")).hexdigest() get_nearby def get_nearby ( latlng , name , excluded_transport_names ) View Source def get_nearby(latlng, name, excluded_transport_names): latlng = format_latlng(latlng) if latlng == \"nan, nan\": return None params = {\"location\": latlng, \"radius\": 100, \"key\": KEY} if name not in excluded_transport_names: params[\"query\"] = name return api_call(NEARBY_URL, params) get_nearby_results def get_nearby_results ( latlng , name , excluded_transport_names ) View Source def get_nearby_results(latlng, name, excluded_transport_names): near = get_nearby(latlng, name, excluded_transport_names) if near is None: return None res = [x for x in near[\"results\"] if name in excluded_transport_names or x[\"name\"] == name] for x in res: if \"opening_hours\" in x: return x if res: return res[0] if not near[\"results\"]: return None return near[\"results\"][0] get_results def get_results ( latlng , name , excluded_transport_names ) View Source def get_results(latlng, name, excluded_transport_names): if name in excluded_transport_names: return geo_get_info(latlng) near_result = get_nearby_results(latlng, name, excluded_transport_names) if near_result is None: return None details = get_details(near_result[\"place_id\"]) if details is None: return None return { \"city\": get_(details, \"locality\", \"postal_town\"), \"country\": get_(details, \"country\"), \"rating\": details, \"details_name\": details.get(\"name\"), \"formatted_address\": details.get(\"formatted_address\"), 'international_phone_number': details.get('international_phone_number'), 'opening_hours': details.get('opening_hours'), 'user_ratings_total': details.get('user_ratings_total'), 'rating': details.get('rating'), 'website': details.get('website'), } process def process ( df , excluded_transport_names , home_regex , work_regex , hometown_regex ) View Source def process ( df , excluded_transport_names , home_regex , work_regex , hometown_regex ): df [ \"start\" ] = pd . to_datetime ( df . start ) df [ \"end\" ] = pd . to_datetime ( df . end ) df [ \"around_noon\" ] = ( df . start . dt . hour &lt ; 12 ) &amp ; ( df . end . dt . hour &gt ; 12 ) df [ \"week\" ] = df . start . dt . weekday &lt ; 5 df [ \"city\" ] = df . city . fillna ( \"\" ) df [ \"transporting\" ] = df . category . isin ( excluded_transport_names ) if work_regex: df . loc [ df . name . str . contains ( work_regex , regex = True , na = False ), \"category\" ] = \"Work\" if home_regex: df . loc [ df . name . str . contains ( home_regex , regex = True , na = False ), \"category\" ] = \"Home\" # if hometown_regex: # df.loc[ # df.name.str.contains(hometown_regex, regex=True, na=False), \"category\" # ] = \"Hometown\" df = df . sort_values ( \"start\" ). reset_index ( drop = True ) if \"Unnamed: 0\" in df . columns: del df [ \"Unnamed: 0\" ] df [ \"start\" ] = df [ \"start\" ]. dt . tz_convert ( tz ) df [ \"end\" ] = df [ \"end\" ]. dt . tz_convert ( tz ) df . index = pd . IntervalIndex . from_arrays ( df [ 'start' ], df [ 'end' ]) return df Classes GooglePlaces class GooglePlaces ( data ) View Source class GooglePlaces ( Places ) : nlp_columns = [ \"name\" , \"city\" , \"country\" , \"category\" , \"website\" ] selected_columns = [ \"date\" , \"name\" , \"city\" , \"_office_hours\" ] @classmethod def load ( cls , file_glob = \"~/Downloads/timeline_data-*\" , nrows = None ) : df = pd . read_csv ( max ( just . glob ( file_glob )), nrows = nrows ) unique_locs = set ([(( y , z ), x ) for x , y , z in zip ( df . name , df . lat , df . lon ) if y != \"nan\" ]) excluded_transport_names = set ( df [ df . name == df . category ]. name ) details_data = [] for ( lat , lon ), name in unique_locs : d = get_results (( lat , lon ), name , excluded_transport_names ) if d is None : continue d [ \"lat\" ] = lat d [ \"lon\" ] = lon d [ \"name\" ] = name details_data . append ( d ) details_data = pd . DataFrame ( details_data ) # all_loc = df . merge ( details_data , on = [ \"name\" , \"lat\" , \"lon\" ], how = \"outer\" ) places = df . merge ( details_data , on = [ \"name\" , \"lat\" , \"lon\" ], how = \"inner\" ) # all_loc = process ( all_loc , excluded_transport_names ) home_regex = \"|\" . join ( cls . home ) work_regex = \"|\" . join ( cls . work ) hometown_regex = \"|\" . join ( cls . hometown ) places = process ( places , excluded_transport_names , home_regex , work_regex , hometown_regex ) return cls ( places ) Ancestors (in MRO) nostalgia.interfaces.places.Places nostalgia.ndf.NDF Class variables home hometown keywords nlp_columns nlp_when selected_columns vendor work Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( file_glob = '~/Downloads/timeline_data-*' , nrows = None ) View Source @classmethod def load(cls, file_glob=\"~/Downloads/timeline_data-*\", nrows=None): df = pd.read_csv(max(just.glob(file_glob)), nrows=nrows) unique_locs = set([((y, z), x) for x, y, z in zip(df.name, df.lat, df.lon) if y != \"nan\"]) excluded_transport_names = set(df[df.name == df.category].name) details_data = [] for (lat, lon), name in unique_locs: d = get_results((lat, lon), name, excluded_transport_names) if d is None: continue d[\"lat\"] = lat d[\"lon\"] = lon d[\"name\"] = name details_data.append(d) details_data = pd.DataFrame(details_data) # all_loc = df.merge(details_data, on=[\"name\", \"lat\", \"lon\"], how=\"outer\") places = df.merge(details_data, on=[\"name\", \"lat\", \"lon\"], how=\"inner\") # all_loc = process(all_loc, excluded_transport_names) home_regex = \"|\".join(cls.home) work_regex = \"|\".join(cls.work) hometown_regex = \"|\".join(cls.hometown) places = process(places, excluded_transport_names, home_regex, work_regex, hometown_regex) return cls(places) Instance variables at_hometown df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time ) View Source def at(self, time): mp = parse_date_tz(time) return self[self.index.overlaps(pd.Interval(mp.start_date, mp.end_date))] at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_home def at_home ( self ) View Source @nlp(\"filter\", \"at home\") def at_home(self): return self.__class__(self[self.category == \"Home\"]) at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) at_work def at_work ( self ) View Source @nlp(\"filter\", \"at work\") def at_work(self): return self.__class__(self[self.category == \"Work\"]) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) length def length ( self ) View Source @nlp(\"end\", \"how long\", \"how much time\") def length(self): return (self.index.right - self.index.left).to_pytimedelta().sum() near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) near_ def near_ ( self , distance_in_meters , other_places = None ) View Source def near_(self, distance_in_meters, other_places=None): places = get_type_from_registry(\"places\") if other_places is None else other_places nearbies = [] for _, row in places[[\"lat\", \"lon\"]].drop_duplicates().iterrows(): nearbies.append(haversine(self.lat, self.lon, *row) &lt; distance_in_meters) return self.__class__(self[np.any(nearbies, axis=0)]) near_home def near_home ( self , distance = 1000 ) View Source @nlp(\"filter\", [\"near home\", \"around home\"]) def near_home(self, distance=1000): return self.near_(distance, get_type_from_registry(\"places\").at_home) near_work def near_work ( self , distance = 1000 ) View Source @nlp(\"filter\", [\"near work\", \"around work\"]) def near_work(self, distance=1000): return self.near_(distance, get_type_from_registry(\"places\").at_work) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) sum def sum ( self ) View Source @nlp(\"end\", \"how much\") def sum(self): return self.distance.sum() tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) travel def travel ( self ) View Source @nlp(\"filter\", \"travel\") def travel(self): return self[self.transporting] travel_by_bus def travel_by_bus ( self ) View Source @nlp( \"filter\", \"drive by bus\", \"by bus\", \"travel by bus\", \"using the bus\", \"going by bus\", \"on a bus\", ) def travel_by_bus(self): return self.col_contains(\"On a bus\", \"category\") travel_by_car def travel_by_car ( self ) View Source @nlp( \"filter\", \"drive\", \"by car\", \"travel by car\", \"travel using the car\", \"by car\", \"going by car\", \"on a car\", \"driving\", ) def travel_by_car(self): return self.col_contains(\"Driving\", \"category\") travel_by_train def travel_by_train ( self ) View Source @nlp( \"filter\", \"drive by train\", \"by train\", \"travel by train\", \"using the train\", \"going by train\", \"on a train\", ) def travel_by_train(self): return self.col_contains(\"On a train\", \"category\") view def view ( self , index ) View Source def view(self, index): view(self.path[index]) what_address def what_address ( self ) View Source @nlp(\"end\", \"address of\", \"what is the address\", \"how to find\", \"how can i find\") def what_address(self): return self[\"address\"] when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) work_days def work_days ( self ) View Source @nlp(\"filter\", \"work days\", \"work-days\", \"on working days\") def work_days(self): return self[self.time.dt.weekday &lt; 5] work_hours def work_hours ( self ) View Source @nlp(\"filter\", \"during work\", \"during work hours\") def work_hours(self): return self[self._office_hours]","title":"Timeline"},{"location":"reference/nostalgia/sources/google/timeline/#module-nostalgiasourcesgoogletimeline","text":"View Source from datetime import datetime import os import requests import diskcache import dotenv import hashlib import pandas as pd from pytz import timezone import just from nostalgia.times import tz from nostalgia.utils import format_latlng from nostalgia.cache import get_cache from nostalgia.interfaces.places import Places def api_call ( url , params ): key = get_hash ( url , params ) status = CACHE . get ( key , {}) . get ( \"status\" ) if status == \"ZERO_RESULTS\" : return None if status == \"OK\" : return CACHE [ key ] jdata = s . get ( url , params = params ) . json () jdata [ \"meta\" ] = { \"time\" : str ( datetime . utcnow ()), \"url\" : url , \"params\" : { k : v for k , v in params . items () if k != \"key\" }, } if jdata . get ( \"status\" ) == \"ZERO_RESULTS\" : CACHE [ key ] = jdata return None if jdata . get ( \"status\" ) != \"OK\" : print ( \"status\" , url , params , key , jdata . get ( \"status\" )) raise ValueError ( \"not ok\" ) CACHE [ key ] = jdata return jdata def get_nearby ( latlng , name , excluded_transport_names ): latlng = format_latlng ( latlng ) if latlng == \"nan, nan\" : return None params = { \"location\" : latlng , \"radius\" : 100 , \"key\" : KEY } if name not in excluded_transport_names : params [ \"query\" ] = name return api_call ( NEARBY_URL , params ) def get_nearby_results ( latlng , name , excluded_transport_names ): near = get_nearby ( latlng , name , excluded_transport_names ) if near is None : return None res = [ x for x in near [ \"results\" ] if name in excluded_transport_names or x [ \"name\" ] == name ] for x in res : if \"opening_hours\" in x : return x if res : return res [ 0 ] if not near [ \"results\" ]: return None return near [ \"results\" ][ 0 ] def get_details ( place_id ): params = { 'placeid' : place_id , 'sensor' : 'false' , 'key' : KEY } return api_call ( DETAILS_URL , params ) . get ( \"result\" , {}) def get_ ( details , * types ): for tp in types : for addr in details . get ( 'address_components' , []): if tp in addr [ \"types\" ]: return addr [ \"long_name\" ] def get_hash ( url , params ): data = { k : v for k , v in params . items () if k != \"key\" } data [ \"url\" ] = url data = sorted ( data . items ()) return hashlib . sha256 ( str ( data ) . encode ( \"utf8\" )) . hexdigest () GEOCODE_URL = \"https://maps.googleapis.com/maps/api/geocode/json\" def get_address ( json_response ): for res in json_response . get ( \"results\" , []): addr = res . get ( 'formatted_address' ) if addr : return addr def geo_get_ ( json_response , * types ): for res in json_response . get ( \"results\" , []): for tp in types : for addr in res . get ( 'address_components' , []): if tp in addr [ \"types\" ]: return addr [ \"long_name\" ] def geo_get_info ( latlng ): latlng = format_latlng ( latlng ) if latlng == \"nan, nan\" : return None params = { \"method\" : \"reverse\" , \"latlng\" : latlng , \"key\" : KEY } json_response = api_call ( GEOCODE_URL , params = params ) city = geo_get_ ( json_response , \"locality\" , \"postal_town\" ) # place_id = json_response.get(\"results\", [{}])[0].get(\"place_id\") country = geo_get_ ( json_response , \"country\" ) address = get_address ( json_response ) return { \"city\" : city , \"country\" : country , \"formatted_address\" : address } dotenv . load_dotenv ( \"google/.env\" ) dotenv . load_dotenv ( \".env\" ) PYTHON_ENV = os . environ . get ( \"PYTHON_ENV\" , \"dev\" ) if PYTHON_ENV != \"prod\" : KEY = None else : KEY = os . environ . get ( \"GOOGLE_API_KEY\" , None ) CACHE = get_cache ( \"google_timeline\" ) DETAILS_URL = \"https://maps.googleapis.com/maps/api/place/details/json\" NEARBY_URL = 'https://maps.googleapis.com/maps/api/place/nearbysearch/json' s = requests . Session () # {'Boating', 'Cycling', 'Driving', 'Flying', 'In transit', 'Moving', 'On a bus', 'On a ferry', 'On a train', 'On a tram', 'On the subway', 'Running', 'Walking'} def get_results ( latlng , name , excluded_transport_names ): if name in excluded_transport_names : return geo_get_info ( latlng ) near_result = get_nearby_results ( latlng , name , excluded_transport_names ) if near_result is None : return None details = get_details ( near_result [ \"place_id\" ]) if details is None : return None return { \"city\" : get_ ( details , \"locality\" , \"postal_town\" ), \"country\" : get_ ( details , \"country\" ), \"rating\" : details , \"details_name\" : details . get ( \"name\" ), \"formatted_address\" : details . get ( \"formatted_address\" ), 'international_phone_number' : details . get ( 'international_phone_number' ), 'opening_hours' : details . get ( 'opening_hours' ), 'user_ratings_total' : details . get ( 'user_ratings_total' ), 'rating' : details . get ( 'rating' ), 'website' : details . get ( 'website' ), } def process ( df , excluded_transport_names , home_regex , work_regex , hometown_regex ): df [ \"start\" ] = pd . to_datetime ( df . start ) df [ \"end\" ] = pd . to_datetime ( df . end ) df [ \"around_noon\" ] = ( df . start . dt . hour & lt ; 12 ) & amp ; ( df . end . dt . hour & gt ; 12 ) df [ \"week\" ] = df . start . dt . weekday & lt ; 5 df [ \"city\" ] = df . city . fillna ( \"\" ) df [ \"transporting\" ] = df . category . isin ( excluded_transport_names ) if work_regex : df . loc [ df . name . str . contains ( work_regex , regex = True , na = False ), \"category\" ] = \"Work\" if home_regex : df . loc [ df . name . str . contains ( home_regex , regex = True , na = False ), \"category\" ] = \"Home\" # if hometown_regex: # df.loc[ # df.name.str.contains(hometown_regex, regex=True, na=False), \"category\" # ] = \"Hometown\" df = df . sort_values ( \"start\" ) . reset_index ( drop = True ) if \"Unnamed: 0\" in df . columns : del df [ \"Unnamed: 0\" ] df [ \"start\" ] = df [ \"start\" ] . dt . tz_convert ( tz ) df [ \"end\" ] = df [ \"end\" ] . dt . tz_convert ( tz ) df . index = pd . IntervalIndex . from_arrays ( df [ 'start' ], df [ 'end' ]) return df class GooglePlaces ( Places ): nlp_columns = [ \"name\" , \"city\" , \"country\" , \"category\" , \"website\" ] selected_columns = [ \"date\" , \"name\" , \"city\" , \"_office_hours\" ] @classmethod def load ( cls , file_glob = \"~/Downloads/timeline_data-*\" , nrows = None ): df = pd . read_csv ( max ( just . glob ( file_glob )), nrows = nrows ) unique_locs = set ([(( y , z ), x ) for x , y , z in zip ( df . name , df . lat , df . lon ) if y != \"nan\" ]) excluded_transport_names = set ( df [ df . name == df . category ] . name ) details_data = [] for ( lat , lon ), name in unique_locs : d = get_results (( lat , lon ), name , excluded_transport_names ) if d is None : continue d [ \"lat\" ] = lat d [ \"lon\" ] = lon d [ \"name\" ] = name details_data . append ( d ) details_data = pd . DataFrame ( details_data ) # all_loc = df.merge(details_data, on=[\"name\", \"lat\", \"lon\"], how=\"outer\") places = df . merge ( details_data , on = [ \"name\" , \"lat\" , \"lon\" ], how = \"inner\" ) # all_loc = process(all_loc, excluded_transport_names) home_regex = \"|\" . join ( cls . home ) work_regex = \"|\" . join ( cls . work ) hometown_regex = \"|\" . join ( cls . hometown ) places = process ( places , excluded_transport_names , home_regex , work_regex , hometown_regex ) return cls ( places ) GooglePlaces . work = [ \"Jibes\" , \"Sleepboot\" , \"De Meerpaal\" , \"Papendorp\" , \"Vektis\" , \"Work \" ] # at_work = places.at_work() # # places.travel_by_car()\\ # .duration_longer_than(minutes=30)\\ # .at_day(at_work) # .at_time(\"november 2019\")\\ # .during_weekdays\\ # cat_str = \", \".join([\"{}: {}\".format(i, x) for i, x in enumerate(places.category)]) # [places.iloc[[int(y) for y in x]] for x in re.findall(\", \".join([\"([0-9]+): {}\".format(x) for x in pats]), cat_str)]","title":"Module nostalgia.sources.google.timeline"},{"location":"reference/nostalgia/sources/google/timeline/#variables","text":"CACHE DETAILS_URL GEOCODE_URL KEY NEARBY_URL PYTHON_ENV s","title":"Variables"},{"location":"reference/nostalgia/sources/google/timeline/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/google/timeline/#api_call","text":"def api_call ( url , params ) View Source def api_call(url, params): key = get_hash(url, params) status = CACHE.get(key, {}).get(\"status\") if status == \"ZERO_RESULTS\": return None if status == \"OK\": return CACHE[key] jdata = s.get(url, params=params).json() jdata[\"meta\"] = { \"time\": str(datetime.utcnow()), \"url\": url, \"params\": {k: v for k, v in params.items() if k != \"key\"}, } if jdata.get(\"status\") == \"ZERO_RESULTS\": CACHE[key] = jdata return None if jdata.get(\"status\") != \"OK\": print(\"status\", url, params, key, jdata.get(\"status\")) raise ValueError(\"not ok\") CACHE[key] = jdata return jdata","title":"api_call"},{"location":"reference/nostalgia/sources/google/timeline/#geo_get_","text":"def geo_get_ ( json_response , * types ) View Source def geo_get_(json_response, *types): for res in json_response.get(\"results\", []): for tp in types: for addr in res.get('address_components', []): if tp in addr[\"types\"]: return addr[\"long_name\"]","title":"geo_get_"},{"location":"reference/nostalgia/sources/google/timeline/#geo_get_info","text":"def geo_get_info ( latlng ) View Source def geo_get_info(latlng): latlng = format_latlng(latlng) if latlng == \"nan, nan\": return None params = {\"method\": \"reverse\", \"latlng\": latlng, \"key\": KEY} json_response = api_call(GEOCODE_URL, params=params) city = geo_get_(json_response, \"locality\", \"postal_town\") # place_id = json_response.get(\"results\", [{}])[0].get(\"place_id\") country = geo_get_(json_response, \"country\") address = get_address(json_response) return {\"city\": city, \"country\": country, \"formatted_address\": address}","title":"geo_get_info"},{"location":"reference/nostalgia/sources/google/timeline/#get_","text":"def get_ ( details , * types ) View Source def get_(details, *types): for tp in types: for addr in details.get('address_components', []): if tp in addr[\"types\"]: return addr[\"long_name\"]","title":"get_"},{"location":"reference/nostalgia/sources/google/timeline/#get_address","text":"def get_address ( json_response ) View Source def get_address(json_response): for res in json_response.get(\"results\", []): addr = res.get('formatted_address') if addr: return addr","title":"get_address"},{"location":"reference/nostalgia/sources/google/timeline/#get_details","text":"def get_details ( place_id ) View Source def get_details(place_id): params = {'placeid': place_id, 'sensor': 'false', 'key': KEY} return api_call(DETAILS_URL, params).get(\"result\", {})","title":"get_details"},{"location":"reference/nostalgia/sources/google/timeline/#get_hash","text":"def get_hash ( url , params ) View Source def get_hash(url, params): data = {k: v for k, v in params.items() if k != \"key\"} data[\"url\"] = url data = sorted(data.items()) return hashlib.sha256(str(data).encode(\"utf8\")).hexdigest()","title":"get_hash"},{"location":"reference/nostalgia/sources/google/timeline/#get_nearby","text":"def get_nearby ( latlng , name , excluded_transport_names ) View Source def get_nearby(latlng, name, excluded_transport_names): latlng = format_latlng(latlng) if latlng == \"nan, nan\": return None params = {\"location\": latlng, \"radius\": 100, \"key\": KEY} if name not in excluded_transport_names: params[\"query\"] = name return api_call(NEARBY_URL, params)","title":"get_nearby"},{"location":"reference/nostalgia/sources/google/timeline/#get_nearby_results","text":"def get_nearby_results ( latlng , name , excluded_transport_names ) View Source def get_nearby_results(latlng, name, excluded_transport_names): near = get_nearby(latlng, name, excluded_transport_names) if near is None: return None res = [x for x in near[\"results\"] if name in excluded_transport_names or x[\"name\"] == name] for x in res: if \"opening_hours\" in x: return x if res: return res[0] if not near[\"results\"]: return None return near[\"results\"][0]","title":"get_nearby_results"},{"location":"reference/nostalgia/sources/google/timeline/#get_results","text":"def get_results ( latlng , name , excluded_transport_names ) View Source def get_results(latlng, name, excluded_transport_names): if name in excluded_transport_names: return geo_get_info(latlng) near_result = get_nearby_results(latlng, name, excluded_transport_names) if near_result is None: return None details = get_details(near_result[\"place_id\"]) if details is None: return None return { \"city\": get_(details, \"locality\", \"postal_town\"), \"country\": get_(details, \"country\"), \"rating\": details, \"details_name\": details.get(\"name\"), \"formatted_address\": details.get(\"formatted_address\"), 'international_phone_number': details.get('international_phone_number'), 'opening_hours': details.get('opening_hours'), 'user_ratings_total': details.get('user_ratings_total'), 'rating': details.get('rating'), 'website': details.get('website'), }","title":"get_results"},{"location":"reference/nostalgia/sources/google/timeline/#process","text":"def process ( df , excluded_transport_names , home_regex , work_regex , hometown_regex ) View Source def process ( df , excluded_transport_names , home_regex , work_regex , hometown_regex ): df [ \"start\" ] = pd . to_datetime ( df . start ) df [ \"end\" ] = pd . to_datetime ( df . end ) df [ \"around_noon\" ] = ( df . start . dt . hour &lt ; 12 ) &amp ; ( df . end . dt . hour &gt ; 12 ) df [ \"week\" ] = df . start . dt . weekday &lt ; 5 df [ \"city\" ] = df . city . fillna ( \"\" ) df [ \"transporting\" ] = df . category . isin ( excluded_transport_names ) if work_regex: df . loc [ df . name . str . contains ( work_regex , regex = True , na = False ), \"category\" ] = \"Work\" if home_regex: df . loc [ df . name . str . contains ( home_regex , regex = True , na = False ), \"category\" ] = \"Home\" # if hometown_regex: # df.loc[ # df.name.str.contains(hometown_regex, regex=True, na=False), \"category\" # ] = \"Hometown\" df = df . sort_values ( \"start\" ). reset_index ( drop = True ) if \"Unnamed: 0\" in df . columns: del df [ \"Unnamed: 0\" ] df [ \"start\" ] = df [ \"start\" ]. dt . tz_convert ( tz ) df [ \"end\" ] = df [ \"end\" ]. dt . tz_convert ( tz ) df . index = pd . IntervalIndex . from_arrays ( df [ 'start' ], df [ 'end' ]) return df","title":"process"},{"location":"reference/nostalgia/sources/google/timeline/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/google/timeline/#googleplaces","text":"class GooglePlaces ( data ) View Source class GooglePlaces ( Places ) : nlp_columns = [ \"name\" , \"city\" , \"country\" , \"category\" , \"website\" ] selected_columns = [ \"date\" , \"name\" , \"city\" , \"_office_hours\" ] @classmethod def load ( cls , file_glob = \"~/Downloads/timeline_data-*\" , nrows = None ) : df = pd . read_csv ( max ( just . glob ( file_glob )), nrows = nrows ) unique_locs = set ([(( y , z ), x ) for x , y , z in zip ( df . name , df . lat , df . lon ) if y != \"nan\" ]) excluded_transport_names = set ( df [ df . name == df . category ]. name ) details_data = [] for ( lat , lon ), name in unique_locs : d = get_results (( lat , lon ), name , excluded_transport_names ) if d is None : continue d [ \"lat\" ] = lat d [ \"lon\" ] = lon d [ \"name\" ] = name details_data . append ( d ) details_data = pd . DataFrame ( details_data ) # all_loc = df . merge ( details_data , on = [ \"name\" , \"lat\" , \"lon\" ], how = \"outer\" ) places = df . merge ( details_data , on = [ \"name\" , \"lat\" , \"lon\" ], how = \"inner\" ) # all_loc = process ( all_loc , excluded_transport_names ) home_regex = \"|\" . join ( cls . home ) work_regex = \"|\" . join ( cls . work ) hometown_regex = \"|\" . join ( cls . hometown ) places = process ( places , excluded_transport_names , home_regex , work_regex , hometown_regex ) return cls ( places )","title":"GooglePlaces"},{"location":"reference/nostalgia/sources/google/timeline/#ancestors-in-mro","text":"nostalgia.interfaces.places.Places nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/google/timeline/#class-variables","text":"home hometown keywords nlp_columns nlp_when selected_columns vendor work","title":"Class variables"},{"location":"reference/nostalgia/sources/google/timeline/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/google/timeline/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/google/timeline/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/google/timeline/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/google/timeline/#load","text":"def load ( file_glob = '~/Downloads/timeline_data-*' , nrows = None ) View Source @classmethod def load(cls, file_glob=\"~/Downloads/timeline_data-*\", nrows=None): df = pd.read_csv(max(just.glob(file_glob)), nrows=nrows) unique_locs = set([((y, z), x) for x, y, z in zip(df.name, df.lat, df.lon) if y != \"nan\"]) excluded_transport_names = set(df[df.name == df.category].name) details_data = [] for (lat, lon), name in unique_locs: d = get_results((lat, lon), name, excluded_transport_names) if d is None: continue d[\"lat\"] = lat d[\"lon\"] = lon d[\"name\"] = name details_data.append(d) details_data = pd.DataFrame(details_data) # all_loc = df.merge(details_data, on=[\"name\", \"lat\", \"lon\"], how=\"outer\") places = df.merge(details_data, on=[\"name\", \"lat\", \"lon\"], how=\"inner\") # all_loc = process(all_loc, excluded_transport_names) home_regex = \"|\".join(cls.home) work_regex = \"|\".join(cls.work) hometown_regex = \"|\".join(cls.hometown) places = process(places, excluded_transport_names, home_regex, work_regex, hometown_regex) return cls(places)","title":"load"},{"location":"reference/nostalgia/sources/google/timeline/#instance-variables","text":"at_hometown df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/google/timeline/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/google/timeline/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/google/timeline/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/google/timeline/#at","text":"def at ( self , time ) View Source def at(self, time): mp = parse_date_tz(time) return self[self.index.overlaps(pd.Interval(mp.start_date, mp.end_date))]","title":"at"},{"location":"reference/nostalgia/sources/google/timeline/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/google/timeline/#at_home","text":"def at_home ( self ) View Source @nlp(\"filter\", \"at home\") def at_home(self): return self.__class__(self[self.category == \"Home\"])","title":"at_home"},{"location":"reference/nostalgia/sources/google/timeline/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/google/timeline/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/google/timeline/#at_work","text":"def at_work ( self ) View Source @nlp(\"filter\", \"at work\") def at_work(self): return self.__class__(self[self.category == \"Work\"])","title":"at_work"},{"location":"reference/nostalgia/sources/google/timeline/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/google/timeline/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/google/timeline/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/google/timeline/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/google/timeline/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/google/timeline/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/google/timeline/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/google/timeline/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/google/timeline/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/google/timeline/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/google/timeline/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/google/timeline/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/google/timeline/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/google/timeline/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/google/timeline/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/google/timeline/#length","text":"def length ( self ) View Source @nlp(\"end\", \"how long\", \"how much time\") def length(self): return (self.index.right - self.index.left).to_pytimedelta().sum()","title":"length"},{"location":"reference/nostalgia/sources/google/timeline/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/google/timeline/#near_","text":"def near_ ( self , distance_in_meters , other_places = None ) View Source def near_(self, distance_in_meters, other_places=None): places = get_type_from_registry(\"places\") if other_places is None else other_places nearbies = [] for _, row in places[[\"lat\", \"lon\"]].drop_duplicates().iterrows(): nearbies.append(haversine(self.lat, self.lon, *row) &lt; distance_in_meters) return self.__class__(self[np.any(nearbies, axis=0)])","title":"near_"},{"location":"reference/nostalgia/sources/google/timeline/#near_home","text":"def near_home ( self , distance = 1000 ) View Source @nlp(\"filter\", [\"near home\", \"around home\"]) def near_home(self, distance=1000): return self.near_(distance, get_type_from_registry(\"places\").at_home)","title":"near_home"},{"location":"reference/nostalgia/sources/google/timeline/#near_work","text":"def near_work ( self , distance = 1000 ) View Source @nlp(\"filter\", [\"near work\", \"around work\"]) def near_work(self, distance=1000): return self.near_(distance, get_type_from_registry(\"places\").at_work)","title":"near_work"},{"location":"reference/nostalgia/sources/google/timeline/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/google/timeline/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/google/timeline/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/google/timeline/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/google/timeline/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/google/timeline/#sum","text":"def sum ( self ) View Source @nlp(\"end\", \"how much\") def sum(self): return self.distance.sum()","title":"sum"},{"location":"reference/nostalgia/sources/google/timeline/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/google/timeline/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/google/timeline/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/google/timeline/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/google/timeline/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/google/timeline/#travel","text":"def travel ( self ) View Source @nlp(\"filter\", \"travel\") def travel(self): return self[self.transporting]","title":"travel"},{"location":"reference/nostalgia/sources/google/timeline/#travel_by_bus","text":"def travel_by_bus ( self ) View Source @nlp( \"filter\", \"drive by bus\", \"by bus\", \"travel by bus\", \"using the bus\", \"going by bus\", \"on a bus\", ) def travel_by_bus(self): return self.col_contains(\"On a bus\", \"category\")","title":"travel_by_bus"},{"location":"reference/nostalgia/sources/google/timeline/#travel_by_car","text":"def travel_by_car ( self ) View Source @nlp( \"filter\", \"drive\", \"by car\", \"travel by car\", \"travel using the car\", \"by car\", \"going by car\", \"on a car\", \"driving\", ) def travel_by_car(self): return self.col_contains(\"Driving\", \"category\")","title":"travel_by_car"},{"location":"reference/nostalgia/sources/google/timeline/#travel_by_train","text":"def travel_by_train ( self ) View Source @nlp( \"filter\", \"drive by train\", \"by train\", \"travel by train\", \"using the train\", \"going by train\", \"on a train\", ) def travel_by_train(self): return self.col_contains(\"On a train\", \"category\")","title":"travel_by_train"},{"location":"reference/nostalgia/sources/google/timeline/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/google/timeline/#what_address","text":"def what_address ( self ) View Source @nlp(\"end\", \"address of\", \"what is the address\", \"how to find\", \"how can i find\") def what_address(self): return self[\"address\"]","title":"what_address"},{"location":"reference/nostalgia/sources/google/timeline/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/google/timeline/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/google/timeline/#work_days","text":"def work_days ( self ) View Source @nlp(\"filter\", \"work days\", \"work-days\", \"on working days\") def work_days(self): return self[self.time.dt.weekday &lt; 5]","title":"work_days"},{"location":"reference/nostalgia/sources/google/timeline/#work_hours","text":"def work_hours ( self ) View Source @nlp(\"filter\", \"during work\", \"during work hours\") def work_hours(self): return self[self._office_hours]","title":"work_hours"},{"location":"reference/nostalgia/sources/google/timeline_download/","text":"Module nostalgia.sources.google.timeline_download View Source import time import os import shutil from selenium import webdriver from calendar import monthrange from datetime import datetime from selenium . common . exceptions import ( NoSuchElementException , ElementNotVisibleException , ElementClickInterceptedException , ElementNotInteractableException , ) SELENIUM_EXCEPTIONS = ( NoSuchElementException , ElementNotVisibleException , ElementClickInterceptedException , ElementNotInteractableException , ) BASE_PATH = os . path . expanduser ( \"~/Downloads\" ) # chrome_options = webdriver.ChromeOptions() # chrome_options.binary_location = \"/usr/bin/google-chrome-stable\" def move ( fname ) : src = os . path . join ( BASE_PATH , fname ) try : shutil . move ( src , os . path . join ( BASE_PATH , \"ghistory\" , fname )) except FileNotFoundError : print ( \"ERROR: file not found\" , src ) def exists ( year , month , day ) : fname = \"history-{}-{:02d}-{:02d}.kml\" . format ( year , month , day ) return os . path . exists ( os . path . join ( BASE_PATH , \"ghistory\" , fname )) def get_last_day_of_month ( year , num_month ) : return monthrange ( year , num_month )[ 1 ] def select_year ( year ) : driver . find_element_by_class_name ( \"year-picker\" ). click () time . sleep ( 0.3 ) for ele in driver . find_elements_by_xpath ( \"//div[text() = '{}']/parent::*\" . format ( year )) : try : ele . click () break except SELENIUM_EXCEPTIONS : pass time . sleep ( 1 ) def select_month ( month ) : driver . find_element_by_class_name ( \"month-picker\" ). click () time . sleep ( 0.3 ) for ele in driver . find_elements_by_xpath ( \"//div[text() = '{}']/parent::*\" . format ( month )) : try : ele . click () break except SELENIUM_EXCEPTIONS : pass time . sleep ( 1 ) def click_download () : driver . find_element_by_xpath ( \"//div[@aria-label=' Settings ']\" ). click () time . sleep ( 1 ) for ele in driver . find_elements_by_xpath ( \"//div[text() = 'Export this day to KML']/parent::*\" ) : try : ele . click () break except SELENIUM_EXCEPTIONS : pass time . sleep ( 1 ) MONTHS = [ \"January\" , \"February\" , \"March\" , \"April\" , \"May\" , \"June\" , \"July\" , \"August\" , \"September\" , \"October\" , \"November\" , \"December\" , ] if __ name__ == \"__main__\" : driver = webdriver . Chrome () # , options = chrome_options url = \"https://www.google.com/maps/timeline?pb=!1m2!1m1!1s2019-02-29\" driver . get ( url ) now = datetime . now () today = ( now . year , now . month , now . day ) # wait for being logged in while not driver . find_elements_by_xpath ( \"//span[@class='header-feature-name' and text() = ' Timeline ']\" ) : time . sleep ( 2 ) time . sleep ( 5 ) try : last_clicked_year = None last_clicked_month = None for year in range ( now . year , 2014 , - 1 ) : for num_month , month in enumerate ( MONTHS [ ::- 1 ]) : num_month = 12 - num_month for day in range ( get_last_day_of_month ( year , num_month ), 0 , - 1 ) : date_tuple = ( year , num_month , day ) if date_tuple &gt ; = today : continue if exists ( year , num_month , day ) : continue print ( num_month , date_tuple ) # select year if neccessary if last_clicked_year ! = year : select_year ( year ) last_clicked_year = year # select month if neccessary if last_clicked_month ! = month : select_month ( month ) last_clicked_month = month # handle day driver . find_element_by_class_name ( \"day-picker\" ). click () driver . find_element_by_xpath ( \"//td[@aria-label='{} {}']\" . format ( day , month [ : 3 ]) ). click () time . sleep ( 3 ) click_download () time . sleep ( 3 ) fname = \"history-{}-{:02d}-{:02d}.kml\" . format ( year , num_month , day ) move ( fname ) finally : driver . quit () from nostalgia . sources . google . timeline_transform import * from nostalgia . sources . google . timeline import * Variables BASE_PATH MONTHS SELENIUM_EXCEPTIONS Functions click_download def click_download ( ) View Source def click_download(): driver.find_element_by_xpath(\"//div[@aria-label=' Settings ']\").click() time.sleep(1) for ele in driver.find_elements_by_xpath(\"//div[text() = 'Export this day to KML']/parent::*\"): try: ele.click() break except SELENIUM_EXCEPTIONS: pass time.sleep(1) exists def exists ( year , month , day ) View Source def exists(year, month, day): fname = \"history-{}-{:02d}-{:02d}.kml\".format(year, month, day) return os.path.exists(os.path.join(BASE_PATH, \"ghistory\", fname)) get_last_day_of_month def get_last_day_of_month ( year , num_month ) View Source def get_last_day_of_month(year, num_month): return monthrange(year, num_month)[1] move def move ( fname ) View Source def move(fname): src = os.path.join(BASE_PATH, fname) try: shutil.move(src, os.path.join(BASE_PATH, \"ghistory\", fname)) except FileNotFoundError: print(\"ERROR: file not found\", src) select_month def select_month ( month ) View Source def select_month(month): driver.find_element_by_class_name(\"month-picker\").click() time.sleep(0.3) for ele in driver.find_elements_by_xpath(\"//div[text() = '{}']/parent::*\".format(month)): try: ele.click() break except SELENIUM_EXCEPTIONS: pass time.sleep(1) select_year def select_year ( year ) View Source def select_year(year): driver.find_element_by_class_name(\"year-picker\").click() time.sleep(0.3) for ele in driver.find_elements_by_xpath(\"//div[text() = '{}']/parent::*\".format(year)): try: ele.click() break except SELENIUM_EXCEPTIONS: pass time.sleep(1)","title":"Timeline Download"},{"location":"reference/nostalgia/sources/google/timeline_download/#module-nostalgiasourcesgoogletimeline_download","text":"View Source import time import os import shutil from selenium import webdriver from calendar import monthrange from datetime import datetime from selenium . common . exceptions import ( NoSuchElementException , ElementNotVisibleException , ElementClickInterceptedException , ElementNotInteractableException , ) SELENIUM_EXCEPTIONS = ( NoSuchElementException , ElementNotVisibleException , ElementClickInterceptedException , ElementNotInteractableException , ) BASE_PATH = os . path . expanduser ( \"~/Downloads\" ) # chrome_options = webdriver.ChromeOptions() # chrome_options.binary_location = \"/usr/bin/google-chrome-stable\" def move ( fname ) : src = os . path . join ( BASE_PATH , fname ) try : shutil . move ( src , os . path . join ( BASE_PATH , \"ghistory\" , fname )) except FileNotFoundError : print ( \"ERROR: file not found\" , src ) def exists ( year , month , day ) : fname = \"history-{}-{:02d}-{:02d}.kml\" . format ( year , month , day ) return os . path . exists ( os . path . join ( BASE_PATH , \"ghistory\" , fname )) def get_last_day_of_month ( year , num_month ) : return monthrange ( year , num_month )[ 1 ] def select_year ( year ) : driver . find_element_by_class_name ( \"year-picker\" ). click () time . sleep ( 0.3 ) for ele in driver . find_elements_by_xpath ( \"//div[text() = '{}']/parent::*\" . format ( year )) : try : ele . click () break except SELENIUM_EXCEPTIONS : pass time . sleep ( 1 ) def select_month ( month ) : driver . find_element_by_class_name ( \"month-picker\" ). click () time . sleep ( 0.3 ) for ele in driver . find_elements_by_xpath ( \"//div[text() = '{}']/parent::*\" . format ( month )) : try : ele . click () break except SELENIUM_EXCEPTIONS : pass time . sleep ( 1 ) def click_download () : driver . find_element_by_xpath ( \"//div[@aria-label=' Settings ']\" ). click () time . sleep ( 1 ) for ele in driver . find_elements_by_xpath ( \"//div[text() = 'Export this day to KML']/parent::*\" ) : try : ele . click () break except SELENIUM_EXCEPTIONS : pass time . sleep ( 1 ) MONTHS = [ \"January\" , \"February\" , \"March\" , \"April\" , \"May\" , \"June\" , \"July\" , \"August\" , \"September\" , \"October\" , \"November\" , \"December\" , ] if __ name__ == \"__main__\" : driver = webdriver . Chrome () # , options = chrome_options url = \"https://www.google.com/maps/timeline?pb=!1m2!1m1!1s2019-02-29\" driver . get ( url ) now = datetime . now () today = ( now . year , now . month , now . day ) # wait for being logged in while not driver . find_elements_by_xpath ( \"//span[@class='header-feature-name' and text() = ' Timeline ']\" ) : time . sleep ( 2 ) time . sleep ( 5 ) try : last_clicked_year = None last_clicked_month = None for year in range ( now . year , 2014 , - 1 ) : for num_month , month in enumerate ( MONTHS [ ::- 1 ]) : num_month = 12 - num_month for day in range ( get_last_day_of_month ( year , num_month ), 0 , - 1 ) : date_tuple = ( year , num_month , day ) if date_tuple &gt ; = today : continue if exists ( year , num_month , day ) : continue print ( num_month , date_tuple ) # select year if neccessary if last_clicked_year ! = year : select_year ( year ) last_clicked_year = year # select month if neccessary if last_clicked_month ! = month : select_month ( month ) last_clicked_month = month # handle day driver . find_element_by_class_name ( \"day-picker\" ). click () driver . find_element_by_xpath ( \"//td[@aria-label='{} {}']\" . format ( day , month [ : 3 ]) ). click () time . sleep ( 3 ) click_download () time . sleep ( 3 ) fname = \"history-{}-{:02d}-{:02d}.kml\" . format ( year , num_month , day ) move ( fname ) finally : driver . quit () from nostalgia . sources . google . timeline_transform import * from nostalgia . sources . google . timeline import *","title":"Module nostalgia.sources.google.timeline_download"},{"location":"reference/nostalgia/sources/google/timeline_download/#variables","text":"BASE_PATH MONTHS SELENIUM_EXCEPTIONS","title":"Variables"},{"location":"reference/nostalgia/sources/google/timeline_download/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/google/timeline_download/#click_download","text":"def click_download ( ) View Source def click_download(): driver.find_element_by_xpath(\"//div[@aria-label=' Settings ']\").click() time.sleep(1) for ele in driver.find_elements_by_xpath(\"//div[text() = 'Export this day to KML']/parent::*\"): try: ele.click() break except SELENIUM_EXCEPTIONS: pass time.sleep(1)","title":"click_download"},{"location":"reference/nostalgia/sources/google/timeline_download/#exists","text":"def exists ( year , month , day ) View Source def exists(year, month, day): fname = \"history-{}-{:02d}-{:02d}.kml\".format(year, month, day) return os.path.exists(os.path.join(BASE_PATH, \"ghistory\", fname))","title":"exists"},{"location":"reference/nostalgia/sources/google/timeline_download/#get_last_day_of_month","text":"def get_last_day_of_month ( year , num_month ) View Source def get_last_day_of_month(year, num_month): return monthrange(year, num_month)[1]","title":"get_last_day_of_month"},{"location":"reference/nostalgia/sources/google/timeline_download/#move","text":"def move ( fname ) View Source def move(fname): src = os.path.join(BASE_PATH, fname) try: shutil.move(src, os.path.join(BASE_PATH, \"ghistory\", fname)) except FileNotFoundError: print(\"ERROR: file not found\", src)","title":"move"},{"location":"reference/nostalgia/sources/google/timeline_download/#select_month","text":"def select_month ( month ) View Source def select_month(month): driver.find_element_by_class_name(\"month-picker\").click() time.sleep(0.3) for ele in driver.find_elements_by_xpath(\"//div[text() = '{}']/parent::*\".format(month)): try: ele.click() break except SELENIUM_EXCEPTIONS: pass time.sleep(1)","title":"select_month"},{"location":"reference/nostalgia/sources/google/timeline_download/#select_year","text":"def select_year ( year ) View Source def select_year(year): driver.find_element_by_class_name(\"year-picker\").click() time.sleep(0.3) for ele in driver.find_elements_by_xpath(\"//div[text() = '{}']/parent::*\".format(year)): try: ele.click() break except SELENIUM_EXCEPTIONS: pass time.sleep(1)","title":"select_year"},{"location":"reference/nostalgia/sources/google/timeline_transform/","text":"Module nostalgia.sources.google.timeline_transform View Source # &lt;Placemark&gt; # &lt;name&gt;Home (Laan van Henegouwen 16)&lt;/name&gt; # &lt;address&gt;3703 TD Zeist&lt;/address&gt; # &lt;ExtendedData&gt; # &lt;Data name=\"Email\"&gt; # &lt;value&gt;kootenpv@gmail.com&lt;/value&gt; # &lt;/Data&gt; # &lt;Data name=\"Category\"&gt; # &lt;value/&gt; # &lt;/Data&gt; # &lt;Data name=\"Distance\"&gt; # &lt;value&gt;0&lt;/value&gt; # &lt;/Data&gt; # &lt;/ExtendedData&gt; # &lt;description&gt; from 2019-01-04T16:41:11.016Z to 2019-01-05T16:29:16.926Z. Distance 0m &lt;/description&gt; # &lt;Point&gt; # &lt;coordinates&gt;5.2261821,52.080966499999995,0&lt;/coordinates&gt; # &lt;/Point&gt; # &lt;TimeSpan&gt; # &lt;begin&gt;2019-01-04T16:41:11.016Z&lt;/begin&gt; # &lt;end&gt;2019-01-05T16:29:16.926Z&lt;/end&gt; # &lt;/TimeSpan&gt; # &lt;/Placemark&gt; import pandas as pd import lxml.etree from dateutil.parser import parse as date_parse import just from nostalgia.times import yesterday from nostalgia.utils import format_latlng N = { \"klm\" : \"http://www.opengis.net/kml/2.2\" } # cats = set() days = [] for fname in sorted ( just . glob ( \"/home/pascal/Downloads/ghistory/history-*.kml\" )): # for fname in sorted([\"/home/pascal/Downloads/ghistory/history-2018-07-25.kml\"]): tree = lxml . etree . parse ( fname ) for placemark in tree . xpath ( \"//klm:Placemark\" , namespaces = N ): name = placemark . xpath ( \"./klm:name/text()\" , namespaces = N )[ 0 ] address = placemark . xpath ( \"./klm:address/text()\" , namespaces = N ) address = address [ 0 ] if address else None start = date_parse ( placemark . xpath ( \"./klm:TimeSpan/klm:begin/text()\" , namespaces = N )[ 0 ]) end = date_parse ( placemark . xpath ( \"./klm:TimeSpan/klm:end/text()\" , namespaces = N )[ 0 ]) category = placemark . xpath ( \"./klm:ExtendedData/klm:Data[@name='Category']/klm:value/text()\" , namespaces = N ) category = category [ 0 ] if category else None # just a location if category is None and address is None : continue if category is None and \" (\" in name : category = name . split ( \" (\" )[ 0 ] if \"Steynlaan\" in name : category = \"Home\" distance = placemark . xpath ( \"./klm:ExtendedData/klm:Data[@name='Distance']/klm:value/text()\" , namespaces = N ) distance = distance [ 0 ] if distance else None # fuckers, lon|lat instead of lat|lon coords = placemark . xpath ( \"./klm:Point/klm:coordinates/text()\" , namespaces = N ) coords = coords or placemark . xpath ( \"./klm:LineString/klm:coordinates/text()\" , namespaces = N ) d = { \"date\" : start . date (), \"name\" : name , \"distance\" : distance , \"category\" : category , \"start\" : start , \"end\" : end , \"address\" : address , \"month\" : start . month , \"hour\" : start . hour , \"year\" : start . year , } loc = { \"lat\" : None , \"lon\" : None , \"coordinates\" : []} # here switch lon|lat coor = sum ( [ [ format_latlng (( x . split ( \",\" )[ 1 ], x . split ( \",\" )[ 0 ])) . split ( \", \" ) for x in c . split ()] for c in coords ], [], ) if coor : loc [ \"coordinates\" ] = coor loc [ \"lat\" ] = coor [ 0 ][ 0 ] loc [ \"lon\" ] = coor [ 0 ][ 1 ] # for data in placemark.xpath(\"//klm:Data\", namespaces=N): # cats.update(data.xpath(\"./@name\", namespaces=N)) d . update ( loc ) days . append ( d ) df = pd . DataFrame ( days ) df = df . reindex ( list ( d . keys ()), axis = 1 ) yday = yesterday () tmpl = \"/home/pascal/Downloads/timeline_data-{}-{:02d}-{:02d}.csv\" df . to_csv ( tmpl . format ( yday . year , yday . month , yday . day )) Variables N address category coor coords d days df distance end fname loc name placemark start tmpl tree yday","title":"Timeline Transform"},{"location":"reference/nostalgia/sources/google/timeline_transform/#module-nostalgiasourcesgoogletimeline_transform","text":"View Source # &lt;Placemark&gt; # &lt;name&gt;Home (Laan van Henegouwen 16)&lt;/name&gt; # &lt;address&gt;3703 TD Zeist&lt;/address&gt; # &lt;ExtendedData&gt; # &lt;Data name=\"Email\"&gt; # &lt;value&gt;kootenpv@gmail.com&lt;/value&gt; # &lt;/Data&gt; # &lt;Data name=\"Category\"&gt; # &lt;value/&gt; # &lt;/Data&gt; # &lt;Data name=\"Distance\"&gt; # &lt;value&gt;0&lt;/value&gt; # &lt;/Data&gt; # &lt;/ExtendedData&gt; # &lt;description&gt; from 2019-01-04T16:41:11.016Z to 2019-01-05T16:29:16.926Z. Distance 0m &lt;/description&gt; # &lt;Point&gt; # &lt;coordinates&gt;5.2261821,52.080966499999995,0&lt;/coordinates&gt; # &lt;/Point&gt; # &lt;TimeSpan&gt; # &lt;begin&gt;2019-01-04T16:41:11.016Z&lt;/begin&gt; # &lt;end&gt;2019-01-05T16:29:16.926Z&lt;/end&gt; # &lt;/TimeSpan&gt; # &lt;/Placemark&gt; import pandas as pd import lxml.etree from dateutil.parser import parse as date_parse import just from nostalgia.times import yesterday from nostalgia.utils import format_latlng N = { \"klm\" : \"http://www.opengis.net/kml/2.2\" } # cats = set() days = [] for fname in sorted ( just . glob ( \"/home/pascal/Downloads/ghistory/history-*.kml\" )): # for fname in sorted([\"/home/pascal/Downloads/ghistory/history-2018-07-25.kml\"]): tree = lxml . etree . parse ( fname ) for placemark in tree . xpath ( \"//klm:Placemark\" , namespaces = N ): name = placemark . xpath ( \"./klm:name/text()\" , namespaces = N )[ 0 ] address = placemark . xpath ( \"./klm:address/text()\" , namespaces = N ) address = address [ 0 ] if address else None start = date_parse ( placemark . xpath ( \"./klm:TimeSpan/klm:begin/text()\" , namespaces = N )[ 0 ]) end = date_parse ( placemark . xpath ( \"./klm:TimeSpan/klm:end/text()\" , namespaces = N )[ 0 ]) category = placemark . xpath ( \"./klm:ExtendedData/klm:Data[@name='Category']/klm:value/text()\" , namespaces = N ) category = category [ 0 ] if category else None # just a location if category is None and address is None : continue if category is None and \" (\" in name : category = name . split ( \" (\" )[ 0 ] if \"Steynlaan\" in name : category = \"Home\" distance = placemark . xpath ( \"./klm:ExtendedData/klm:Data[@name='Distance']/klm:value/text()\" , namespaces = N ) distance = distance [ 0 ] if distance else None # fuckers, lon|lat instead of lat|lon coords = placemark . xpath ( \"./klm:Point/klm:coordinates/text()\" , namespaces = N ) coords = coords or placemark . xpath ( \"./klm:LineString/klm:coordinates/text()\" , namespaces = N ) d = { \"date\" : start . date (), \"name\" : name , \"distance\" : distance , \"category\" : category , \"start\" : start , \"end\" : end , \"address\" : address , \"month\" : start . month , \"hour\" : start . hour , \"year\" : start . year , } loc = { \"lat\" : None , \"lon\" : None , \"coordinates\" : []} # here switch lon|lat coor = sum ( [ [ format_latlng (( x . split ( \",\" )[ 1 ], x . split ( \",\" )[ 0 ])) . split ( \", \" ) for x in c . split ()] for c in coords ], [], ) if coor : loc [ \"coordinates\" ] = coor loc [ \"lat\" ] = coor [ 0 ][ 0 ] loc [ \"lon\" ] = coor [ 0 ][ 1 ] # for data in placemark.xpath(\"//klm:Data\", namespaces=N): # cats.update(data.xpath(\"./@name\", namespaces=N)) d . update ( loc ) days . append ( d ) df = pd . DataFrame ( days ) df = df . reindex ( list ( d . keys ()), axis = 1 ) yday = yesterday () tmpl = \"/home/pascal/Downloads/timeline_data-{}-{:02d}-{:02d}.csv\" df . to_csv ( tmpl . format ( yday . year , yday . month , yday . day ))","title":"Module nostalgia.sources.google.timeline_transform"},{"location":"reference/nostalgia/sources/google/timeline_transform/#variables","text":"N address category coor coords d days df distance end fname loc name placemark start tmpl tree yday","title":"Variables"},{"location":"reference/nostalgia/sources/ing_banking/","text":"Module nostalgia.sources.ing_banking Sub-modules nostalgia.sources.ing_banking.mijn_ing nostalgia.sources.ing_banking.mijn_ing_download","title":"Index"},{"location":"reference/nostalgia/sources/ing_banking/#module-nostalgiasourcesing_banking","text":"","title":"Module nostalgia.sources.ing_banking"},{"location":"reference/nostalgia/sources/ing_banking/#sub-modules","text":"nostalgia.sources.ing_banking.mijn_ing nostalgia.sources.ing_banking.mijn_ing_download","title":"Sub-modules"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/","text":"Module nostalgia.sources.ing_banking.mijn_ing View Source import os import just import string from datetime import datetime from dateutil.relativedelta import relativedelta from dateutil.parser import parse from datetime import timedelta import numpy as np import pandas as pd from collections import Counter import re from nostalgia.times import try_date , tz from nostalgia.ndf import NDF from nostalgia.nlp import nlp digits = set ( string . digits ) def old_date ( col ): return pd . to_datetime ( col . str . slice ( 0 , 14 ), format = \" %d -%m-%y %H:%M\" , errors = \"coerce\" ) . dt . tz_localize ( tz ) class Payments ( NDF ): keywords = [ \"pay\" , \"buy\" , \"purchase\" , \"spend\" , \"cost\" ] nlp_columns = [ \"mededelingen\" , \"naam\" ] selected_columns = [ \"time\" , \"naam\" , \"bedrag\" , \"mededelingen\" ] @classmethod def handle_dataframe_per_file ( cls , data , fname = None ): ing = data ing [ \"Bedrag (EUR)\" ] = [ float ( x . replace ( \",\" , \".\" )) for x in ing [ \"Bedrag (EUR)\" ]] ing [ \"Datum\" ] = [ pd . to_datetime ( x , format = \"%Y%m %d \" ) for x in ing [ \"Datum\" ]] ing [ \"Naam / Omschrijving\" ] = [ x . replace ( \"'\" , \"''\" ) for x in ing [ \"Naam / Omschrijving\" ]] ing [ \"Naam / Omschrijving\" ] = [ x . replace ( '\"' , '\"\"' ) for x in ing [ \"Naam / Omschrijving\" ]] ing [ \"timestamp\" ] = [ x if pd . notnull ( x ) else try_date ( y , min_level = 3 ) # only minutes and below for x , y in zip ( old_date ( ing [ \"Naam / Omschrijving\" ]), ing . Mededelingen ) ] ing [ \"year\" ] = [ x . year for x in ing . timestamp ] ing [ \"pas\" ] = [ x if y == \"Betaalautomaat\" and x . startswith ( \"0\" ) else \"---\" for x , y in zip ( ing . Mededelingen . str . replace ( \":\" , \"\" ) . str . slice ( 10 , 13 ), ing . MutatieSoort ) ] # ing = ing.drop([\"Code\", \"Tegenrekening\", \"Rekening\", \"Mededelingen\"], axis=1) ing . columns = [ \"bedrag\" if x == \"Bedrag (EUR)\" else x for x in ing . columns ] ing . columns = [ \"naam\" if x == \"Naam / Omschrijving\" else x for x in ing . columns ] ing . columns = [ x . lower () . replace ( \" \" , \"_\" ) for x in ing . columns ] ing = ing [ ing . timestamp . notnull ()] return ing @classmethod def load ( cls , file_glob = \"~/Downloads/NL*20*20*.csv\" , nrows = None , from_cache = True ): data = cls . latest_file_is_historic ( file_glob , nrows = nrows , from_cache = from_cache ) return cls ( data ) @property def expenses ( self ): return self . query ( \"af_bij == 'Af'\" ) @nlp ( \"filter\" , \"by me\" , \"i\" , \"my\" ) def by_me ( self ): return self . query ( \"pas == '010'\" ) @property def by_card ( self ): return self . query ( \"pas != '---'\" ) @nlp ( \"end\" , \"how much\" ) def sum ( self ): return self . bedrag . sum () def equal_to ( self , amount ): return self . query ( \"bedrag == {}\" . format ( amount )) def amount_between ( self , lower_bound , upper_bound ): return self . query ( \"{} &lt; bedrag &lt; {}\" . format ( lower_bound , upper_bound )) Variables digits Functions old_date def old_date ( col ) View Source def old_date(col): return pd.to_datetime( col.str.slice(0, 14), format=\"%d-%m-%y %H:%M\", errors=\"coerce\" ).dt.tz_localize(tz) Classes Payments class Payments ( data ) View Source class Payments ( NDF ) : keywords = [ \"pay\" , \"buy\" , \"purchase\" , \"spend\" , \"cost\" ] nlp_columns = [ \"mededelingen\" , \"naam\" ] selected_columns = [ \"time\" , \"naam\" , \"bedrag\" , \"mededelingen\" ] @classmethod def handle_dataframe_per_file ( cls , data , fname = None ) : ing = data ing [ \"Bedrag (EUR)\" ] = [ float ( x . replace ( \",\" , \".\" )) for x in ing [ \"Bedrag (EUR)\" ]] ing [ \"Datum\" ] = [ pd . to_datetime ( x , format = \"%Y%m%d\" ) for x in ing [ \"Datum\" ]] ing [ \"Naam / Omschrijving\" ] = [ x . replace ( \"'\" , \"''\" ) for x in ing [ \"Naam / Omschrijving\" ]] ing [ \"Naam / Omschrijving\" ] = [ x . replace ( '\"' , '\"\"' ) for x in ing [ \"Naam / Omschrijving\" ]] ing [ \"timestamp\" ] = [ x if pd . notnull ( x ) else try_date ( y , min_level = 3 ) # only minutes and below for x , y in zip ( old_date ( ing [ \"Naam / Omschrijving\" ]), ing . Mededelingen ) ] ing [ \"year\" ] = [ x . year for x in ing . timestamp ] ing [ \"pas\" ] = [ x if y == \"Betaalautomaat\" and x . startswith ( \"0\" ) else \"---\" for x , y in zip ( ing . Mededelingen . str . replace ( \":\" , \"\" ). str . slice ( 10 , 13 ), ing . MutatieSoort ) ] # ing = ing . drop ([ \"Code\" , \"Tegenrekening\" , \"Rekening\" , \"Mededelingen\" ], axis = 1 ) ing . columns = [ \"bedrag\" if x == \"Bedrag (EUR)\" else x for x in ing . columns ] ing . columns = [ \"naam\" if x == \"Naam / Omschrijving\" else x for x in ing . columns ] ing . columns = [ x . lower (). replace ( \" \" , \"_\" ) for x in ing . columns ] ing = ing [ ing . timestamp . notnull ()] return ing @classmethod def load ( cls , file_glob = \"~/Downloads/NL*20*20*.csv\" , nrows = None , from_cache = True ) : data = cls . latest_file_is_historic ( file_glob , nrows = nrows , from_cache = from_cache ) return cls ( data ) @property def expenses ( self ) : return self . query ( \"af_bij == 'Af'\" ) @nlp ( \"filter\" , \"by me\" , \"i\" , \"my\" ) def by_me ( self ) : return self . query ( \"pas == '010'\" ) @property def by_card ( self ) : return self . query ( \"pas != '---'\" ) @nlp ( \"end\" , \"how much\" ) def sum ( self ) : return self . bedrag . sum () def equal_to ( self , amount ) : return self . query ( \"bedrag == {}\" . format ( amount )) def amount_between ( self , lower_bound , upper_bound ) : return self . query ( \"{} &lt; bedrag &lt; {}\" . format ( lower_bound , upper_bound )) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} handle_dataframe_per_file def handle_dataframe_per_file ( data , fname = None ) View Source @classmethod def handle_dataframe_per_file(cls, data, fname=None): ing = data ing[\"Bedrag (EUR)\"] = [float(x.replace(\",\", \".\")) for x in ing[\"Bedrag (EUR)\"]] ing[\"Datum\"] = [pd.to_datetime(x, format=\"%Y%m%d\") for x in ing[\"Datum\"]] ing[\"Naam / Omschrijving\"] = [x.replace(\"'\", \"''\") for x in ing[\"Naam / Omschrijving\"]] ing[\"Naam / Omschrijving\"] = [x.replace('\"', '\"\"') for x in ing[\"Naam / Omschrijving\"]] ing[\"timestamp\"] = [ x if pd.notnull(x) else try_date(y, min_level=3) # only minutes and below for x, y in zip(old_date(ing[\"Naam / Omschrijving\"]), ing.Mededelingen) ] ing[\"year\"] = [x.year for x in ing.timestamp] ing[\"pas\"] = [ x if y == \"Betaalautomaat\" and x.startswith(\"0\") else \"---\" for x, y in zip( ing.Mededelingen.str.replace(\":\", \"\").str.slice(10, 13), ing.MutatieSoort ) ] # ing = ing.drop([\"Code\", \"Tegenrekening\", \"Rekening\", \"Mededelingen\"], axis=1) ing.columns = [\"bedrag\" if x == \"Bedrag (EUR)\" else x for x in ing.columns] ing.columns = [\"naam\" if x == \"Naam / Omschrijving\" else x for x in ing.columns] ing.columns = [x.lower().replace(\" \", \"_\") for x in ing.columns] ing = ing[ing.timestamp.notnull()] return ing ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( file_glob = '~/Downloads/NL*20*20*.csv' , nrows = None , from_cache = True ) View Source @classmethod def load(cls, file_glob=\"~/Downloads/NL*20*20*.csv\", nrows=None, from_cache=True): data = cls.latest_file_is_historic(file_glob, nrows=nrows, from_cache=from_cache) return cls(data) Instance variables at_home at_work by_card df_name duration during_office_hours end expenses in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") amount_between def amount_between ( self , lower_bound , upper_bound ) View Source def amount_between(self, lower_bound, upper_bound): return self.query(\"{} &lt; bedrag &lt; {}\".format(lower_bound, upper_bound)) as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self.query(\"pas == '010'\") col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] equal_to def equal_to ( self , amount ) View Source def equal_to(self, amount): return self.query(\"bedrag == {}\".format(amount)) get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) sum def sum ( self ) View Source @nlp(\"end\", \"how much\") def sum(self): return self.bedrag.sum() tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Mijn Ing"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#module-nostalgiasourcesing_bankingmijn_ing","text":"View Source import os import just import string from datetime import datetime from dateutil.relativedelta import relativedelta from dateutil.parser import parse from datetime import timedelta import numpy as np import pandas as pd from collections import Counter import re from nostalgia.times import try_date , tz from nostalgia.ndf import NDF from nostalgia.nlp import nlp digits = set ( string . digits ) def old_date ( col ): return pd . to_datetime ( col . str . slice ( 0 , 14 ), format = \" %d -%m-%y %H:%M\" , errors = \"coerce\" ) . dt . tz_localize ( tz ) class Payments ( NDF ): keywords = [ \"pay\" , \"buy\" , \"purchase\" , \"spend\" , \"cost\" ] nlp_columns = [ \"mededelingen\" , \"naam\" ] selected_columns = [ \"time\" , \"naam\" , \"bedrag\" , \"mededelingen\" ] @classmethod def handle_dataframe_per_file ( cls , data , fname = None ): ing = data ing [ \"Bedrag (EUR)\" ] = [ float ( x . replace ( \",\" , \".\" )) for x in ing [ \"Bedrag (EUR)\" ]] ing [ \"Datum\" ] = [ pd . to_datetime ( x , format = \"%Y%m %d \" ) for x in ing [ \"Datum\" ]] ing [ \"Naam / Omschrijving\" ] = [ x . replace ( \"'\" , \"''\" ) for x in ing [ \"Naam / Omschrijving\" ]] ing [ \"Naam / Omschrijving\" ] = [ x . replace ( '\"' , '\"\"' ) for x in ing [ \"Naam / Omschrijving\" ]] ing [ \"timestamp\" ] = [ x if pd . notnull ( x ) else try_date ( y , min_level = 3 ) # only minutes and below for x , y in zip ( old_date ( ing [ \"Naam / Omschrijving\" ]), ing . Mededelingen ) ] ing [ \"year\" ] = [ x . year for x in ing . timestamp ] ing [ \"pas\" ] = [ x if y == \"Betaalautomaat\" and x . startswith ( \"0\" ) else \"---\" for x , y in zip ( ing . Mededelingen . str . replace ( \":\" , \"\" ) . str . slice ( 10 , 13 ), ing . MutatieSoort ) ] # ing = ing.drop([\"Code\", \"Tegenrekening\", \"Rekening\", \"Mededelingen\"], axis=1) ing . columns = [ \"bedrag\" if x == \"Bedrag (EUR)\" else x for x in ing . columns ] ing . columns = [ \"naam\" if x == \"Naam / Omschrijving\" else x for x in ing . columns ] ing . columns = [ x . lower () . replace ( \" \" , \"_\" ) for x in ing . columns ] ing = ing [ ing . timestamp . notnull ()] return ing @classmethod def load ( cls , file_glob = \"~/Downloads/NL*20*20*.csv\" , nrows = None , from_cache = True ): data = cls . latest_file_is_historic ( file_glob , nrows = nrows , from_cache = from_cache ) return cls ( data ) @property def expenses ( self ): return self . query ( \"af_bij == 'Af'\" ) @nlp ( \"filter\" , \"by me\" , \"i\" , \"my\" ) def by_me ( self ): return self . query ( \"pas == '010'\" ) @property def by_card ( self ): return self . query ( \"pas != '---'\" ) @nlp ( \"end\" , \"how much\" ) def sum ( self ): return self . bedrag . sum () def equal_to ( self , amount ): return self . query ( \"bedrag == {}\" . format ( amount )) def amount_between ( self , lower_bound , upper_bound ): return self . query ( \"{} &lt; bedrag &lt; {}\" . format ( lower_bound , upper_bound ))","title":"Module nostalgia.sources.ing_banking.mijn_ing"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#variables","text":"digits","title":"Variables"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#old_date","text":"def old_date ( col ) View Source def old_date(col): return pd.to_datetime( col.str.slice(0, 14), format=\"%d-%m-%y %H:%M\", errors=\"coerce\" ).dt.tz_localize(tz)","title":"old_date"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#payments","text":"class Payments ( data ) View Source class Payments ( NDF ) : keywords = [ \"pay\" , \"buy\" , \"purchase\" , \"spend\" , \"cost\" ] nlp_columns = [ \"mededelingen\" , \"naam\" ] selected_columns = [ \"time\" , \"naam\" , \"bedrag\" , \"mededelingen\" ] @classmethod def handle_dataframe_per_file ( cls , data , fname = None ) : ing = data ing [ \"Bedrag (EUR)\" ] = [ float ( x . replace ( \",\" , \".\" )) for x in ing [ \"Bedrag (EUR)\" ]] ing [ \"Datum\" ] = [ pd . to_datetime ( x , format = \"%Y%m%d\" ) for x in ing [ \"Datum\" ]] ing [ \"Naam / Omschrijving\" ] = [ x . replace ( \"'\" , \"''\" ) for x in ing [ \"Naam / Omschrijving\" ]] ing [ \"Naam / Omschrijving\" ] = [ x . replace ( '\"' , '\"\"' ) for x in ing [ \"Naam / Omschrijving\" ]] ing [ \"timestamp\" ] = [ x if pd . notnull ( x ) else try_date ( y , min_level = 3 ) # only minutes and below for x , y in zip ( old_date ( ing [ \"Naam / Omschrijving\" ]), ing . Mededelingen ) ] ing [ \"year\" ] = [ x . year for x in ing . timestamp ] ing [ \"pas\" ] = [ x if y == \"Betaalautomaat\" and x . startswith ( \"0\" ) else \"---\" for x , y in zip ( ing . Mededelingen . str . replace ( \":\" , \"\" ). str . slice ( 10 , 13 ), ing . MutatieSoort ) ] # ing = ing . drop ([ \"Code\" , \"Tegenrekening\" , \"Rekening\" , \"Mededelingen\" ], axis = 1 ) ing . columns = [ \"bedrag\" if x == \"Bedrag (EUR)\" else x for x in ing . columns ] ing . columns = [ \"naam\" if x == \"Naam / Omschrijving\" else x for x in ing . columns ] ing . columns = [ x . lower (). replace ( \" \" , \"_\" ) for x in ing . columns ] ing = ing [ ing . timestamp . notnull ()] return ing @classmethod def load ( cls , file_glob = \"~/Downloads/NL*20*20*.csv\" , nrows = None , from_cache = True ) : data = cls . latest_file_is_historic ( file_glob , nrows = nrows , from_cache = from_cache ) return cls ( data ) @property def expenses ( self ) : return self . query ( \"af_bij == 'Af'\" ) @nlp ( \"filter\" , \"by me\" , \"i\" , \"my\" ) def by_me ( self ) : return self . query ( \"pas == '010'\" ) @property def by_card ( self ) : return self . query ( \"pas != '---'\" ) @nlp ( \"end\" , \"how much\" ) def sum ( self ) : return self . bedrag . sum () def equal_to ( self , amount ) : return self . query ( \"bedrag == {}\" . format ( amount )) def amount_between ( self , lower_bound , upper_bound ) : return self . query ( \"{} &lt; bedrag &lt; {}\" . format ( lower_bound , upper_bound ))","title":"Payments"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#handle_dataframe_per_file","text":"def handle_dataframe_per_file ( data , fname = None ) View Source @classmethod def handle_dataframe_per_file(cls, data, fname=None): ing = data ing[\"Bedrag (EUR)\"] = [float(x.replace(\",\", \".\")) for x in ing[\"Bedrag (EUR)\"]] ing[\"Datum\"] = [pd.to_datetime(x, format=\"%Y%m%d\") for x in ing[\"Datum\"]] ing[\"Naam / Omschrijving\"] = [x.replace(\"'\", \"''\") for x in ing[\"Naam / Omschrijving\"]] ing[\"Naam / Omschrijving\"] = [x.replace('\"', '\"\"') for x in ing[\"Naam / Omschrijving\"]] ing[\"timestamp\"] = [ x if pd.notnull(x) else try_date(y, min_level=3) # only minutes and below for x, y in zip(old_date(ing[\"Naam / Omschrijving\"]), ing.Mededelingen) ] ing[\"year\"] = [x.year for x in ing.timestamp] ing[\"pas\"] = [ x if y == \"Betaalautomaat\" and x.startswith(\"0\") else \"---\" for x, y in zip( ing.Mededelingen.str.replace(\":\", \"\").str.slice(10, 13), ing.MutatieSoort ) ] # ing = ing.drop([\"Code\", \"Tegenrekening\", \"Rekening\", \"Mededelingen\"], axis=1) ing.columns = [\"bedrag\" if x == \"Bedrag (EUR)\" else x for x in ing.columns] ing.columns = [\"naam\" if x == \"Naam / Omschrijving\" else x for x in ing.columns] ing.columns = [x.lower().replace(\" \", \"_\") for x in ing.columns] ing = ing[ing.timestamp.notnull()] return ing","title":"handle_dataframe_per_file"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#load","text":"def load ( file_glob = '~/Downloads/NL*20*20*.csv' , nrows = None , from_cache = True ) View Source @classmethod def load(cls, file_glob=\"~/Downloads/NL*20*20*.csv\", nrows=None, from_cache=True): data = cls.latest_file_is_historic(file_glob, nrows=nrows, from_cache=from_cache) return cls(data)","title":"load"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#instance-variables","text":"at_home at_work by_card df_name duration during_office_hours end expenses in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#amount_between","text":"def amount_between ( self , lower_bound , upper_bound ) View Source def amount_between(self, lower_bound, upper_bound): return self.query(\"{} &lt; bedrag &lt; {}\".format(lower_bound, upper_bound))","title":"amount_between"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self.query(\"pas == '010'\")","title":"by_me"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#equal_to","text":"def equal_to ( self , amount ) View Source def equal_to(self, amount): return self.query(\"bedrag == {}\".format(amount))","title":"equal_to"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#sum","text":"def sum ( self ) View Source @nlp(\"end\", \"how much\") def sum(self): return self.bedrag.sum()","title":"sum"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing_download/","text":"Module nostalgia.sources.ing_banking.mijn_ing_download View Source from selenium import webdriver import time from datetime import datetime from dateutil.relativedelta import relativedelta def expand_shadow_element ( base , tags ): for tag in tags : print ( tag ) element = base . find_element_by_css_selector ( tag ) base = driver . execute_script ( 'return arguments[0].shadowRoot' , element ) return base if __name__ == \"__main__\" : chrome_options = webdriver . ChromeOptions () chrome_options . binary_location = \"/usr/bin/google-chrome-stable\" driver = webdriver . Chrome ( executable_path = \"/usr/bin/chromedriver\" , options = chrome_options ) url = \"https://mijn.ing.nl/banking/service\" driver . get ( url ) while driver . find_elements_by_xpath ( \"/html/body/ing-app-authentication\" ): time . sleep ( 1 ) time . sleep ( 5 ) tags = [ \"dba-app\" , \"ing-app-daily-banking-service\" , \"ing-orange-service\" , \"ing-orange-service-self-control\" , ] ele = expand_shadow_element ( driver , tags ) ele . find_element_by_css_selector ( \"div:nth-child(2) &gt; .card__content &gt; ul &gt; li:nth-child(2) &gt; a\" ) . click () time . sleep ( 5 ) tags = [ \"dba-download-transactions-dialog\" , \"ing-orange-transaction-download-dialog\" , \"ing-uic-dialog-next &gt; section &gt; ing-orange-transaction-download-filter\" , \"ing-uic-form &gt; form &gt; div &gt; div &gt; ing-uic-date-input\" , \"#viewInput\" , ] form = expand_shadow_element ( driver , tags ) inp = form . find_element_by_css_selector ( \"ing-uic-input-container &gt; ing-uic-native-input &gt; input\" ) inp . clear () inp . send_keys ( \"01-01-2010\" ) tags = [ \"dba-download-transactions-dialog\" , \"ing-orange-transaction-download-dialog\" , \"ing-uic-dialog-next &gt; section &gt; ing-orange-transaction-download-filter\" , \"ing-uic-form &gt; form &gt; div &gt; div &gt; ing-uic-date-input[name='endDate']\" , \"#viewInput\" , ] form = expand_shadow_element ( driver , tags ) inp = form . find_element_by_css_selector ( \"ing-uic-input-container &gt; ing-uic-native-input &gt; input\" ) now = datetime . now () - relativedelta ( days = 1 ) inp . clear () inp . send_keys ( \"{}-{}-{}\" . format ( now . day , now . month , now . year )) tags = [ \"dba-download-transactions-dialog\" , \"ing-orange-transaction-download-dialog\" , \"ing-uic-dialog-next &gt; section &gt; ing-orange-transaction-download-filter\" , ] form = expand_shadow_element ( driver , tags ) form . find_element_by_css_selector ( \"ing-uic-form &gt; form &gt; div &gt; ing-uic-select.format-container\" ) . click () time . sleep ( 2 ) form . find_element_by_css_selector ( \"ing-uic-form &gt; form &gt; div &gt; ing-uic-select.format-container &gt; paper-listbox &gt; ing-uic-item[value='CSV']\" ) . click () time . sleep ( 2 ) form . find_element_by_css_selector ( \"ing-uic-form &gt; form &gt; paper-button[data-type='submit']\" ) . click () time . sleep ( 15 ) driver . close () Functions expand_shadow_element def expand_shadow_element ( base , tags ) View Source def expand_shadow_element(base, tags): for tag in tags: print(tag) element = base.find_element_by_css_selector(tag) base = driver.execute_script('return arguments[0].shadowRoot', element) return base","title":"Mijn Ing Download"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing_download/#module-nostalgiasourcesing_bankingmijn_ing_download","text":"View Source from selenium import webdriver import time from datetime import datetime from dateutil.relativedelta import relativedelta def expand_shadow_element ( base , tags ): for tag in tags : print ( tag ) element = base . find_element_by_css_selector ( tag ) base = driver . execute_script ( 'return arguments[0].shadowRoot' , element ) return base if __name__ == \"__main__\" : chrome_options = webdriver . ChromeOptions () chrome_options . binary_location = \"/usr/bin/google-chrome-stable\" driver = webdriver . Chrome ( executable_path = \"/usr/bin/chromedriver\" , options = chrome_options ) url = \"https://mijn.ing.nl/banking/service\" driver . get ( url ) while driver . find_elements_by_xpath ( \"/html/body/ing-app-authentication\" ): time . sleep ( 1 ) time . sleep ( 5 ) tags = [ \"dba-app\" , \"ing-app-daily-banking-service\" , \"ing-orange-service\" , \"ing-orange-service-self-control\" , ] ele = expand_shadow_element ( driver , tags ) ele . find_element_by_css_selector ( \"div:nth-child(2) &gt; .card__content &gt; ul &gt; li:nth-child(2) &gt; a\" ) . click () time . sleep ( 5 ) tags = [ \"dba-download-transactions-dialog\" , \"ing-orange-transaction-download-dialog\" , \"ing-uic-dialog-next &gt; section &gt; ing-orange-transaction-download-filter\" , \"ing-uic-form &gt; form &gt; div &gt; div &gt; ing-uic-date-input\" , \"#viewInput\" , ] form = expand_shadow_element ( driver , tags ) inp = form . find_element_by_css_selector ( \"ing-uic-input-container &gt; ing-uic-native-input &gt; input\" ) inp . clear () inp . send_keys ( \"01-01-2010\" ) tags = [ \"dba-download-transactions-dialog\" , \"ing-orange-transaction-download-dialog\" , \"ing-uic-dialog-next &gt; section &gt; ing-orange-transaction-download-filter\" , \"ing-uic-form &gt; form &gt; div &gt; div &gt; ing-uic-date-input[name='endDate']\" , \"#viewInput\" , ] form = expand_shadow_element ( driver , tags ) inp = form . find_element_by_css_selector ( \"ing-uic-input-container &gt; ing-uic-native-input &gt; input\" ) now = datetime . now () - relativedelta ( days = 1 ) inp . clear () inp . send_keys ( \"{}-{}-{}\" . format ( now . day , now . month , now . year )) tags = [ \"dba-download-transactions-dialog\" , \"ing-orange-transaction-download-dialog\" , \"ing-uic-dialog-next &gt; section &gt; ing-orange-transaction-download-filter\" , ] form = expand_shadow_element ( driver , tags ) form . find_element_by_css_selector ( \"ing-uic-form &gt; form &gt; div &gt; ing-uic-select.format-container\" ) . click () time . sleep ( 2 ) form . find_element_by_css_selector ( \"ing-uic-form &gt; form &gt; div &gt; ing-uic-select.format-container &gt; paper-listbox &gt; ing-uic-item[value='CSV']\" ) . click () time . sleep ( 2 ) form . find_element_by_css_selector ( \"ing-uic-form &gt; form &gt; paper-button[data-type='submit']\" ) . click () time . sleep ( 15 ) driver . close ()","title":"Module nostalgia.sources.ing_banking.mijn_ing_download"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing_download/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/ing_banking/mijn_ing_download/#expand_shadow_element","text":"def expand_shadow_element ( base , tags ) View Source def expand_shadow_element(base, tags): for tag in tags: print(tag) element = base.find_element_by_css_selector(tag) base = driver.execute_script('return arguments[0].shadowRoot', element) return base","title":"expand_shadow_element"},{"location":"reference/nostalgia/sources/samsung/","text":"Module nostalgia.sources.samsung View Source from nostalgia.ndf import NDF class Samsung ( NDF ): vendor = \"samsung\" ingest_settings = { \"ingest_glob\" : \"~/Downloads/samsunghealth_*.zip\" , \"recent_only\" : True , \"delete_existing\" : True , } Sub-modules nostalgia.sources.samsung.heartrate nostalgia.sources.samsung.sleep nostalgia.sources.samsung.stress Classes Samsung class Samsung ( data ) View Source class Samsung ( NDF ) : vendor = \"samsung\" ingest_settings = { \"ingest_glob\" : \"~/Downloads/samsunghealth_*.zip\" , \"recent_only\" : True , \"delete_existing\" : True , } Ancestors (in MRO) nostalgia.ndf.NDF Descendants nostalgia.sources.samsung.heartrate.SamsungHeartrate nostalgia.sources.samsung.sleep.SamsungSleep nostalgia.sources.samsung.stress.SamsungStress Class variables ingest_settings keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Index"},{"location":"reference/nostalgia/sources/samsung/#module-nostalgiasourcessamsung","text":"View Source from nostalgia.ndf import NDF class Samsung ( NDF ): vendor = \"samsung\" ingest_settings = { \"ingest_glob\" : \"~/Downloads/samsunghealth_*.zip\" , \"recent_only\" : True , \"delete_existing\" : True , }","title":"Module nostalgia.sources.samsung"},{"location":"reference/nostalgia/sources/samsung/#sub-modules","text":"nostalgia.sources.samsung.heartrate nostalgia.sources.samsung.sleep nostalgia.sources.samsung.stress","title":"Sub-modules"},{"location":"reference/nostalgia/sources/samsung/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/samsung/#samsung","text":"class Samsung ( data ) View Source class Samsung ( NDF ) : vendor = \"samsung\" ingest_settings = { \"ingest_glob\" : \"~/Downloads/samsunghealth_*.zip\" , \"recent_only\" : True , \"delete_existing\" : True , }","title":"Samsung"},{"location":"reference/nostalgia/sources/samsung/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/samsung/#descendants","text":"nostalgia.sources.samsung.heartrate.SamsungHeartrate nostalgia.sources.samsung.sleep.SamsungSleep nostalgia.sources.samsung.stress.SamsungStress","title":"Descendants"},{"location":"reference/nostalgia/sources/samsung/#class-variables","text":"ingest_settings keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/samsung/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/samsung/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/samsung/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/samsung/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/samsung/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/samsung/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/samsung/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/samsung/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/samsung/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/samsung/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/samsung/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/samsung/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/samsung/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/samsung/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/samsung/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/samsung/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/samsung/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/samsung/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/samsung/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/samsung/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/samsung/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/samsung/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/samsung/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/samsung/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/samsung/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/samsung/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/samsung/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/samsung/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/samsung/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/samsung/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/samsung/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/samsung/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/samsung/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/samsung/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/samsung/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/samsung/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/samsung/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/samsung/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/samsung/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/samsung/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/samsung/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/samsung/heartrate/","text":"Module nostalgia.sources.samsung.heartrate View Source import re import os import pandas as pd import just from nostalgia.times import datetime_from_timestamp , tz from nostalgia.ndf import NDF from nostalgia.sources.samsung import Samsung class SamsungHeartrate ( Samsung , NDF ): @classmethod def handle_dataframe_per_file ( cls , df , fname ): if df . empty : return None df [ \"start\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . start_time ] df [ \"end\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . end_time ] del df [ \"start_time\" ] del df [ \"end_time\" ] return df @classmethod def load ( cls , nrows = None , ** kwargs ): file_glob = \"~/nostalgia_data/input/samsung/samsunghealth_*/jsons/com.samsung.shealth.tracker.heart_rate\" file_glob += \"/*.com.samsung.health.heart_rate.binning_data.json\" # TODO FIX heartrate = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = heartrate [ \"start\" ] end = heartrate [ \"end\" ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) heartrate = pd . DataFrame ( heartrate ) heartrate = heartrate . set_index ( interval_index ) . sort_index () heartrate [ \"end\" ] = heartrate . index . right - pd . Timedelta ( seconds = 1 ) # heartrate = heartrate[heartrate.time != heartrate.end] # heartrate = heartrate[heartrate.time &lt;= heartrate.end] # if nrows is not None: # heartrate = heartrate.iloc[:nrows] heartrate [ \"value\" ] = heartrate . heart_rate del heartrate [ \"heart_rate\" ] return cls ( heartrate ) Classes SamsungHeartrate class SamsungHeartrate ( data ) View Source class SamsungHeartrate ( Samsung , NDF ) : @classmethod def handle_dataframe_per_file ( cls , df , fname ) : if df.empty : return None df [ \"start\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . start_time ] df [ \"end\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . end_time ] del df [ \"start_time\" ] del df [ \"end_time\" ] return df @classmethod def load ( cls , nrows = None , ** kwargs ) : file_glob = \"~/nostalgia_data/input/samsung/samsunghealth_*/jsons/com.samsung.shealth.tracker.heart_rate\" file_glob += \"/*.com.samsung.health.heart_rate.binning_data.json\" # TODO FIX heartrate = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = heartrate [ \"start\" ] end = heartrate [ \"end\" ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) heartrate = pd . DataFrame ( heartrate ) heartrate = heartrate . set_index ( interval_index ). sort_index () heartrate [ \"end\" ] = heartrate . index . right - pd . Timedelta ( seconds = 1 ) # heartrate = heartrate [ heartrate . time != heartrate . end ] # heartrate = heartrate [ heartrate . time & lt ; = heartrate . end ] # if nrows is not None : # heartrate = heartrate . iloc [ : nrows ] heartrate [ \"value\" ] = heartrate . heart_rate del heartrate [ \"heart_rate\" ] return cls ( heartrate ) Ancestors (in MRO) nostalgia.sources.samsung.Samsung nostalgia.ndf.NDF Class variables ingest_settings keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} handle_dataframe_per_file def handle_dataframe_per_file ( df , fname ) View Source @classmethod def handle_dataframe_per_file(cls, df, fname): if df.empty: return None df[\"start\"] = [ datetime_from_timestamp(x) if isinstance(x, int) else tz.localize(x) for x in df.start_time ] df[\"end\"] = [ datetime_from_timestamp(x) if isinstance(x, int) else tz.localize(x) for x in df.end_time ] del df[\"start_time\"] del df[\"end_time\"] return df ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None , ** kwargs ) View Source @classmethod def load(cls, nrows=None, **kwargs): file_glob = \"~/nostalgia_data/input/samsung/samsunghealth_*/jsons/com.samsung.shealth.tracker.heart_rate\" file_glob += \"/*.com.samsung.health.heart_rate.binning_data.json\" # TODO FIX heartrate = cls.load_dataframe_per_json_file(file_glob, nrows=nrows) start = heartrate[\"start\"] end = heartrate[\"end\"] interval_index = pd.IntervalIndex.from_arrays(start, end) heartrate = pd.DataFrame(heartrate) heartrate = heartrate.set_index(interval_index).sort_index() heartrate[\"end\"] = heartrate.index.right - pd.Timedelta(seconds=1) # heartrate = heartrate[heartrate.time != heartrate.end] # heartrate = heartrate[heartrate.time &lt;= heartrate.end] # if nrows is not None: # heartrate = heartrate.iloc[:nrows] heartrate[\"value\"] = heartrate.heart_rate del heartrate[\"heart_rate\"] return cls(heartrate) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Heartrate"},{"location":"reference/nostalgia/sources/samsung/heartrate/#module-nostalgiasourcessamsungheartrate","text":"View Source import re import os import pandas as pd import just from nostalgia.times import datetime_from_timestamp , tz from nostalgia.ndf import NDF from nostalgia.sources.samsung import Samsung class SamsungHeartrate ( Samsung , NDF ): @classmethod def handle_dataframe_per_file ( cls , df , fname ): if df . empty : return None df [ \"start\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . start_time ] df [ \"end\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . end_time ] del df [ \"start_time\" ] del df [ \"end_time\" ] return df @classmethod def load ( cls , nrows = None , ** kwargs ): file_glob = \"~/nostalgia_data/input/samsung/samsunghealth_*/jsons/com.samsung.shealth.tracker.heart_rate\" file_glob += \"/*.com.samsung.health.heart_rate.binning_data.json\" # TODO FIX heartrate = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = heartrate [ \"start\" ] end = heartrate [ \"end\" ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) heartrate = pd . DataFrame ( heartrate ) heartrate = heartrate . set_index ( interval_index ) . sort_index () heartrate [ \"end\" ] = heartrate . index . right - pd . Timedelta ( seconds = 1 ) # heartrate = heartrate[heartrate.time != heartrate.end] # heartrate = heartrate[heartrate.time &lt;= heartrate.end] # if nrows is not None: # heartrate = heartrate.iloc[:nrows] heartrate [ \"value\" ] = heartrate . heart_rate del heartrate [ \"heart_rate\" ] return cls ( heartrate )","title":"Module nostalgia.sources.samsung.heartrate"},{"location":"reference/nostalgia/sources/samsung/heartrate/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/samsung/heartrate/#samsungheartrate","text":"class SamsungHeartrate ( data ) View Source class SamsungHeartrate ( Samsung , NDF ) : @classmethod def handle_dataframe_per_file ( cls , df , fname ) : if df.empty : return None df [ \"start\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . start_time ] df [ \"end\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . end_time ] del df [ \"start_time\" ] del df [ \"end_time\" ] return df @classmethod def load ( cls , nrows = None , ** kwargs ) : file_glob = \"~/nostalgia_data/input/samsung/samsunghealth_*/jsons/com.samsung.shealth.tracker.heart_rate\" file_glob += \"/*.com.samsung.health.heart_rate.binning_data.json\" # TODO FIX heartrate = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = heartrate [ \"start\" ] end = heartrate [ \"end\" ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) heartrate = pd . DataFrame ( heartrate ) heartrate = heartrate . set_index ( interval_index ). sort_index () heartrate [ \"end\" ] = heartrate . index . right - pd . Timedelta ( seconds = 1 ) # heartrate = heartrate [ heartrate . time != heartrate . end ] # heartrate = heartrate [ heartrate . time & lt ; = heartrate . end ] # if nrows is not None : # heartrate = heartrate . iloc [ : nrows ] heartrate [ \"value\" ] = heartrate . heart_rate del heartrate [ \"heart_rate\" ] return cls ( heartrate )","title":"SamsungHeartrate"},{"location":"reference/nostalgia/sources/samsung/heartrate/#ancestors-in-mro","text":"nostalgia.sources.samsung.Samsung nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/samsung/heartrate/#class-variables","text":"ingest_settings keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/samsung/heartrate/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/samsung/heartrate/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/samsung/heartrate/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/samsung/heartrate/#handle_dataframe_per_file","text":"def handle_dataframe_per_file ( df , fname ) View Source @classmethod def handle_dataframe_per_file(cls, df, fname): if df.empty: return None df[\"start\"] = [ datetime_from_timestamp(x) if isinstance(x, int) else tz.localize(x) for x in df.start_time ] df[\"end\"] = [ datetime_from_timestamp(x) if isinstance(x, int) else tz.localize(x) for x in df.end_time ] del df[\"start_time\"] del df[\"end_time\"] return df","title":"handle_dataframe_per_file"},{"location":"reference/nostalgia/sources/samsung/heartrate/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/samsung/heartrate/#load","text":"def load ( nrows = None , ** kwargs ) View Source @classmethod def load(cls, nrows=None, **kwargs): file_glob = \"~/nostalgia_data/input/samsung/samsunghealth_*/jsons/com.samsung.shealth.tracker.heart_rate\" file_glob += \"/*.com.samsung.health.heart_rate.binning_data.json\" # TODO FIX heartrate = cls.load_dataframe_per_json_file(file_glob, nrows=nrows) start = heartrate[\"start\"] end = heartrate[\"end\"] interval_index = pd.IntervalIndex.from_arrays(start, end) heartrate = pd.DataFrame(heartrate) heartrate = heartrate.set_index(interval_index).sort_index() heartrate[\"end\"] = heartrate.index.right - pd.Timedelta(seconds=1) # heartrate = heartrate[heartrate.time != heartrate.end] # heartrate = heartrate[heartrate.time &lt;= heartrate.end] # if nrows is not None: # heartrate = heartrate.iloc[:nrows] heartrate[\"value\"] = heartrate.heart_rate del heartrate[\"heart_rate\"] return cls(heartrate)","title":"load"},{"location":"reference/nostalgia/sources/samsung/heartrate/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/samsung/heartrate/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/samsung/heartrate/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/samsung/heartrate/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/samsung/heartrate/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/samsung/heartrate/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/samsung/heartrate/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/samsung/heartrate/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/samsung/heartrate/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/samsung/heartrate/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/samsung/heartrate/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/samsung/heartrate/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/samsung/heartrate/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/samsung/heartrate/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/samsung/heartrate/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/samsung/heartrate/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/samsung/heartrate/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/samsung/heartrate/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/samsung/heartrate/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/samsung/heartrate/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/samsung/heartrate/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/samsung/heartrate/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/samsung/heartrate/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/samsung/heartrate/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/samsung/heartrate/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/samsung/heartrate/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/samsung/heartrate/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/samsung/heartrate/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/samsung/heartrate/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/samsung/heartrate/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/samsung/heartrate/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/samsung/heartrate/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/samsung/heartrate/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/samsung/heartrate/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/samsung/heartrate/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/samsung/heartrate/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/samsung/heartrate/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/samsung/sleep/","text":"Module nostalgia.sources.samsung.sleep View Source import just from nostalgia.times import datetime_from_format from nostalgia.sources.samsung import Samsung class SamsungSleep ( Samsung ): @classmethod def handle_dataframe_per_file ( cls , data , fname ): data = data [[ \"start_time\" , \"end_time\" , \"stage\" ]] encoding = { 40001 : \"awake\" , 40002 : \"light\" , 40003 : \"deep\" , 40004 : \"rem\" } data [ \"stage\" ] = data . stage . replace ( encoding ) return data @classmethod def load ( cls , nrows = None ): path = \"~/nostalgia_data/input/samsung/samsunghealth_*/com.samsung.health.sleep_stage.*.csv\" fname = just . glob ( path )[ 0 ] data = cls . load_data_file_modified_time ( fname , nrows = nrows , skiprows = 1 ) data [ \"start_time\" ] = [ datetime_from_format ( x , \"%Y-%m- %d %H:%M:%S. %f \" ) for x in data [ \"start_time\" ] ] data [ \"end_time\" ] = [ datetime_from_format ( x , \"%Y-%m- %d %H:%M:%S. %f \" ) for x in data [ \"end_time\" ] ] return cls ( data ) @property def asleep ( self ): return self . query ( \"stage != 'awake'\" ) Classes SamsungSleep class SamsungSleep ( data ) View Source class SamsungSleep ( Samsung ) : @classmethod def handle_dataframe_per_file ( cls , data , fname ) : data = data [[ \"start_time\" , \"end_time\" , \"stage\" ]] encoding = { 40001 : \"awake\" , 40002 : \"light\" , 40003 : \"deep\" , 40004 : \"rem\" } data [ \"stage\" ] = data . stage . replace ( encoding ) return data @classmethod def load ( cls , nrows = None ) : path = \"~/nostalgia_data/input/samsung/samsunghealth_*/com.samsung.health.sleep_stage.*.csv\" fname = just . glob ( path )[ 0 ] data = cls . load_data_file_modified_time ( fname , nrows = nrows , skiprows = 1 ) data [ \"start_time\" ] = [ datetime_from_format ( x , \"%Y-%m-%d %H:%M:%S.%f\" ) for x in data [ \"start_time\" ] ] data [ \"end_time\" ] = [ datetime_from_format ( x , \"%Y-%m-%d %H:%M:%S.%f\" ) for x in data [ \"end_time\" ] ] return cls ( data ) @property def asleep ( self ) : return self . query ( \"stage != 'awake'\" ) Ancestors (in MRO) nostalgia.sources.samsung.Samsung nostalgia.ndf.NDF Class variables ingest_settings keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} handle_dataframe_per_file def handle_dataframe_per_file ( data , fname ) View Source @classmethod def handle_dataframe_per_file(cls, data, fname): data = data[[\"start_time\", \"end_time\", \"stage\"]] encoding = {40001: \"awake\", 40002: \"light\", 40003: \"deep\", 40004: \"rem\"} data[\"stage\"] = data.stage.replace(encoding) return data ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): path = \"~/nostalgia_data/input/samsung/samsunghealth_*/com.samsung.health.sleep_stage.*.csv\" fname = just.glob(path)[0] data = cls.load_data_file_modified_time(fname, nrows=nrows, skiprows=1) data[\"start_time\"] = [ datetime_from_format(x, \"%Y-%m-%d %H:%M:%S.%f\") for x in data[\"start_time\"] ] data[\"end_time\"] = [ datetime_from_format(x, \"%Y-%m-%d %H:%M:%S.%f\") for x in data[\"end_time\"] ] return cls(data) Instance variables asleep at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Sleep"},{"location":"reference/nostalgia/sources/samsung/sleep/#module-nostalgiasourcessamsungsleep","text":"View Source import just from nostalgia.times import datetime_from_format from nostalgia.sources.samsung import Samsung class SamsungSleep ( Samsung ): @classmethod def handle_dataframe_per_file ( cls , data , fname ): data = data [[ \"start_time\" , \"end_time\" , \"stage\" ]] encoding = { 40001 : \"awake\" , 40002 : \"light\" , 40003 : \"deep\" , 40004 : \"rem\" } data [ \"stage\" ] = data . stage . replace ( encoding ) return data @classmethod def load ( cls , nrows = None ): path = \"~/nostalgia_data/input/samsung/samsunghealth_*/com.samsung.health.sleep_stage.*.csv\" fname = just . glob ( path )[ 0 ] data = cls . load_data_file_modified_time ( fname , nrows = nrows , skiprows = 1 ) data [ \"start_time\" ] = [ datetime_from_format ( x , \"%Y-%m- %d %H:%M:%S. %f \" ) for x in data [ \"start_time\" ] ] data [ \"end_time\" ] = [ datetime_from_format ( x , \"%Y-%m- %d %H:%M:%S. %f \" ) for x in data [ \"end_time\" ] ] return cls ( data ) @property def asleep ( self ): return self . query ( \"stage != 'awake'\" )","title":"Module nostalgia.sources.samsung.sleep"},{"location":"reference/nostalgia/sources/samsung/sleep/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/samsung/sleep/#samsungsleep","text":"class SamsungSleep ( data ) View Source class SamsungSleep ( Samsung ) : @classmethod def handle_dataframe_per_file ( cls , data , fname ) : data = data [[ \"start_time\" , \"end_time\" , \"stage\" ]] encoding = { 40001 : \"awake\" , 40002 : \"light\" , 40003 : \"deep\" , 40004 : \"rem\" } data [ \"stage\" ] = data . stage . replace ( encoding ) return data @classmethod def load ( cls , nrows = None ) : path = \"~/nostalgia_data/input/samsung/samsunghealth_*/com.samsung.health.sleep_stage.*.csv\" fname = just . glob ( path )[ 0 ] data = cls . load_data_file_modified_time ( fname , nrows = nrows , skiprows = 1 ) data [ \"start_time\" ] = [ datetime_from_format ( x , \"%Y-%m-%d %H:%M:%S.%f\" ) for x in data [ \"start_time\" ] ] data [ \"end_time\" ] = [ datetime_from_format ( x , \"%Y-%m-%d %H:%M:%S.%f\" ) for x in data [ \"end_time\" ] ] return cls ( data ) @property def asleep ( self ) : return self . query ( \"stage != 'awake'\" )","title":"SamsungSleep"},{"location":"reference/nostalgia/sources/samsung/sleep/#ancestors-in-mro","text":"nostalgia.sources.samsung.Samsung nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/samsung/sleep/#class-variables","text":"ingest_settings keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/samsung/sleep/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/samsung/sleep/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/samsung/sleep/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/samsung/sleep/#handle_dataframe_per_file","text":"def handle_dataframe_per_file ( data , fname ) View Source @classmethod def handle_dataframe_per_file(cls, data, fname): data = data[[\"start_time\", \"end_time\", \"stage\"]] encoding = {40001: \"awake\", 40002: \"light\", 40003: \"deep\", 40004: \"rem\"} data[\"stage\"] = data.stage.replace(encoding) return data","title":"handle_dataframe_per_file"},{"location":"reference/nostalgia/sources/samsung/sleep/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/samsung/sleep/#load","text":"def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): path = \"~/nostalgia_data/input/samsung/samsunghealth_*/com.samsung.health.sleep_stage.*.csv\" fname = just.glob(path)[0] data = cls.load_data_file_modified_time(fname, nrows=nrows, skiprows=1) data[\"start_time\"] = [ datetime_from_format(x, \"%Y-%m-%d %H:%M:%S.%f\") for x in data[\"start_time\"] ] data[\"end_time\"] = [ datetime_from_format(x, \"%Y-%m-%d %H:%M:%S.%f\") for x in data[\"end_time\"] ] return cls(data)","title":"load"},{"location":"reference/nostalgia/sources/samsung/sleep/#instance-variables","text":"asleep at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/samsung/sleep/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/samsung/sleep/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/samsung/sleep/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/samsung/sleep/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/samsung/sleep/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/samsung/sleep/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/samsung/sleep/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/samsung/sleep/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/samsung/sleep/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/samsung/sleep/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/samsung/sleep/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/samsung/sleep/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/samsung/sleep/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/samsung/sleep/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/samsung/sleep/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/samsung/sleep/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/samsung/sleep/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/samsung/sleep/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/samsung/sleep/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/samsung/sleep/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/samsung/sleep/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/samsung/sleep/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/samsung/sleep/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/samsung/sleep/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/samsung/sleep/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/samsung/sleep/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/samsung/sleep/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/samsung/sleep/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/samsung/sleep/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/samsung/sleep/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/samsung/sleep/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/samsung/sleep/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/samsung/sleep/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/samsung/sleep/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/samsung/sleep/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/samsung/sleep/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/samsung/stress/","text":"Module nostalgia.sources.samsung.stress View Source import re import os import pandas as pd import just from nostalgia.times import datetime_from_timestamp , tz from nostalgia.ndf import NDF from nostalgia.sources.samsung import Samsung class SamsungStress ( Samsung , NDF ): @classmethod def handle_dataframe_per_file ( cls , df , fname ): if df . empty : return None df [ \"start\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . start_time ] df [ \"end\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . end_time ] del df [ \"start_time\" ] del df [ \"end_time\" ] return df @classmethod def load ( cls , nrows = None , ** kwargs ): file_glob = \"~/nostalgia_data/input/samsung/samsunghealth_*/jsons/com.samsung.shealth.stress/*.binning_data.json\" print ( file_glob ) # TODO FIX stress = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = stress [ \"start\" ] end = stress [ \"end\" ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) stress = pd . DataFrame ( stress ) stress = stress . set_index ( interval_index ) . sort_index () stress [ \"end\" ] = stress . index . right - pd . Timedelta ( seconds = 1 ) # if nrows is not None: # stress = stress.iloc[:nrows] stress [ \"value\" ] = stress . score del stress [ \"score\" ] return cls ( stress ) Classes SamsungStress class SamsungStress ( data ) View Source class SamsungStress ( Samsung , NDF ) : @classmethod def handle_dataframe_per_file ( cls , df , fname ) : if df.empty : return None df [ \"start\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . start_time ] df [ \"end\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . end_time ] del df [ \"start_time\" ] del df [ \"end_time\" ] return df @classmethod def load ( cls , nrows = None , ** kwargs ) : file_glob = \"~/nostalgia_data/input/samsung/samsunghealth_*/jsons/com.samsung.shealth.stress/*.binning_data.json\" print ( file_glob ) # TODO FIX stress = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = stress [ \"start\" ] end = stress [ \"end\" ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) stress = pd . DataFrame ( stress ) stress = stress . set_index ( interval_index ). sort_index () stress [ \"end\" ] = stress . index . right - pd . Timedelta ( seconds = 1 ) # if nrows is not None : # stress = stress . iloc [ : nrows ] stress [ \"value\" ] = stress . score del stress [ \"score\" ] return cls ( stress ) Ancestors (in MRO) nostalgia.sources.samsung.Samsung nostalgia.ndf.NDF Class variables ingest_settings keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} handle_dataframe_per_file def handle_dataframe_per_file ( df , fname ) View Source @classmethod def handle_dataframe_per_file(cls, df, fname): if df.empty: return None df[\"start\"] = [ datetime_from_timestamp(x) if isinstance(x, int) else tz.localize(x) for x in df.start_time ] df[\"end\"] = [ datetime_from_timestamp(x) if isinstance(x, int) else tz.localize(x) for x in df.end_time ] del df[\"start_time\"] del df[\"end_time\"] return df ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None , ** kwargs ) View Source @classmethod def load(cls, nrows=None, **kwargs): file_glob = \"~/nostalgia_data/input/samsung/samsunghealth_*/jsons/com.samsung.shealth.stress/*.binning_data.json\" print(file_glob) # TODO FIX stress = cls.load_dataframe_per_json_file(file_glob, nrows=nrows) start = stress[\"start\"] end = stress[\"end\"] interval_index = pd.IntervalIndex.from_arrays(start, end) stress = pd.DataFrame(stress) stress = stress.set_index(interval_index).sort_index() stress[\"end\"] = stress.index.right - pd.Timedelta(seconds=1) # if nrows is not None: # stress = stress.iloc[:nrows] stress[\"value\"] = stress.score del stress[\"score\"] return cls(stress) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Stress"},{"location":"reference/nostalgia/sources/samsung/stress/#module-nostalgiasourcessamsungstress","text":"View Source import re import os import pandas as pd import just from nostalgia.times import datetime_from_timestamp , tz from nostalgia.ndf import NDF from nostalgia.sources.samsung import Samsung class SamsungStress ( Samsung , NDF ): @classmethod def handle_dataframe_per_file ( cls , df , fname ): if df . empty : return None df [ \"start\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . start_time ] df [ \"end\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . end_time ] del df [ \"start_time\" ] del df [ \"end_time\" ] return df @classmethod def load ( cls , nrows = None , ** kwargs ): file_glob = \"~/nostalgia_data/input/samsung/samsunghealth_*/jsons/com.samsung.shealth.stress/*.binning_data.json\" print ( file_glob ) # TODO FIX stress = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = stress [ \"start\" ] end = stress [ \"end\" ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) stress = pd . DataFrame ( stress ) stress = stress . set_index ( interval_index ) . sort_index () stress [ \"end\" ] = stress . index . right - pd . Timedelta ( seconds = 1 ) # if nrows is not None: # stress = stress.iloc[:nrows] stress [ \"value\" ] = stress . score del stress [ \"score\" ] return cls ( stress )","title":"Module nostalgia.sources.samsung.stress"},{"location":"reference/nostalgia/sources/samsung/stress/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/samsung/stress/#samsungstress","text":"class SamsungStress ( data ) View Source class SamsungStress ( Samsung , NDF ) : @classmethod def handle_dataframe_per_file ( cls , df , fname ) : if df.empty : return None df [ \"start\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . start_time ] df [ \"end\" ] = [ datetime_from_timestamp ( x ) if isinstance ( x , int ) else tz . localize ( x ) for x in df . end_time ] del df [ \"start_time\" ] del df [ \"end_time\" ] return df @classmethod def load ( cls , nrows = None , ** kwargs ) : file_glob = \"~/nostalgia_data/input/samsung/samsunghealth_*/jsons/com.samsung.shealth.stress/*.binning_data.json\" print ( file_glob ) # TODO FIX stress = cls . load_dataframe_per_json_file ( file_glob , nrows = nrows ) start = stress [ \"start\" ] end = stress [ \"end\" ] interval_index = pd . IntervalIndex . from_arrays ( start , end ) stress = pd . DataFrame ( stress ) stress = stress . set_index ( interval_index ). sort_index () stress [ \"end\" ] = stress . index . right - pd . Timedelta ( seconds = 1 ) # if nrows is not None : # stress = stress . iloc [ : nrows ] stress [ \"value\" ] = stress . score del stress [ \"score\" ] return cls ( stress )","title":"SamsungStress"},{"location":"reference/nostalgia/sources/samsung/stress/#ancestors-in-mro","text":"nostalgia.sources.samsung.Samsung nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/samsung/stress/#class-variables","text":"ingest_settings keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/samsung/stress/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/samsung/stress/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/samsung/stress/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/samsung/stress/#handle_dataframe_per_file","text":"def handle_dataframe_per_file ( df , fname ) View Source @classmethod def handle_dataframe_per_file(cls, df, fname): if df.empty: return None df[\"start\"] = [ datetime_from_timestamp(x) if isinstance(x, int) else tz.localize(x) for x in df.start_time ] df[\"end\"] = [ datetime_from_timestamp(x) if isinstance(x, int) else tz.localize(x) for x in df.end_time ] del df[\"start_time\"] del df[\"end_time\"] return df","title":"handle_dataframe_per_file"},{"location":"reference/nostalgia/sources/samsung/stress/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/samsung/stress/#load","text":"def load ( nrows = None , ** kwargs ) View Source @classmethod def load(cls, nrows=None, **kwargs): file_glob = \"~/nostalgia_data/input/samsung/samsunghealth_*/jsons/com.samsung.shealth.stress/*.binning_data.json\" print(file_glob) # TODO FIX stress = cls.load_dataframe_per_json_file(file_glob, nrows=nrows) start = stress[\"start\"] end = stress[\"end\"] interval_index = pd.IntervalIndex.from_arrays(start, end) stress = pd.DataFrame(stress) stress = stress.set_index(interval_index).sort_index() stress[\"end\"] = stress.index.right - pd.Timedelta(seconds=1) # if nrows is not None: # stress = stress.iloc[:nrows] stress[\"value\"] = stress.score del stress[\"score\"] return cls(stress)","title":"load"},{"location":"reference/nostalgia/sources/samsung/stress/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/samsung/stress/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/samsung/stress/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/samsung/stress/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/samsung/stress/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/samsung/stress/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/samsung/stress/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/samsung/stress/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/samsung/stress/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/samsung/stress/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/samsung/stress/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/samsung/stress/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/samsung/stress/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/samsung/stress/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/samsung/stress/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/samsung/stress/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/samsung/stress/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/samsung/stress/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/samsung/stress/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/samsung/stress/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/samsung/stress/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/samsung/stress/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/samsung/stress/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/samsung/stress/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/samsung/stress/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/samsung/stress/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/samsung/stress/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/samsung/stress/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/samsung/stress/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/samsung/stress/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/samsung/stress/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/samsung/stress/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/samsung/stress/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/samsung/stress/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/samsung/stress/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/samsung/stress/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/samsung/stress/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/web/","text":"Module nostalgia.sources.web Sub-modules nostalgia.sources.web.get_keywords_for_product nostalgia.sources.web.linked_events nostalgia.sources.web.linked_google_search nostalgia.sources.web.linked_offers nostalgia.sources.web.linked_person nostalgia.sources.web.linked_videos nostalgia.sources.web.videos_watched","title":"Index"},{"location":"reference/nostalgia/sources/web/#module-nostalgiasourcesweb","text":"","title":"Module nostalgia.sources.web"},{"location":"reference/nostalgia/sources/web/#sub-modules","text":"nostalgia.sources.web.get_keywords_for_product nostalgia.sources.web.linked_events nostalgia.sources.web.linked_google_search nostalgia.sources.web.linked_offers nostalgia.sources.web.linked_person nostalgia.sources.web.linked_videos nostalgia.sources.web.videos_watched","title":"Sub-modules"},{"location":"reference/nostalgia/sources/web/get_keywords_for_product/","text":"Module nostalgia.sources.web.get_keywords_for_product View Source import re from collections import Counter import just import requests from nearnlp.nearnlp import is_noun , is_verb , singularize import functools from tok import Tokenizer from nltk.corpus import stopwords from nostalgia.enrichers.google.custom_search import google_custom_search ENGLISH_STOP = set ( stopwords . words ( \"english\" )) t = Tokenizer ( True ) t . drop ( \"&lt;b&gt;\" , \"remove html\" ) t . drop ( \"&lt;b/&gt;\" , \"remove html\" ) # can also use qoogle interesting_keys = set () for prefix in [ \"og:\" , \"twitter:\" , \"\" ]: for key in [ \"title\" , \"description\" , \"name\" , 'manufacturer_name' , 'category_name_singular' , 'long_description' , 'snippet' , ]: interesting_keys . add ( prefix + key ) def recurse_json ( json_data , results = None ): if results is None : results = [] if isinstance ( json_data , dict ): for k , v in json_data . items (): if k in interesting_keys and isinstance ( v , str ) and v : results . append ( v ) elif isinstance ( v , ( dict , list )): recurse_json ( v , results ) elif isinstance ( json_data , list ): for x in json_data : recurse_json ( x , results ) return results @functools.lru_cache ( 2000 ) def get_keywords_for_product ( product_string ): # json = {\"searchString\": product_string} # res = requests.get(\"https://icecat.biz/search/rest/get-products-list\", json=json).json() res = google_custom_search ( product_string ) zzz = recurse_json ( res ) c = Counter () for x in zzz : c . update ( set ( [ singularize ( y . lower ()) for y in t . word_tokenize ( x ) if re . search ( \"^[a-z]+$\" , y . lower ()) ] ) ) # print([(k,v) for k,v in c.most_common(1000) if is_noun(k) and is_verb(k)]) return [ x for i , x in enumerate ( c . most_common ( 1000 )) if x [ 0 ] not in ENGLISH_STOP and len ( x [ 0 ]) & gt ; 2 and x [ 1 ] & gt ; 1 and is_noun ( x [ 0 ]) # if a verb would occur often then probably it is still important and ( not is_verb ( x [ 0 ]) or i & lt ; 5 ) ] Variables ENGLISH_STOP interesting_keys key prefix t Functions get_keywords_for_product def get_keywords_for_product ( product_string ) View Source @functools.lru_cache(2000) def get_keywords_for_product(product_string): # json = {\"searchString\": product_string} # res = requests.get(\"https://icecat.biz/search/rest/get-products-list\", json=json).json() res = google_custom_search(product_string) zzz = recurse_json(res) c = Counter() for x in zzz: c.update( set( [ singularize(y.lower()) for y in t.word_tokenize(x) if re.search(\"^[a-z]+$\", y.lower()) ] ) ) # print([(k,v) for k,v in c.most_common(1000) if is_noun(k) and is_verb(k)]) return [ x for i, x in enumerate(c.most_common(1000)) if x[0] not in ENGLISH_STOP and len(x[0]) &gt; 2 and x[1] &gt; 1 and is_noun(x[0]) # if a verb would occur often then probably it is still important and (not is_verb(x[0]) or i &lt; 5) ] recurse_json def recurse_json ( json_data , results = None ) View Source def recurse_json(json_data, results=None): if results is None: results = [] if isinstance(json_data, dict): for k, v in json_data.items(): if k in interesting_keys and isinstance(v, str) and v: results.append(v) elif isinstance(v, (dict, list)): recurse_json(v, results) elif isinstance(json_data, list): for x in json_data: recurse_json(x, results) return results","title":"Get Keywords For Product"},{"location":"reference/nostalgia/sources/web/get_keywords_for_product/#module-nostalgiasourceswebget_keywords_for_product","text":"View Source import re from collections import Counter import just import requests from nearnlp.nearnlp import is_noun , is_verb , singularize import functools from tok import Tokenizer from nltk.corpus import stopwords from nostalgia.enrichers.google.custom_search import google_custom_search ENGLISH_STOP = set ( stopwords . words ( \"english\" )) t = Tokenizer ( True ) t . drop ( \"&lt;b&gt;\" , \"remove html\" ) t . drop ( \"&lt;b/&gt;\" , \"remove html\" ) # can also use qoogle interesting_keys = set () for prefix in [ \"og:\" , \"twitter:\" , \"\" ]: for key in [ \"title\" , \"description\" , \"name\" , 'manufacturer_name' , 'category_name_singular' , 'long_description' , 'snippet' , ]: interesting_keys . add ( prefix + key ) def recurse_json ( json_data , results = None ): if results is None : results = [] if isinstance ( json_data , dict ): for k , v in json_data . items (): if k in interesting_keys and isinstance ( v , str ) and v : results . append ( v ) elif isinstance ( v , ( dict , list )): recurse_json ( v , results ) elif isinstance ( json_data , list ): for x in json_data : recurse_json ( x , results ) return results @functools.lru_cache ( 2000 ) def get_keywords_for_product ( product_string ): # json = {\"searchString\": product_string} # res = requests.get(\"https://icecat.biz/search/rest/get-products-list\", json=json).json() res = google_custom_search ( product_string ) zzz = recurse_json ( res ) c = Counter () for x in zzz : c . update ( set ( [ singularize ( y . lower ()) for y in t . word_tokenize ( x ) if re . search ( \"^[a-z]+$\" , y . lower ()) ] ) ) # print([(k,v) for k,v in c.most_common(1000) if is_noun(k) and is_verb(k)]) return [ x for i , x in enumerate ( c . most_common ( 1000 )) if x [ 0 ] not in ENGLISH_STOP and len ( x [ 0 ]) & gt ; 2 and x [ 1 ] & gt ; 1 and is_noun ( x [ 0 ]) # if a verb would occur often then probably it is still important and ( not is_verb ( x [ 0 ]) or i & lt ; 5 ) ]","title":"Module nostalgia.sources.web.get_keywords_for_product"},{"location":"reference/nostalgia/sources/web/get_keywords_for_product/#variables","text":"ENGLISH_STOP interesting_keys key prefix t","title":"Variables"},{"location":"reference/nostalgia/sources/web/get_keywords_for_product/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/web/get_keywords_for_product/#get_keywords_for_product","text":"def get_keywords_for_product ( product_string ) View Source @functools.lru_cache(2000) def get_keywords_for_product(product_string): # json = {\"searchString\": product_string} # res = requests.get(\"https://icecat.biz/search/rest/get-products-list\", json=json).json() res = google_custom_search(product_string) zzz = recurse_json(res) c = Counter() for x in zzz: c.update( set( [ singularize(y.lower()) for y in t.word_tokenize(x) if re.search(\"^[a-z]+$\", y.lower()) ] ) ) # print([(k,v) for k,v in c.most_common(1000) if is_noun(k) and is_verb(k)]) return [ x for i, x in enumerate(c.most_common(1000)) if x[0] not in ENGLISH_STOP and len(x[0]) &gt; 2 and x[1] &gt; 1 and is_noun(x[0]) # if a verb would occur often then probably it is still important and (not is_verb(x[0]) or i &lt; 5) ]","title":"get_keywords_for_product"},{"location":"reference/nostalgia/sources/web/get_keywords_for_product/#recurse_json","text":"def recurse_json ( json_data , results = None ) View Source def recurse_json(json_data, results=None): if results is None: results = [] if isinstance(json_data, dict): for k, v in json_data.items(): if k in interesting_keys and isinstance(v, str) and v: results.append(v) elif isinstance(v, (dict, list)): recurse_json(v, results) elif isinstance(json_data, list): for x in json_data: recurse_json(x, results) return results","title":"recurse_json"},{"location":"reference/nostalgia/sources/web/linked_events/","text":"Module nostalgia.sources.web.linked_events View Source import json from datetime import datetime import just import pandas as pd from nostalgia.times import tz from auto_extract import parse_article from nostalgia.cache import get_cache from nostalgia.ndf import NDF from nostalgia.utils import normalize_name CACHE = get_cache ( \"linked_events\" ) def getter ( dc , key , default = None ): res = dc . get ( key , default ) if isinstance ( res , list ): res = res [ 0 ] elif isinstance ( res , dict ): res = json . dumps ( res ) return res def get_linked_data_jd ( art ): data = None try : jdata = art . jsonld except json . JSONDecodeError : return None for y in jdata : if not y : continue if isinstance ( y , list ): y = y [ 0 ] if y . get ( \"@type\" ) != \"Event\" : continue return { 'description' : getter ( y , \"description\" ), 'startDate' : getter ( y , \"startDate\" ), 'endDate' : getter ( y , \"endDate\" ), 'location' : getter ( y , \"location\" ), \"source\" : \"jsonld\" , } def get_linked_data_md ( art ): data = None for y in art . microdata : props = y . get ( \"properties\" ) if props is None : continue tp = str ( y . get ( \"type\" , \"\" )) if not tp . endswith ( \"/Event\" ): continue return { \"startDate\" : getter ( props , \"startDate\" ), \"endDate\" : getter ( props , \"endDate\" ), \"description\" : getter ( props , \"description\" ), \"name\" : \"\" . join ( getter ( props , \"name\" , \"\" )), \"location\" : getter ( props , \"location\" ), \"source\" : \"md\" , } def get_linked_data ( x ): path = x [ \"path\" ] if path in CACHE : return CACHE [ path ] try : html = just . read ( path ) except EOFError : CACHE [ path ] = None return None if not html . strip (): CACHE [ path ] = None return None art = parse_article ( html , x [ \"url\" ]) linked_data = get_linked_data_md ( art ) if linked_data is None : linked_data = get_linked_data_jd ( art ) CACHE [ path ] = linked_data return linked_data class Events ( NDF ): vendor = \"web\" @classmethod def object_to_row ( cls , obj ): row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ): events = cls . load_object_per_newline ( file_path , nrows ) return cls ( events ) Variables CACHE Functions get_linked_data def get_linked_data ( x ) View Source def get_linked_data(x): path = x[\"path\"] if path in CACHE: return CACHE[path] try: html = just.read(path) except EOFError: CACHE[path] = None return None if not html.strip(): CACHE[path] = None return None art = parse_article(html, x[\"url\"]) linked_data = get_linked_data_md(art) if linked_data is None: linked_data = get_linked_data_jd(art) CACHE[path] = linked_data return linked_data get_linked_data_jd def get_linked_data_jd ( art ) View Source def get_linked_data_jd(art): data = None try: jdata = art.jsonld except json.JSONDecodeError: return None for y in jdata: if not y: continue if isinstance(y, list): y = y[0] if y.get(\"@type\") != \"Event\": continue return { 'description': getter(y, \"description\"), 'startDate': getter(y, \"startDate\"), 'endDate': getter(y, \"endDate\"), 'location': getter(y, \"location\"), \"source\": \"jsonld\", } get_linked_data_md def get_linked_data_md ( art ) View Source def get_linked_data_md(art): data = None for y in art.microdata: props = y.get(\"properties\") if props is None: continue tp = str(y.get(\"type\", \"\")) if not tp.endswith(\"/Event\"): continue return { \"startDate\": getter(props, \"startDate\"), \"endDate\": getter(props, \"endDate\"), \"description\": getter(props, \"description\"), \"name\": \"\".join(getter(props, \"name\", \"\")), \"location\": getter(props, \"location\"), \"source\": \"md\", } getter def getter ( dc , key , default = None ) View Source def getter(dc, key, default=None): res = dc.get(key, default) if isinstance(res, list): res = res[0] elif isinstance(res, dict): res = json.dumps(res) return res Classes Events class Events ( data ) View Source class Events ( NDF ) : vendor = \"web\" @classmethod def object_to_row ( cls , obj ) : row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ) : events = cls . load_object_per_newline ( file_path , nrows ) return cls ( events ) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( file_path = '~/nostalgia_data/meta.jsonl' , nrows = None , ** kwargs ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/meta.jsonl\", nrows=None, **kwargs): events = cls.load_object_per_newline(file_path, nrows) return cls(events) object_to_row def object_to_row ( obj ) View Source @classmethod def object_to_row(cls, obj): row = get_linked_data(obj) if row is not None: row[\"time\"] = datetime.fromtimestamp(float(obj[\"time\"]), tz=tz) row[\"url\"] = obj[\"url\"] row[\"path\"] = obj[\"path\"] row[\"keywords\"] = \"\" return row Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Linked Events"},{"location":"reference/nostalgia/sources/web/linked_events/#module-nostalgiasourcesweblinked_events","text":"View Source import json from datetime import datetime import just import pandas as pd from nostalgia.times import tz from auto_extract import parse_article from nostalgia.cache import get_cache from nostalgia.ndf import NDF from nostalgia.utils import normalize_name CACHE = get_cache ( \"linked_events\" ) def getter ( dc , key , default = None ): res = dc . get ( key , default ) if isinstance ( res , list ): res = res [ 0 ] elif isinstance ( res , dict ): res = json . dumps ( res ) return res def get_linked_data_jd ( art ): data = None try : jdata = art . jsonld except json . JSONDecodeError : return None for y in jdata : if not y : continue if isinstance ( y , list ): y = y [ 0 ] if y . get ( \"@type\" ) != \"Event\" : continue return { 'description' : getter ( y , \"description\" ), 'startDate' : getter ( y , \"startDate\" ), 'endDate' : getter ( y , \"endDate\" ), 'location' : getter ( y , \"location\" ), \"source\" : \"jsonld\" , } def get_linked_data_md ( art ): data = None for y in art . microdata : props = y . get ( \"properties\" ) if props is None : continue tp = str ( y . get ( \"type\" , \"\" )) if not tp . endswith ( \"/Event\" ): continue return { \"startDate\" : getter ( props , \"startDate\" ), \"endDate\" : getter ( props , \"endDate\" ), \"description\" : getter ( props , \"description\" ), \"name\" : \"\" . join ( getter ( props , \"name\" , \"\" )), \"location\" : getter ( props , \"location\" ), \"source\" : \"md\" , } def get_linked_data ( x ): path = x [ \"path\" ] if path in CACHE : return CACHE [ path ] try : html = just . read ( path ) except EOFError : CACHE [ path ] = None return None if not html . strip (): CACHE [ path ] = None return None art = parse_article ( html , x [ \"url\" ]) linked_data = get_linked_data_md ( art ) if linked_data is None : linked_data = get_linked_data_jd ( art ) CACHE [ path ] = linked_data return linked_data class Events ( NDF ): vendor = \"web\" @classmethod def object_to_row ( cls , obj ): row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ): events = cls . load_object_per_newline ( file_path , nrows ) return cls ( events )","title":"Module nostalgia.sources.web.linked_events"},{"location":"reference/nostalgia/sources/web/linked_events/#variables","text":"CACHE","title":"Variables"},{"location":"reference/nostalgia/sources/web/linked_events/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/web/linked_events/#get_linked_data","text":"def get_linked_data ( x ) View Source def get_linked_data(x): path = x[\"path\"] if path in CACHE: return CACHE[path] try: html = just.read(path) except EOFError: CACHE[path] = None return None if not html.strip(): CACHE[path] = None return None art = parse_article(html, x[\"url\"]) linked_data = get_linked_data_md(art) if linked_data is None: linked_data = get_linked_data_jd(art) CACHE[path] = linked_data return linked_data","title":"get_linked_data"},{"location":"reference/nostalgia/sources/web/linked_events/#get_linked_data_jd","text":"def get_linked_data_jd ( art ) View Source def get_linked_data_jd(art): data = None try: jdata = art.jsonld except json.JSONDecodeError: return None for y in jdata: if not y: continue if isinstance(y, list): y = y[0] if y.get(\"@type\") != \"Event\": continue return { 'description': getter(y, \"description\"), 'startDate': getter(y, \"startDate\"), 'endDate': getter(y, \"endDate\"), 'location': getter(y, \"location\"), \"source\": \"jsonld\", }","title":"get_linked_data_jd"},{"location":"reference/nostalgia/sources/web/linked_events/#get_linked_data_md","text":"def get_linked_data_md ( art ) View Source def get_linked_data_md(art): data = None for y in art.microdata: props = y.get(\"properties\") if props is None: continue tp = str(y.get(\"type\", \"\")) if not tp.endswith(\"/Event\"): continue return { \"startDate\": getter(props, \"startDate\"), \"endDate\": getter(props, \"endDate\"), \"description\": getter(props, \"description\"), \"name\": \"\".join(getter(props, \"name\", \"\")), \"location\": getter(props, \"location\"), \"source\": \"md\", }","title":"get_linked_data_md"},{"location":"reference/nostalgia/sources/web/linked_events/#getter","text":"def getter ( dc , key , default = None ) View Source def getter(dc, key, default=None): res = dc.get(key, default) if isinstance(res, list): res = res[0] elif isinstance(res, dict): res = json.dumps(res) return res","title":"getter"},{"location":"reference/nostalgia/sources/web/linked_events/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/web/linked_events/#events","text":"class Events ( data ) View Source class Events ( NDF ) : vendor = \"web\" @classmethod def object_to_row ( cls , obj ) : row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ) : events = cls . load_object_per_newline ( file_path , nrows ) return cls ( events )","title":"Events"},{"location":"reference/nostalgia/sources/web/linked_events/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/web/linked_events/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/web/linked_events/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/web/linked_events/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/web/linked_events/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/web/linked_events/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/web/linked_events/#load","text":"def load ( file_path = '~/nostalgia_data/meta.jsonl' , nrows = None , ** kwargs ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/meta.jsonl\", nrows=None, **kwargs): events = cls.load_object_per_newline(file_path, nrows) return cls(events)","title":"load"},{"location":"reference/nostalgia/sources/web/linked_events/#object_to_row","text":"def object_to_row ( obj ) View Source @classmethod def object_to_row(cls, obj): row = get_linked_data(obj) if row is not None: row[\"time\"] = datetime.fromtimestamp(float(obj[\"time\"]), tz=tz) row[\"url\"] = obj[\"url\"] row[\"path\"] = obj[\"path\"] row[\"keywords\"] = \"\" return row","title":"object_to_row"},{"location":"reference/nostalgia/sources/web/linked_events/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/web/linked_events/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/web/linked_events/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/web/linked_events/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/web/linked_events/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/web/linked_events/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/web/linked_events/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/web/linked_events/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/web/linked_events/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/web/linked_events/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/web/linked_events/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/web/linked_events/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/web/linked_events/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/web/linked_events/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/web/linked_events/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/web/linked_events/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/web/linked_events/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/web/linked_events/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/web/linked_events/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/web/linked_events/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/web/linked_events/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/web/linked_events/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/web/linked_events/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/web/linked_events/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/web/linked_events/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/web/linked_events/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/web/linked_events/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/web/linked_events/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/web/linked_events/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/web/linked_events/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/web/linked_events/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/web/linked_events/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/web/linked_events/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/web/linked_events/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/web/linked_events/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/web/linked_events/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/web/linked_events/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/web/linked_google_search/","text":"Module nostalgia.sources.web.linked_google_search View Source import just import pandas as pd import lxml.html from nostalgia.cache import get_cache from nostalgia.ndf import NDF from datetime import datetime from nostalgia.times import tz CACHE = get_cache ( \"linked_google_search\" ) def get_linked_data ( x ): path = x [ \"path\" ] if path in CACHE : return CACHE [ path ] try : html = just . read ( path ) except EOFError : CACHE [ path ] = None return None if not html . strip (): CACHE [ path ] = None return None tree = lxml . html . fromstring ( html ) res = tree . xpath ( \"//input[@name='q' and @type='text']\" ) if not res : linked_data = None else : linked_data = { \"title\" : res [ 0 ] . value } CACHE [ path ] = linked_data return linked_data class GoogleSearch ( NDF ): vendor = \"web\" keywords = [ \"search\" ] nlp_columns = [ \"title\" ] selected_columns = [ \"time\" , \"title\" , \"url\" ] @classmethod def object_to_row ( cls , obj ): if \"google\" not in str ( obj [ \"url\" ]) or \"/search\" not in str ( obj [ \"url\" ]): return None row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None ): data = cls . load_object_per_newline ( file_path , nrows ) # google pages found more often? data = data [ data . title != data . title . shift ( 1 )] return cls ( data ) Variables CACHE Functions get_linked_data def get_linked_data ( x ) View Source def get_linked_data(x): path = x[\"path\"] if path in CACHE: return CACHE[path] try: html = just.read(path) except EOFError: CACHE[path] = None return None if not html.strip(): CACHE[path] = None return None tree = lxml.html.fromstring(html) res = tree.xpath(\"//input[@name='q' and @type='text']\") if not res: linked_data = None else: linked_data = {\"title\": res[0].value} CACHE[path] = linked_data return linked_data Classes GoogleSearch class GoogleSearch ( data ) View Source class GoogleSearch ( NDF ) : vendor = \"web\" keywords = [ \"search\" ] nlp_columns = [ \"title\" ] selected_columns = [ \"time\" , \"title\" , \"url\" ] @classmethod def object_to_row ( cls , obj ) : if \"google\" not in str ( obj [ \"url\" ]) or \"/search\" not in str ( obj [ \"url\" ]) : return None row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None ) : data = cls . load_object_per_newline ( file_path , nrows ) # google pages found more often ? data = data [ data . title != data . title . shift ( 1 )] return cls ( data ) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( file_path = '~/nostalgia_data/meta.jsonl' , nrows = None ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/meta.jsonl\", nrows=None): data = cls.load_object_per_newline(file_path, nrows) # google pages found more often? data = data[data.title != data.title.shift(1)] return cls(data) object_to_row def object_to_row ( obj ) View Source @classmethod def object_to_row(cls, obj): if \"google\" not in str(obj[\"url\"]) or \"/search\" not in str(obj[\"url\"]): return None row = get_linked_data(obj) if row is not None: row[\"time\"] = datetime.fromtimestamp(float(obj[\"time\"]), tz=tz) row[\"url\"] = obj[\"url\"] row[\"path\"] = obj[\"path\"] row[\"keywords\"] = \"\" return row Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Linked Google Search"},{"location":"reference/nostalgia/sources/web/linked_google_search/#module-nostalgiasourcesweblinked_google_search","text":"View Source import just import pandas as pd import lxml.html from nostalgia.cache import get_cache from nostalgia.ndf import NDF from datetime import datetime from nostalgia.times import tz CACHE = get_cache ( \"linked_google_search\" ) def get_linked_data ( x ): path = x [ \"path\" ] if path in CACHE : return CACHE [ path ] try : html = just . read ( path ) except EOFError : CACHE [ path ] = None return None if not html . strip (): CACHE [ path ] = None return None tree = lxml . html . fromstring ( html ) res = tree . xpath ( \"//input[@name='q' and @type='text']\" ) if not res : linked_data = None else : linked_data = { \"title\" : res [ 0 ] . value } CACHE [ path ] = linked_data return linked_data class GoogleSearch ( NDF ): vendor = \"web\" keywords = [ \"search\" ] nlp_columns = [ \"title\" ] selected_columns = [ \"time\" , \"title\" , \"url\" ] @classmethod def object_to_row ( cls , obj ): if \"google\" not in str ( obj [ \"url\" ]) or \"/search\" not in str ( obj [ \"url\" ]): return None row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None ): data = cls . load_object_per_newline ( file_path , nrows ) # google pages found more often? data = data [ data . title != data . title . shift ( 1 )] return cls ( data )","title":"Module nostalgia.sources.web.linked_google_search"},{"location":"reference/nostalgia/sources/web/linked_google_search/#variables","text":"CACHE","title":"Variables"},{"location":"reference/nostalgia/sources/web/linked_google_search/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/web/linked_google_search/#get_linked_data","text":"def get_linked_data ( x ) View Source def get_linked_data(x): path = x[\"path\"] if path in CACHE: return CACHE[path] try: html = just.read(path) except EOFError: CACHE[path] = None return None if not html.strip(): CACHE[path] = None return None tree = lxml.html.fromstring(html) res = tree.xpath(\"//input[@name='q' and @type='text']\") if not res: linked_data = None else: linked_data = {\"title\": res[0].value} CACHE[path] = linked_data return linked_data","title":"get_linked_data"},{"location":"reference/nostalgia/sources/web/linked_google_search/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/web/linked_google_search/#googlesearch","text":"class GoogleSearch ( data ) View Source class GoogleSearch ( NDF ) : vendor = \"web\" keywords = [ \"search\" ] nlp_columns = [ \"title\" ] selected_columns = [ \"time\" , \"title\" , \"url\" ] @classmethod def object_to_row ( cls , obj ) : if \"google\" not in str ( obj [ \"url\" ]) or \"/search\" not in str ( obj [ \"url\" ]) : return None row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None ) : data = cls . load_object_per_newline ( file_path , nrows ) # google pages found more often ? data = data [ data . title != data . title . shift ( 1 )] return cls ( data )","title":"GoogleSearch"},{"location":"reference/nostalgia/sources/web/linked_google_search/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/web/linked_google_search/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/web/linked_google_search/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/web/linked_google_search/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/web/linked_google_search/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/web/linked_google_search/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/web/linked_google_search/#load","text":"def load ( file_path = '~/nostalgia_data/meta.jsonl' , nrows = None ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/meta.jsonl\", nrows=None): data = cls.load_object_per_newline(file_path, nrows) # google pages found more often? data = data[data.title != data.title.shift(1)] return cls(data)","title":"load"},{"location":"reference/nostalgia/sources/web/linked_google_search/#object_to_row","text":"def object_to_row ( obj ) View Source @classmethod def object_to_row(cls, obj): if \"google\" not in str(obj[\"url\"]) or \"/search\" not in str(obj[\"url\"]): return None row = get_linked_data(obj) if row is not None: row[\"time\"] = datetime.fromtimestamp(float(obj[\"time\"]), tz=tz) row[\"url\"] = obj[\"url\"] row[\"path\"] = obj[\"path\"] row[\"keywords\"] = \"\" return row","title":"object_to_row"},{"location":"reference/nostalgia/sources/web/linked_google_search/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/web/linked_google_search/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/web/linked_google_search/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/web/linked_google_search/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/web/linked_google_search/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/web/linked_google_search/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/web/linked_google_search/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/web/linked_google_search/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/web/linked_google_search/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/web/linked_google_search/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/web/linked_google_search/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/web/linked_google_search/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/web/linked_google_search/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/web/linked_google_search/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/web/linked_google_search/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/web/linked_google_search/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/web/linked_google_search/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/web/linked_google_search/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/web/linked_google_search/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/web/linked_google_search/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/web/linked_google_search/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/web/linked_google_search/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/web/linked_google_search/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/web/linked_google_search/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/web/linked_google_search/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/web/linked_google_search/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/web/linked_google_search/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/web/linked_google_search/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/web/linked_google_search/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/web/linked_google_search/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/web/linked_google_search/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/web/linked_google_search/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/web/linked_google_search/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/web/linked_google_search/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/web/linked_google_search/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/web/linked_google_search/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/web/linked_google_search/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/web/linked_offers/","text":"Module nostalgia.sources.web.linked_offers View Source import os import json from collections import Counter from datetime import datetime from urllib.parse import urljoin import just import pandas as pd from nostalgia.utils import parse_price from nostalgia.times import tz from nostalgia.nlp import nlp from nostalgia.ndf import NDF from auto_extract import parse_article from nostalgia.sources.web.get_keywords_for_product import get_keywords_for_product from nostalgia.cache import get_cache CACHE = get_cache ( \"linked_offers\" ) def getter ( dc , key , default = None ): res = dc . get ( key , default ) if isinstance ( res , list ): res = res [ 0 ] elif isinstance ( res , dict ): res = json . dumps ( res ) return res from natura import Finder finder = Finder () def get_linked_data_jd ( art ): data = None try : jdata = art . jsonld except json . JSONDecodeError : return None for y in jdata : if not y : continue if isinstance ( y , list ): y = y [ 0 ] if y . get ( \"@type\" ) != \"Product\" : continue offers = y . get ( \"offers\" , []) if isinstance ( offers , dict ): offers = [ offers ] brand = y . get ( \"brand\" , {}) if isinstance ( brand , dict ): brand = brand . get ( \"name\" ) for offer in offers : currency = offer . get ( 'priceCurrency' ) if currency is None : continue price = offer . get ( \"price\" ) or offer . get ( \"lowPrice\" ) price = parse_price ( price ) if price is None or price == 0 : continue name = getter ( y , \"name\" ) if isinstance ( name , list ): name = \"\" . join ( name [: 1 ]) data = { \"currency\" : currency , \"price\" : price , \"name\" : name , \"origin\" : art . url , \"image\" : getter ( y , \"image\" ), \"brand\" : brand , \"description\" : getter ( y , \"description\" ), \"source\" : \"jsonld\" , } return data def deeper_price_md ( art ): data = None for y in art . microdata : tp = str ( y . get ( \"type\" , \"\" )) if not tp . endswith ( \"/Product\" ): continue props = y . get ( \"properties\" ) if props is None : continue name = y . get ( \"properties\" , {}) . get ( \"name\" ) or \" \" . join ( art . title . split ()) if isinstance ( name , list ): name = name [ 0 ] offers = y . get ( \"properties\" , {}) . get ( \"offers\" , {}) offers = props . get ( \"offers\" , []) if isinstance ( offers , dict ): offers = [ offers ] for offer in offers : offer = offer . get ( \"properties\" , offer ) price = offer . get ( \"price\" ) or offer . get ( \"lowPrice\" ) price = parse_price ( price ) if price is None or price == 0 : continue currency = offer . get ( \"priceCurrency\" ) if currency is None : continue brand = y . get ( \"brand\" ) or props . get ( \"brand\" ) or offer . get ( \"brand\" ) if isinstance ( brand , dict ): brand = brand . get ( \"properties\" , {}) . get ( \"name\" ) image = y . get ( \"image\" ) or props . get ( \"image\" ) or offer . get ( \"image\" ) if isinstance ( image , list ): image = image [ 0 ] if isinstance ( image , dict ): prop = image . get ( \"properties\" , image ) image = [ v for k , v in prop . items () if \"url\" in k . lower ()] if image : image = image [ 0 ] description = ( y . get ( \"description\" ) or props . get ( \"description\" ) or offer . get ( \"description\" ) ) if isinstance ( description , list ): description = description [ 0 ] elif isinstance ( description , dict ): description = json . dumps ( description ) return { \"currency\" : currency , \"price\" : price , \"name\" : name , \"origin\" : art . url , \"image\" : urljoin ( art . url , image ), \"brand\" : brand , \"description\" : description , \"source\" : \"md\" , } def get_linked_data_md ( art ): data = None for y in art . microdata : props = y . get ( \"properties\" ) if props is None : continue tp = str ( y . get ( \"type\" , \"\" )) if tp . endswith ( \"/Offer\" ) or tp . endswith ( \"/AggregateOffer\" ): price = props . get ( \"price\" ) or props . get ( \"lowPrice\" ) price = parse_price ( price ) if price is None or price == 0 : continue currency = props . get ( \"priceCurrency\" ) if currency is None : continue return { \"currency\" : currency , \"price\" : price , \"name\" : \" \" . join ( art . title . split ()), \"origin\" : art . url , \"image\" : getter ( y , \"image\" ), \"brand\" : None , \"description\" : y . get ( \"description\" ), \"source\" : \"md\" , } if tp . endswith ( \"/Product\" ): data = deeper_price_md ( art ) if data is not None : return data continue offers = props . get ( \"offers\" , []) if isinstance ( offers , dict ): offers = [ offers ] brand = props . get ( \"brand\" , {}) if isinstance ( brand , dict ): brand = brand . get ( \"properties\" , {}) . get ( \"name\" ) for offer in offers : offer_properties = offer . get ( \"properties\" ) if offer_properties is None : continue currency = offer_properties . get ( 'priceCurrency' ) if currency is None : continue price = offer_properties . get ( \"price\" ) or offer_properties . get ( \"lowPrice\" ) price = parse_price ( price ) if price is None or price == 0 : continue data = { \"currency\" : currency , \"price\" : price , \"name\" : \"\" . join ( props . get ( \"name\" , \"\" )), \"origin\" : art . url , \"image\" : getter ( props , \"image\" ), \"brand\" : brand , \"description\" : getter ( props , \"description\" ), \"source\" : \"md\" , } return data def get_linked_amazon ( art ): if \"www.amazon.com\" not in str ( art . url ): return None for x in [ \"account\" , \"cart\" ]: if x in art . url : return None paths = [ \"//span[@id='priceblock_ourprice']\" , \"//div[@id='buyNewSection']\" , \"//form//span[contains(@class, 'offer-price')]\" , \"//div[@id='olp-upd-new']//span/a\" , \"//div[@id='olp-upd-used']//span/a\" , \"//div[@id='olp_feature_div']//span/a\" , \"//span[contains(@class, 'header-price')]\" , ] price = None for path in paths : res = art . tree . xpath ( path ) if res : price = res [ 0 ] . text_content () break if price is not None : brand = art . tree . xpath ( \"//a[@id='bylineInfo']\" ) brand = brand [ 0 ] . text_content () if brand else None if brand is None : br = art . tree . xpath ( \"//div[@id='bylineInfo']\" ) if br : brand = \" \" . join ( br [ 0 ] . text_content () . split ()[ 1 :]) features = art . tree . xpath ( \"//div[@id='feature-bullets']\" ) features = features [ 0 ] . text_content () if features else None data = { \"currency\" : \"USD\" , \"price\" : max ([ x . value for x in finder . findall ( price )]), \"name\" : \" \" . join ( art . title . split ()), \"origin\" : art . url , \"image\" : None , \"brand\" : brand , \"description\" : features , \"source\" : \"custom_amazon\" , } return data def get_linked_data ( x ): path = x [ \"path\" ] if path in CACHE : return CACHE [ path ] try : html = just . read ( path ) except EOFError : CACHE [ path ] = None return None if not html . strip (): CACHE [ path ] = None return None art = parse_article ( html , x [ \"url\" ]) linked_data = get_linked_amazon ( art ) if linked_data is None : linked_data = get_linked_data_md ( art ) if linked_data is None : linked_data = get_linked_data_jd ( art ) CACHE [ path ] = linked_data return linked_data class Offers ( NDF ): vendor = \"web\" keywords = [ \"product\" , \"offer\" , \"office\" , \"did i see\" , \"did i look\" , \"did i search\" , \"looked at\" , \"searched for\" , \"did i google\" , \"cost\" , ] nlp_columns = [ \"name\" , \"brand\" , \"keywords\" ] selected_columns = [ \"time\" , \"name\" , \"price\" , \"url\" ] @nlp ( \"end\" , \"how much\" ) def sum ( self ): return self . price . sum () @classmethod def object_to_row ( cls , obj ): row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] if row . get ( \"name\" ): row [ \"keywords\" ] = \" \" . join ([ x [ 0 ] for x in get_keywords_for_product ( row [ \"name\" ])]) else : row [ \"keywords\" ] = \"\" if \"image\" in row : img = row [ \"image\" ] row [ \"image\" ] = img if isinstance ( img , str ) or img is None else img [ \"url\" ] return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ): offers = cls . load_object_per_newline ( file_path , nrows ) return cls ( offers ) Variables CACHE finder Functions deeper_price_md def deeper_price_md ( art ) View Source def deeper_price_md(art): data = None for y in art.microdata: tp = str(y.get(\"type\", \"\")) if not tp.endswith(\"/Product\"): continue props = y.get(\"properties\") if props is None: continue name = y.get(\"properties\", {}).get(\"name\") or \" \".join(art.title.split()) if isinstance(name, list): name = name[0] offers = y.get(\"properties\", {}).get(\"offers\", {}) offers = props.get(\"offers\", []) if isinstance(offers, dict): offers = [offers] for offer in offers: offer = offer.get(\"properties\", offer) price = offer.get(\"price\") or offer.get(\"lowPrice\") price = parse_price(price) if price is None or price == 0: continue currency = offer.get(\"priceCurrency\") if currency is None: continue brand = y.get(\"brand\") or props.get(\"brand\") or offer.get(\"brand\") if isinstance(brand, dict): brand = brand.get(\"properties\", {}).get(\"name\") image = y.get(\"image\") or props.get(\"image\") or offer.get(\"image\") if isinstance(image, list): image = image[0] if isinstance(image, dict): prop = image.get(\"properties\", image) image = [v for k, v in prop.items() if \"url\" in k.lower()] if image: image = image[0] description = ( y.get(\"description\") or props.get(\"description\") or offer.get(\"description\") ) if isinstance(description, list): description = description[0] elif isinstance(description, dict): description = json.dumps(description) return { \"currency\": currency, \"price\": price, \"name\": name, \"origin\": art.url, \"image\": urljoin(art.url, image), \"brand\": brand, \"description\": description, \"source\": \"md\", } get_linked_amazon def get_linked_amazon ( art ) View Source def get_linked_amazon(art): if \"www.amazon.com\" not in str(art.url): return None for x in [\"account\", \"cart\"]: if x in art.url: return None paths = [ \"//span[@id='priceblock_ourprice']\", \"//div[@id='buyNewSection']\", \"//form//span[contains(@class, 'offer-price')]\", \"//div[@id='olp-upd-new']//span/a\", \"//div[@id='olp-upd-used']//span/a\", \"//div[@id='olp_feature_div']//span/a\", \"//span[contains(@class, 'header-price')]\", ] price = None for path in paths: res = art.tree.xpath(path) if res: price = res[0].text_content() break if price is not None: brand = art.tree.xpath(\"//a[@id='bylineInfo']\") brand = brand[0].text_content() if brand else None if brand is None: br = art.tree.xpath(\"//div[@id='bylineInfo']\") if br: brand = \" \".join(br[0].text_content().split()[1:]) features = art.tree.xpath(\"//div[@id='feature-bullets']\") features = features[0].text_content() if features else None data = { \"currency\": \"USD\", \"price\": max([x.value for x in finder.findall(price)]), \"name\": \" \".join(art.title.split()), \"origin\": art.url, \"image\": None, \"brand\": brand, \"description\": features, \"source\": \"custom_amazon\", } return data get_linked_data def get_linked_data ( x ) View Source def get_linked_data(x): path = x[\"path\"] if path in CACHE: return CACHE[path] try: html = just.read(path) except EOFError: CACHE[path] = None return None if not html.strip(): CACHE[path] = None return None art = parse_article(html, x[\"url\"]) linked_data = get_linked_amazon(art) if linked_data is None: linked_data = get_linked_data_md(art) if linked_data is None: linked_data = get_linked_data_jd(art) CACHE[path] = linked_data return linked_data get_linked_data_jd def get_linked_data_jd ( art ) View Source def get_linked_data_jd(art): data = None try: jdata = art.jsonld except json.JSONDecodeError: return None for y in jdata: if not y: continue if isinstance(y, list): y = y[0] if y.get(\"@type\") != \"Product\": continue offers = y.get(\"offers\", []) if isinstance(offers, dict): offers = [offers] brand = y.get(\"brand\", {}) if isinstance(brand, dict): brand = brand.get(\"name\") for offer in offers: currency = offer.get('priceCurrency') if currency is None: continue price = offer.get(\"price\") or offer.get(\"lowPrice\") price = parse_price(price) if price is None or price == 0: continue name = getter(y, \"name\") if isinstance(name, list): name = \"\".join(name[:1]) data = { \"currency\": currency, \"price\": price, \"name\": name, \"origin\": art.url, \"image\": getter(y, \"image\"), \"brand\": brand, \"description\": getter(y, \"description\"), \"source\": \"jsonld\", } return data get_linked_data_md def get_linked_data_md ( art ) View Source def get_linked_data_md(art): data = None for y in art.microdata: props = y.get(\"properties\") if props is None: continue tp = str(y.get(\"type\", \"\")) if tp.endswith(\"/Offer\") or tp.endswith(\"/AggregateOffer\"): price = props.get(\"price\") or props.get(\"lowPrice\") price = parse_price(price) if price is None or price == 0: continue currency = props.get(\"priceCurrency\") if currency is None: continue return { \"currency\": currency, \"price\": price, \"name\": \" \".join(art.title.split()), \"origin\": art.url, \"image\": getter(y, \"image\"), \"brand\": None, \"description\": y.get(\"description\"), \"source\": \"md\", } if tp.endswith(\"/Product\"): data = deeper_price_md(art) if data is not None: return data continue offers = props.get(\"offers\", []) if isinstance(offers, dict): offers = [offers] brand = props.get(\"brand\", {}) if isinstance(brand, dict): brand = brand.get(\"properties\", {}).get(\"name\") for offer in offers: offer_properties = offer.get(\"properties\") if offer_properties is None: continue currency = offer_properties.get('priceCurrency') if currency is None: continue price = offer_properties.get(\"price\") or offer_properties.get(\"lowPrice\") price = parse_price(price) if price is None or price == 0: continue data = { \"currency\": currency, \"price\": price, \"name\": \"\".join(props.get(\"name\", \"\")), \"origin\": art.url, \"image\": getter(props, \"image\"), \"brand\": brand, \"description\": getter(props, \"description\"), \"source\": \"md\", } return data getter def getter ( dc , key , default = None ) View Source def getter(dc, key, default=None): res = dc.get(key, default) if isinstance(res, list): res = res[0] elif isinstance(res, dict): res = json.dumps(res) return res Classes Offers class Offers ( data ) View Source class Offers ( NDF ) : vendor = \"web\" keywords = [ \"product\" , \"offer\" , \"office\" , \"did i see\" , \"did i look\" , \"did i search\" , \"looked at\" , \"searched for\" , \"did i google\" , \"cost\" , ] nlp_columns = [ \"name\" , \"brand\" , \"keywords\" ] selected_columns = [ \"time\" , \"name\" , \"price\" , \"url\" ] @nlp ( \"end\" , \"how much\" ) def sum ( self ) : return self . price . sum () @classmethod def object_to_row ( cls , obj ) : row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] if row . get ( \"name\" ) : row [ \"keywords\" ] = \" \" . join ([ x [ 0 ] for x in get_keywords_for_product ( row [ \"name\" ])]) else : row [ \"keywords\" ] = \"\" if \"image\" in row : img = row [ \"image\" ] row [ \"image\" ] = img if isinstance ( img , str ) or img is None else img [ \"url\" ] return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ) : offers = cls . load_object_per_newline ( file_path , nrows ) return cls ( offers ) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( file_path = '~/nostalgia_data/meta.jsonl' , nrows = None , ** kwargs ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/meta.jsonl\", nrows=None, **kwargs): offers = cls.load_object_per_newline(file_path, nrows) return cls(offers) object_to_row def object_to_row ( obj ) View Source @classmethod def object_to_row(cls, obj): row = get_linked_data(obj) if row is not None: row[\"time\"] = datetime.fromtimestamp(float(obj[\"time\"]), tz=tz) row[\"url\"] = obj[\"url\"] row[\"path\"] = obj[\"path\"] if row.get(\"name\"): row[\"keywords\"] = \" \".join([x[0] for x in get_keywords_for_product(row[\"name\"])]) else: row[\"keywords\"] = \"\" if \"image\" in row: img = row[\"image\"] row[\"image\"] = img if isinstance(img, str) or img is None else img[\"url\"] return row Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) sum def sum ( self ) View Source @nlp(\"end\", \"how much\") def sum(self): return self.price.sum() tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Linked Offers"},{"location":"reference/nostalgia/sources/web/linked_offers/#module-nostalgiasourcesweblinked_offers","text":"View Source import os import json from collections import Counter from datetime import datetime from urllib.parse import urljoin import just import pandas as pd from nostalgia.utils import parse_price from nostalgia.times import tz from nostalgia.nlp import nlp from nostalgia.ndf import NDF from auto_extract import parse_article from nostalgia.sources.web.get_keywords_for_product import get_keywords_for_product from nostalgia.cache import get_cache CACHE = get_cache ( \"linked_offers\" ) def getter ( dc , key , default = None ): res = dc . get ( key , default ) if isinstance ( res , list ): res = res [ 0 ] elif isinstance ( res , dict ): res = json . dumps ( res ) return res from natura import Finder finder = Finder () def get_linked_data_jd ( art ): data = None try : jdata = art . jsonld except json . JSONDecodeError : return None for y in jdata : if not y : continue if isinstance ( y , list ): y = y [ 0 ] if y . get ( \"@type\" ) != \"Product\" : continue offers = y . get ( \"offers\" , []) if isinstance ( offers , dict ): offers = [ offers ] brand = y . get ( \"brand\" , {}) if isinstance ( brand , dict ): brand = brand . get ( \"name\" ) for offer in offers : currency = offer . get ( 'priceCurrency' ) if currency is None : continue price = offer . get ( \"price\" ) or offer . get ( \"lowPrice\" ) price = parse_price ( price ) if price is None or price == 0 : continue name = getter ( y , \"name\" ) if isinstance ( name , list ): name = \"\" . join ( name [: 1 ]) data = { \"currency\" : currency , \"price\" : price , \"name\" : name , \"origin\" : art . url , \"image\" : getter ( y , \"image\" ), \"brand\" : brand , \"description\" : getter ( y , \"description\" ), \"source\" : \"jsonld\" , } return data def deeper_price_md ( art ): data = None for y in art . microdata : tp = str ( y . get ( \"type\" , \"\" )) if not tp . endswith ( \"/Product\" ): continue props = y . get ( \"properties\" ) if props is None : continue name = y . get ( \"properties\" , {}) . get ( \"name\" ) or \" \" . join ( art . title . split ()) if isinstance ( name , list ): name = name [ 0 ] offers = y . get ( \"properties\" , {}) . get ( \"offers\" , {}) offers = props . get ( \"offers\" , []) if isinstance ( offers , dict ): offers = [ offers ] for offer in offers : offer = offer . get ( \"properties\" , offer ) price = offer . get ( \"price\" ) or offer . get ( \"lowPrice\" ) price = parse_price ( price ) if price is None or price == 0 : continue currency = offer . get ( \"priceCurrency\" ) if currency is None : continue brand = y . get ( \"brand\" ) or props . get ( \"brand\" ) or offer . get ( \"brand\" ) if isinstance ( brand , dict ): brand = brand . get ( \"properties\" , {}) . get ( \"name\" ) image = y . get ( \"image\" ) or props . get ( \"image\" ) or offer . get ( \"image\" ) if isinstance ( image , list ): image = image [ 0 ] if isinstance ( image , dict ): prop = image . get ( \"properties\" , image ) image = [ v for k , v in prop . items () if \"url\" in k . lower ()] if image : image = image [ 0 ] description = ( y . get ( \"description\" ) or props . get ( \"description\" ) or offer . get ( \"description\" ) ) if isinstance ( description , list ): description = description [ 0 ] elif isinstance ( description , dict ): description = json . dumps ( description ) return { \"currency\" : currency , \"price\" : price , \"name\" : name , \"origin\" : art . url , \"image\" : urljoin ( art . url , image ), \"brand\" : brand , \"description\" : description , \"source\" : \"md\" , } def get_linked_data_md ( art ): data = None for y in art . microdata : props = y . get ( \"properties\" ) if props is None : continue tp = str ( y . get ( \"type\" , \"\" )) if tp . endswith ( \"/Offer\" ) or tp . endswith ( \"/AggregateOffer\" ): price = props . get ( \"price\" ) or props . get ( \"lowPrice\" ) price = parse_price ( price ) if price is None or price == 0 : continue currency = props . get ( \"priceCurrency\" ) if currency is None : continue return { \"currency\" : currency , \"price\" : price , \"name\" : \" \" . join ( art . title . split ()), \"origin\" : art . url , \"image\" : getter ( y , \"image\" ), \"brand\" : None , \"description\" : y . get ( \"description\" ), \"source\" : \"md\" , } if tp . endswith ( \"/Product\" ): data = deeper_price_md ( art ) if data is not None : return data continue offers = props . get ( \"offers\" , []) if isinstance ( offers , dict ): offers = [ offers ] brand = props . get ( \"brand\" , {}) if isinstance ( brand , dict ): brand = brand . get ( \"properties\" , {}) . get ( \"name\" ) for offer in offers : offer_properties = offer . get ( \"properties\" ) if offer_properties is None : continue currency = offer_properties . get ( 'priceCurrency' ) if currency is None : continue price = offer_properties . get ( \"price\" ) or offer_properties . get ( \"lowPrice\" ) price = parse_price ( price ) if price is None or price == 0 : continue data = { \"currency\" : currency , \"price\" : price , \"name\" : \"\" . join ( props . get ( \"name\" , \"\" )), \"origin\" : art . url , \"image\" : getter ( props , \"image\" ), \"brand\" : brand , \"description\" : getter ( props , \"description\" ), \"source\" : \"md\" , } return data def get_linked_amazon ( art ): if \"www.amazon.com\" not in str ( art . url ): return None for x in [ \"account\" , \"cart\" ]: if x in art . url : return None paths = [ \"//span[@id='priceblock_ourprice']\" , \"//div[@id='buyNewSection']\" , \"//form//span[contains(@class, 'offer-price')]\" , \"//div[@id='olp-upd-new']//span/a\" , \"//div[@id='olp-upd-used']//span/a\" , \"//div[@id='olp_feature_div']//span/a\" , \"//span[contains(@class, 'header-price')]\" , ] price = None for path in paths : res = art . tree . xpath ( path ) if res : price = res [ 0 ] . text_content () break if price is not None : brand = art . tree . xpath ( \"//a[@id='bylineInfo']\" ) brand = brand [ 0 ] . text_content () if brand else None if brand is None : br = art . tree . xpath ( \"//div[@id='bylineInfo']\" ) if br : brand = \" \" . join ( br [ 0 ] . text_content () . split ()[ 1 :]) features = art . tree . xpath ( \"//div[@id='feature-bullets']\" ) features = features [ 0 ] . text_content () if features else None data = { \"currency\" : \"USD\" , \"price\" : max ([ x . value for x in finder . findall ( price )]), \"name\" : \" \" . join ( art . title . split ()), \"origin\" : art . url , \"image\" : None , \"brand\" : brand , \"description\" : features , \"source\" : \"custom_amazon\" , } return data def get_linked_data ( x ): path = x [ \"path\" ] if path in CACHE : return CACHE [ path ] try : html = just . read ( path ) except EOFError : CACHE [ path ] = None return None if not html . strip (): CACHE [ path ] = None return None art = parse_article ( html , x [ \"url\" ]) linked_data = get_linked_amazon ( art ) if linked_data is None : linked_data = get_linked_data_md ( art ) if linked_data is None : linked_data = get_linked_data_jd ( art ) CACHE [ path ] = linked_data return linked_data class Offers ( NDF ): vendor = \"web\" keywords = [ \"product\" , \"offer\" , \"office\" , \"did i see\" , \"did i look\" , \"did i search\" , \"looked at\" , \"searched for\" , \"did i google\" , \"cost\" , ] nlp_columns = [ \"name\" , \"brand\" , \"keywords\" ] selected_columns = [ \"time\" , \"name\" , \"price\" , \"url\" ] @nlp ( \"end\" , \"how much\" ) def sum ( self ): return self . price . sum () @classmethod def object_to_row ( cls , obj ): row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] if row . get ( \"name\" ): row [ \"keywords\" ] = \" \" . join ([ x [ 0 ] for x in get_keywords_for_product ( row [ \"name\" ])]) else : row [ \"keywords\" ] = \"\" if \"image\" in row : img = row [ \"image\" ] row [ \"image\" ] = img if isinstance ( img , str ) or img is None else img [ \"url\" ] return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ): offers = cls . load_object_per_newline ( file_path , nrows ) return cls ( offers )","title":"Module nostalgia.sources.web.linked_offers"},{"location":"reference/nostalgia/sources/web/linked_offers/#variables","text":"CACHE finder","title":"Variables"},{"location":"reference/nostalgia/sources/web/linked_offers/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/web/linked_offers/#deeper_price_md","text":"def deeper_price_md ( art ) View Source def deeper_price_md(art): data = None for y in art.microdata: tp = str(y.get(\"type\", \"\")) if not tp.endswith(\"/Product\"): continue props = y.get(\"properties\") if props is None: continue name = y.get(\"properties\", {}).get(\"name\") or \" \".join(art.title.split()) if isinstance(name, list): name = name[0] offers = y.get(\"properties\", {}).get(\"offers\", {}) offers = props.get(\"offers\", []) if isinstance(offers, dict): offers = [offers] for offer in offers: offer = offer.get(\"properties\", offer) price = offer.get(\"price\") or offer.get(\"lowPrice\") price = parse_price(price) if price is None or price == 0: continue currency = offer.get(\"priceCurrency\") if currency is None: continue brand = y.get(\"brand\") or props.get(\"brand\") or offer.get(\"brand\") if isinstance(brand, dict): brand = brand.get(\"properties\", {}).get(\"name\") image = y.get(\"image\") or props.get(\"image\") or offer.get(\"image\") if isinstance(image, list): image = image[0] if isinstance(image, dict): prop = image.get(\"properties\", image) image = [v for k, v in prop.items() if \"url\" in k.lower()] if image: image = image[0] description = ( y.get(\"description\") or props.get(\"description\") or offer.get(\"description\") ) if isinstance(description, list): description = description[0] elif isinstance(description, dict): description = json.dumps(description) return { \"currency\": currency, \"price\": price, \"name\": name, \"origin\": art.url, \"image\": urljoin(art.url, image), \"brand\": brand, \"description\": description, \"source\": \"md\", }","title":"deeper_price_md"},{"location":"reference/nostalgia/sources/web/linked_offers/#get_linked_amazon","text":"def get_linked_amazon ( art ) View Source def get_linked_amazon(art): if \"www.amazon.com\" not in str(art.url): return None for x in [\"account\", \"cart\"]: if x in art.url: return None paths = [ \"//span[@id='priceblock_ourprice']\", \"//div[@id='buyNewSection']\", \"//form//span[contains(@class, 'offer-price')]\", \"//div[@id='olp-upd-new']//span/a\", \"//div[@id='olp-upd-used']//span/a\", \"//div[@id='olp_feature_div']//span/a\", \"//span[contains(@class, 'header-price')]\", ] price = None for path in paths: res = art.tree.xpath(path) if res: price = res[0].text_content() break if price is not None: brand = art.tree.xpath(\"//a[@id='bylineInfo']\") brand = brand[0].text_content() if brand else None if brand is None: br = art.tree.xpath(\"//div[@id='bylineInfo']\") if br: brand = \" \".join(br[0].text_content().split()[1:]) features = art.tree.xpath(\"//div[@id='feature-bullets']\") features = features[0].text_content() if features else None data = { \"currency\": \"USD\", \"price\": max([x.value for x in finder.findall(price)]), \"name\": \" \".join(art.title.split()), \"origin\": art.url, \"image\": None, \"brand\": brand, \"description\": features, \"source\": \"custom_amazon\", } return data","title":"get_linked_amazon"},{"location":"reference/nostalgia/sources/web/linked_offers/#get_linked_data","text":"def get_linked_data ( x ) View Source def get_linked_data(x): path = x[\"path\"] if path in CACHE: return CACHE[path] try: html = just.read(path) except EOFError: CACHE[path] = None return None if not html.strip(): CACHE[path] = None return None art = parse_article(html, x[\"url\"]) linked_data = get_linked_amazon(art) if linked_data is None: linked_data = get_linked_data_md(art) if linked_data is None: linked_data = get_linked_data_jd(art) CACHE[path] = linked_data return linked_data","title":"get_linked_data"},{"location":"reference/nostalgia/sources/web/linked_offers/#get_linked_data_jd","text":"def get_linked_data_jd ( art ) View Source def get_linked_data_jd(art): data = None try: jdata = art.jsonld except json.JSONDecodeError: return None for y in jdata: if not y: continue if isinstance(y, list): y = y[0] if y.get(\"@type\") != \"Product\": continue offers = y.get(\"offers\", []) if isinstance(offers, dict): offers = [offers] brand = y.get(\"brand\", {}) if isinstance(brand, dict): brand = brand.get(\"name\") for offer in offers: currency = offer.get('priceCurrency') if currency is None: continue price = offer.get(\"price\") or offer.get(\"lowPrice\") price = parse_price(price) if price is None or price == 0: continue name = getter(y, \"name\") if isinstance(name, list): name = \"\".join(name[:1]) data = { \"currency\": currency, \"price\": price, \"name\": name, \"origin\": art.url, \"image\": getter(y, \"image\"), \"brand\": brand, \"description\": getter(y, \"description\"), \"source\": \"jsonld\", } return data","title":"get_linked_data_jd"},{"location":"reference/nostalgia/sources/web/linked_offers/#get_linked_data_md","text":"def get_linked_data_md ( art ) View Source def get_linked_data_md(art): data = None for y in art.microdata: props = y.get(\"properties\") if props is None: continue tp = str(y.get(\"type\", \"\")) if tp.endswith(\"/Offer\") or tp.endswith(\"/AggregateOffer\"): price = props.get(\"price\") or props.get(\"lowPrice\") price = parse_price(price) if price is None or price == 0: continue currency = props.get(\"priceCurrency\") if currency is None: continue return { \"currency\": currency, \"price\": price, \"name\": \" \".join(art.title.split()), \"origin\": art.url, \"image\": getter(y, \"image\"), \"brand\": None, \"description\": y.get(\"description\"), \"source\": \"md\", } if tp.endswith(\"/Product\"): data = deeper_price_md(art) if data is not None: return data continue offers = props.get(\"offers\", []) if isinstance(offers, dict): offers = [offers] brand = props.get(\"brand\", {}) if isinstance(brand, dict): brand = brand.get(\"properties\", {}).get(\"name\") for offer in offers: offer_properties = offer.get(\"properties\") if offer_properties is None: continue currency = offer_properties.get('priceCurrency') if currency is None: continue price = offer_properties.get(\"price\") or offer_properties.get(\"lowPrice\") price = parse_price(price) if price is None or price == 0: continue data = { \"currency\": currency, \"price\": price, \"name\": \"\".join(props.get(\"name\", \"\")), \"origin\": art.url, \"image\": getter(props, \"image\"), \"brand\": brand, \"description\": getter(props, \"description\"), \"source\": \"md\", } return data","title":"get_linked_data_md"},{"location":"reference/nostalgia/sources/web/linked_offers/#getter","text":"def getter ( dc , key , default = None ) View Source def getter(dc, key, default=None): res = dc.get(key, default) if isinstance(res, list): res = res[0] elif isinstance(res, dict): res = json.dumps(res) return res","title":"getter"},{"location":"reference/nostalgia/sources/web/linked_offers/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/web/linked_offers/#offers","text":"class Offers ( data ) View Source class Offers ( NDF ) : vendor = \"web\" keywords = [ \"product\" , \"offer\" , \"office\" , \"did i see\" , \"did i look\" , \"did i search\" , \"looked at\" , \"searched for\" , \"did i google\" , \"cost\" , ] nlp_columns = [ \"name\" , \"brand\" , \"keywords\" ] selected_columns = [ \"time\" , \"name\" , \"price\" , \"url\" ] @nlp ( \"end\" , \"how much\" ) def sum ( self ) : return self . price . sum () @classmethod def object_to_row ( cls , obj ) : row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] if row . get ( \"name\" ) : row [ \"keywords\" ] = \" \" . join ([ x [ 0 ] for x in get_keywords_for_product ( row [ \"name\" ])]) else : row [ \"keywords\" ] = \"\" if \"image\" in row : img = row [ \"image\" ] row [ \"image\" ] = img if isinstance ( img , str ) or img is None else img [ \"url\" ] return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ) : offers = cls . load_object_per_newline ( file_path , nrows ) return cls ( offers )","title":"Offers"},{"location":"reference/nostalgia/sources/web/linked_offers/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/web/linked_offers/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/web/linked_offers/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/web/linked_offers/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/web/linked_offers/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/web/linked_offers/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/web/linked_offers/#load","text":"def load ( file_path = '~/nostalgia_data/meta.jsonl' , nrows = None , ** kwargs ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/meta.jsonl\", nrows=None, **kwargs): offers = cls.load_object_per_newline(file_path, nrows) return cls(offers)","title":"load"},{"location":"reference/nostalgia/sources/web/linked_offers/#object_to_row","text":"def object_to_row ( obj ) View Source @classmethod def object_to_row(cls, obj): row = get_linked_data(obj) if row is not None: row[\"time\"] = datetime.fromtimestamp(float(obj[\"time\"]), tz=tz) row[\"url\"] = obj[\"url\"] row[\"path\"] = obj[\"path\"] if row.get(\"name\"): row[\"keywords\"] = \" \".join([x[0] for x in get_keywords_for_product(row[\"name\"])]) else: row[\"keywords\"] = \"\" if \"image\" in row: img = row[\"image\"] row[\"image\"] = img if isinstance(img, str) or img is None else img[\"url\"] return row","title":"object_to_row"},{"location":"reference/nostalgia/sources/web/linked_offers/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/web/linked_offers/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/web/linked_offers/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/web/linked_offers/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/web/linked_offers/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/web/linked_offers/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/web/linked_offers/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/web/linked_offers/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/web/linked_offers/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/web/linked_offers/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/web/linked_offers/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/web/linked_offers/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/web/linked_offers/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/web/linked_offers/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/web/linked_offers/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/web/linked_offers/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/web/linked_offers/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/web/linked_offers/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/web/linked_offers/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/web/linked_offers/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/web/linked_offers/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/web/linked_offers/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/web/linked_offers/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/web/linked_offers/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/web/linked_offers/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/web/linked_offers/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/web/linked_offers/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/web/linked_offers/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/web/linked_offers/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/web/linked_offers/#sum","text":"def sum ( self ) View Source @nlp(\"end\", \"how much\") def sum(self): return self.price.sum()","title":"sum"},{"location":"reference/nostalgia/sources/web/linked_offers/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/web/linked_offers/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/web/linked_offers/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/web/linked_offers/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/web/linked_offers/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/web/linked_offers/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/web/linked_offers/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/web/linked_offers/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/web/linked_person/","text":"Module nostalgia.sources.web.linked_person View Source import json from datetime import datetime import just import pandas as pd from nostalgia.times import tz from auto_extract import parse_article from nostalgia.cache import get_cache from nostalgia.ndf import NDF from nostalgia.utils import normalize_name CACHE = get_cache ( \"linked_person\" ) def getter ( dc , key , default = None ): res = dc . get ( key , default ) if isinstance ( res , list ): res = res [ 0 ] elif isinstance ( res , dict ): res = json . dumps ( res ) return res def get_linked_data_jd ( art ): data = None try : jdata = art . jsonld except json . JSONDecodeError : return None for y in jdata : if not y : continue if isinstance ( y , list ): y = y [ 0 ] if y . get ( \"@type\" ) != \"Person\" : continue return { 'description' : getter ( y , \"description\" ), 'startDate' : getter ( y , \"startDate\" ), 'endDate' : getter ( y , \"endDate\" ), 'location' : getter ( y , \"location\" ), \"source\" : \"jsonld\" , } def get_linked_data_md ( art ): data = None for y in art . microdata : props = y . get ( \"properties\" ) if props is None : continue tp = str ( y . get ( \"type\" , \"\" )) if not tp . endswith ( \"/Person\" ): continue return { \"startDate\" : getter ( props , \"startDate\" ), \"endDate\" : getter ( props , \"endDate\" ), \"description\" : getter ( props , \"description\" ), \"name\" : \"\" . join ( getter ( props , \"name\" , \"\" )), \"location\" : getter ( props , \"location\" ), \"source\" : \"md\" , } def get_linked_data ( x ): path = x [ \"path\" ] if path in CACHE : return CACHE [ path ] try : html = just . read ( path ) except EOFError : CACHE [ path ] = None return None if not html . strip (): CACHE [ path ] = None return None art = parse_article ( html , x [ \"url\" ]) linked_data = get_linked_data_md ( art ) if linked_data is None : linked_data = get_linked_data_jd ( art ) CACHE [ path ] = linked_data return linked_data class Person ( NDF ): @classmethod def object_to_row ( cls , obj ): row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ): person = cls . load_object_per_newline ( file_path , nrows ) return cls ( person ) if __name__ == \"__main__\" : person = Person . load ( nrows = 5 ) # https://schema.org/Blog # \"http://schema.org/blogPost\" # http://schema.org/WebPage # http://schema.org/BlogPosting # exlclude http://schema.org/QAPage it = 0 imo = 0 nice = 0 wrong = 0 from collections import Counter import just c = Counter () for x in just . iread ( \"/home/pascal/nostal_tmp/person.jsonl\" ): if \"/Person\" in ( str ( x . get ( \"microdata\" ))): it += 1 y = x score = 0 mc_count = 0 for mc in y [ \"microdata\" ]: if mc . get ( \"type\" ) in [ 'http://schema.org/ImageObject' , 'http://schema.org/QAPage' , 'http://schema.org/Movie' , 'http://schema.org/videoObject' , 'http://schema.org/Organization' , 'http://schema.org/VideoObject' , 'http://schema.org/Question' , 'http://schema.org/CreativeWork' , 'http://schema.org/Code' , ]: continue mc_count += 1 for opt in [ 'mc[\"properties\"][\"author\"]' , 'mc[\"properties\"][\"author\"][\"properties\"][\"name\"]' , 'mc[\"properties\"][\"author\"][\"value\"]' , 'mc[\"properties\"][\"author\"][0][\"value\"]' , 'mc[\"properties\"][\"author\"][0][\"properties\"][\"name\"]' , 'mc[\"properties\"][\"author\"][\"properties\"][\"author\"][\"properties\"][\"name\"][0]' , 'mc[\"properties\"][\"creator\"][\"properties\"][\"name\"]' , 'mc[\"properties\"][\"author\"][0][\"value\"]' , 'mc[\"properties\"][\"mainEntity\"][\"properties\"][\"author\"][\"properties\"][\"name\"]' , 'mc[\"properties\"][\"blogPost\"][\"properties\"][\"author\"][\"properties\"][\"name\"]' , ]: try : x = eval ( opt ) . strip () if not x or x . startswith ( \"http\" ) or \" \\n \" in x : continue print ( x ) score += 1 c [ mc [ \"type\" ] . split ( \"/\" )[ - 1 ]] += 1 break except KeyboardInterrupt : \"a\" + 1 except : pass if mc_count and score == 0 : if len ( y [ \"microdata\" ]) == 1 and list ( y [ \"microdata\" ][ 0 ]) == [ \"value\" ]: continue wrong += 1 continue if score : nice += 1 Variables CACHE Functions get_linked_data def get_linked_data ( x ) View Source def get_linked_data(x): path = x[\"path\"] if path in CACHE: return CACHE[path] try: html = just.read(path) except EOFError: CACHE[path] = None return None if not html.strip(): CACHE[path] = None return None art = parse_article(html, x[\"url\"]) linked_data = get_linked_data_md(art) if linked_data is None: linked_data = get_linked_data_jd(art) CACHE[path] = linked_data return linked_data get_linked_data_jd def get_linked_data_jd ( art ) View Source def get_linked_data_jd(art): data = None try: jdata = art.jsonld except json.JSONDecodeError: return None for y in jdata: if not y: continue if isinstance(y, list): y = y[0] if y.get(\"@type\") != \"Person\": continue return { 'description': getter(y, \"description\"), 'startDate': getter(y, \"startDate\"), 'endDate': getter(y, \"endDate\"), 'location': getter(y, \"location\"), \"source\": \"jsonld\", } get_linked_data_md def get_linked_data_md ( art ) View Source def get_linked_data_md(art): data = None for y in art.microdata: props = y.get(\"properties\") if props is None: continue tp = str(y.get(\"type\", \"\")) if not tp.endswith(\"/Person\"): continue return { \"startDate\": getter(props, \"startDate\"), \"endDate\": getter(props, \"endDate\"), \"description\": getter(props, \"description\"), \"name\": \"\".join(getter(props, \"name\", \"\")), \"location\": getter(props, \"location\"), \"source\": \"md\", } getter def getter ( dc , key , default = None ) View Source def getter(dc, key, default=None): res = dc.get(key, default) if isinstance(res, list): res = res[0] elif isinstance(res, dict): res = json.dumps(res) return res Classes Person class Person ( data ) View Source class Person ( NDF ) : @classmethod def object_to_row ( cls , obj ) : row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ) : person = cls . load_object_per_newline ( file_path , nrows ) return cls ( person ) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( file_path = '~/nostalgia_data/meta.jsonl' , nrows = None , ** kwargs ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/meta.jsonl\", nrows=None, **kwargs): person = cls.load_object_per_newline(file_path, nrows) return cls(person) object_to_row def object_to_row ( obj ) View Source @classmethod def object_to_row(cls, obj): row = get_linked_data(obj) if row is not None: row[\"time\"] = datetime.fromtimestamp(float(obj[\"time\"]), tz=tz) row[\"url\"] = obj[\"url\"] row[\"path\"] = obj[\"path\"] row[\"keywords\"] = \"\" return row Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Linked Person"},{"location":"reference/nostalgia/sources/web/linked_person/#module-nostalgiasourcesweblinked_person","text":"View Source import json from datetime import datetime import just import pandas as pd from nostalgia.times import tz from auto_extract import parse_article from nostalgia.cache import get_cache from nostalgia.ndf import NDF from nostalgia.utils import normalize_name CACHE = get_cache ( \"linked_person\" ) def getter ( dc , key , default = None ): res = dc . get ( key , default ) if isinstance ( res , list ): res = res [ 0 ] elif isinstance ( res , dict ): res = json . dumps ( res ) return res def get_linked_data_jd ( art ): data = None try : jdata = art . jsonld except json . JSONDecodeError : return None for y in jdata : if not y : continue if isinstance ( y , list ): y = y [ 0 ] if y . get ( \"@type\" ) != \"Person\" : continue return { 'description' : getter ( y , \"description\" ), 'startDate' : getter ( y , \"startDate\" ), 'endDate' : getter ( y , \"endDate\" ), 'location' : getter ( y , \"location\" ), \"source\" : \"jsonld\" , } def get_linked_data_md ( art ): data = None for y in art . microdata : props = y . get ( \"properties\" ) if props is None : continue tp = str ( y . get ( \"type\" , \"\" )) if not tp . endswith ( \"/Person\" ): continue return { \"startDate\" : getter ( props , \"startDate\" ), \"endDate\" : getter ( props , \"endDate\" ), \"description\" : getter ( props , \"description\" ), \"name\" : \"\" . join ( getter ( props , \"name\" , \"\" )), \"location\" : getter ( props , \"location\" ), \"source\" : \"md\" , } def get_linked_data ( x ): path = x [ \"path\" ] if path in CACHE : return CACHE [ path ] try : html = just . read ( path ) except EOFError : CACHE [ path ] = None return None if not html . strip (): CACHE [ path ] = None return None art = parse_article ( html , x [ \"url\" ]) linked_data = get_linked_data_md ( art ) if linked_data is None : linked_data = get_linked_data_jd ( art ) CACHE [ path ] = linked_data return linked_data class Person ( NDF ): @classmethod def object_to_row ( cls , obj ): row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ): person = cls . load_object_per_newline ( file_path , nrows ) return cls ( person ) if __name__ == \"__main__\" : person = Person . load ( nrows = 5 ) # https://schema.org/Blog # \"http://schema.org/blogPost\" # http://schema.org/WebPage # http://schema.org/BlogPosting # exlclude http://schema.org/QAPage it = 0 imo = 0 nice = 0 wrong = 0 from collections import Counter import just c = Counter () for x in just . iread ( \"/home/pascal/nostal_tmp/person.jsonl\" ): if \"/Person\" in ( str ( x . get ( \"microdata\" ))): it += 1 y = x score = 0 mc_count = 0 for mc in y [ \"microdata\" ]: if mc . get ( \"type\" ) in [ 'http://schema.org/ImageObject' , 'http://schema.org/QAPage' , 'http://schema.org/Movie' , 'http://schema.org/videoObject' , 'http://schema.org/Organization' , 'http://schema.org/VideoObject' , 'http://schema.org/Question' , 'http://schema.org/CreativeWork' , 'http://schema.org/Code' , ]: continue mc_count += 1 for opt in [ 'mc[\"properties\"][\"author\"]' , 'mc[\"properties\"][\"author\"][\"properties\"][\"name\"]' , 'mc[\"properties\"][\"author\"][\"value\"]' , 'mc[\"properties\"][\"author\"][0][\"value\"]' , 'mc[\"properties\"][\"author\"][0][\"properties\"][\"name\"]' , 'mc[\"properties\"][\"author\"][\"properties\"][\"author\"][\"properties\"][\"name\"][0]' , 'mc[\"properties\"][\"creator\"][\"properties\"][\"name\"]' , 'mc[\"properties\"][\"author\"][0][\"value\"]' , 'mc[\"properties\"][\"mainEntity\"][\"properties\"][\"author\"][\"properties\"][\"name\"]' , 'mc[\"properties\"][\"blogPost\"][\"properties\"][\"author\"][\"properties\"][\"name\"]' , ]: try : x = eval ( opt ) . strip () if not x or x . startswith ( \"http\" ) or \" \\n \" in x : continue print ( x ) score += 1 c [ mc [ \"type\" ] . split ( \"/\" )[ - 1 ]] += 1 break except KeyboardInterrupt : \"a\" + 1 except : pass if mc_count and score == 0 : if len ( y [ \"microdata\" ]) == 1 and list ( y [ \"microdata\" ][ 0 ]) == [ \"value\" ]: continue wrong += 1 continue if score : nice += 1","title":"Module nostalgia.sources.web.linked_person"},{"location":"reference/nostalgia/sources/web/linked_person/#variables","text":"CACHE","title":"Variables"},{"location":"reference/nostalgia/sources/web/linked_person/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/web/linked_person/#get_linked_data","text":"def get_linked_data ( x ) View Source def get_linked_data(x): path = x[\"path\"] if path in CACHE: return CACHE[path] try: html = just.read(path) except EOFError: CACHE[path] = None return None if not html.strip(): CACHE[path] = None return None art = parse_article(html, x[\"url\"]) linked_data = get_linked_data_md(art) if linked_data is None: linked_data = get_linked_data_jd(art) CACHE[path] = linked_data return linked_data","title":"get_linked_data"},{"location":"reference/nostalgia/sources/web/linked_person/#get_linked_data_jd","text":"def get_linked_data_jd ( art ) View Source def get_linked_data_jd(art): data = None try: jdata = art.jsonld except json.JSONDecodeError: return None for y in jdata: if not y: continue if isinstance(y, list): y = y[0] if y.get(\"@type\") != \"Person\": continue return { 'description': getter(y, \"description\"), 'startDate': getter(y, \"startDate\"), 'endDate': getter(y, \"endDate\"), 'location': getter(y, \"location\"), \"source\": \"jsonld\", }","title":"get_linked_data_jd"},{"location":"reference/nostalgia/sources/web/linked_person/#get_linked_data_md","text":"def get_linked_data_md ( art ) View Source def get_linked_data_md(art): data = None for y in art.microdata: props = y.get(\"properties\") if props is None: continue tp = str(y.get(\"type\", \"\")) if not tp.endswith(\"/Person\"): continue return { \"startDate\": getter(props, \"startDate\"), \"endDate\": getter(props, \"endDate\"), \"description\": getter(props, \"description\"), \"name\": \"\".join(getter(props, \"name\", \"\")), \"location\": getter(props, \"location\"), \"source\": \"md\", }","title":"get_linked_data_md"},{"location":"reference/nostalgia/sources/web/linked_person/#getter","text":"def getter ( dc , key , default = None ) View Source def getter(dc, key, default=None): res = dc.get(key, default) if isinstance(res, list): res = res[0] elif isinstance(res, dict): res = json.dumps(res) return res","title":"getter"},{"location":"reference/nostalgia/sources/web/linked_person/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/web/linked_person/#person","text":"class Person ( data ) View Source class Person ( NDF ) : @classmethod def object_to_row ( cls , obj ) : row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , file_path = \"~/nostalgia_data/meta.jsonl\" , nrows = None , ** kwargs ) : person = cls . load_object_per_newline ( file_path , nrows ) return cls ( person )","title":"Person"},{"location":"reference/nostalgia/sources/web/linked_person/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/web/linked_person/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/web/linked_person/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/web/linked_person/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/web/linked_person/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/web/linked_person/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/web/linked_person/#load","text":"def load ( file_path = '~/nostalgia_data/meta.jsonl' , nrows = None , ** kwargs ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/meta.jsonl\", nrows=None, **kwargs): person = cls.load_object_per_newline(file_path, nrows) return cls(person)","title":"load"},{"location":"reference/nostalgia/sources/web/linked_person/#object_to_row","text":"def object_to_row ( obj ) View Source @classmethod def object_to_row(cls, obj): row = get_linked_data(obj) if row is not None: row[\"time\"] = datetime.fromtimestamp(float(obj[\"time\"]), tz=tz) row[\"url\"] = obj[\"url\"] row[\"path\"] = obj[\"path\"] row[\"keywords\"] = \"\" return row","title":"object_to_row"},{"location":"reference/nostalgia/sources/web/linked_person/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/web/linked_person/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/web/linked_person/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/web/linked_person/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/web/linked_person/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/web/linked_person/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/web/linked_person/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/web/linked_person/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/web/linked_person/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/web/linked_person/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/web/linked_person/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/web/linked_person/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/web/linked_person/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/web/linked_person/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/web/linked_person/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/web/linked_person/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/web/linked_person/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/web/linked_person/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/web/linked_person/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/web/linked_person/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/web/linked_person/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/web/linked_person/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/web/linked_person/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/web/linked_person/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/web/linked_person/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/web/linked_person/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/web/linked_person/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/web/linked_person/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/web/linked_person/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/web/linked_person/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/web/linked_person/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/web/linked_person/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/web/linked_person/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/web/linked_person/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/web/linked_person/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/web/linked_person/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/web/linked_person/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/web/linked_videos/","text":"Module nostalgia.sources.web.linked_videos View Source import re import just import urllib.parse import pandas as pd from auto_extract import parse_article from nostalgia.cache import get_cache from datetime import datetime from nostalgia.times import tz from nostalgia.ndf import NDF from nostalgia.nlp import nlp CACHE = get_cache ( \"linked_data_videos\" ) def get_linked_data ( x ): path = x [ \"path\" ] if path in CACHE : return CACHE [ path ] try : html = just . read ( path ) except EOFError : CACHE [ path ] = None return None if not html . strip (): CACHE [ path ] = None return None art = parse_article ( html , x [ \"url\" ]) if \"youtube\" not in art . domain : return None title = re . sub ( \" - YouTube$\" , \"\" , art . tree . xpath ( \"//title/text()\" )[ 0 ]) if title == \"YouTube\" : CACHE [ path ] = None return None if not title : return None vc = art . tree . xpath ( \"//span[contains(@class, 'view-count')]/text()\" ) vc = re . sub ( \"[^0-9]\" , \"\" , vc [ 0 ]) if vc else None watch_part = urllib . parse . parse_qs ( urllib . parse . urlparse ( x [ \"url\" ]) . query )[ \"v\" ] if watch_part : image = \"http://i3.ytimg.com/vi/{}/maxresdefault.jpg\" . format ( watch_part [ 0 ]) else : image = None channel = art . tree . xpath ( \"//ytd-video-owner-renderer//a/text()\" ) if not channel : channel = art . tree . xpath ( \"//ytd-channel-name//a/text()\" ) channel = \" \" . join ( channel ) linked_data = { \"title\" : title , \"type\" : \"video\" , \"source\" : \"youtube\" , \"image\" : image , \"view_count\" : vc , \"channel\" : channel , } CACHE [ path ] = linked_data return linked_data class Videos ( NDF ): vendor = \"web\" keywords = [ \"watch\" , \"viewers\" , \"listen\" , \"video\" , \"music video\" , \"youtube\" ] nlp_columns = [ \"title\" , \"channel\" ] selected_columns = [ \"time\" , \"title\" , \"view_count\" , \"url\" ] @classmethod def object_to_row ( cls , obj ): url = str ( obj [ \"url\" ]) if \"youtub\" not in url or \"watch\" not in url or \"search_query\" in url : return None row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , fname = \"~/nostalgia_data/meta.jsonl\" , nrows = None ): data = cls . load_object_per_newline ( fname , nrows ) return cls ( data ) @nlp ( \"end\" , \"how many views\" ) def sum ( self ): return self . view_count . sum () Variables CACHE Functions get_linked_data def get_linked_data ( x ) View Source def get_linked_data(x): path = x[\"path\"] if path in CACHE: return CACHE[path] try: html = just.read(path) except EOFError: CACHE[path] = None return None if not html.strip(): CACHE[path] = None return None art = parse_article(html, x[\"url\"]) if \"youtube\" not in art.domain: return None title = re.sub(\" - YouTube$\", \"\", art.tree.xpath(\"//title/text()\")[0]) if title == \"YouTube\": CACHE[path] = None return None if not title: return None vc = art.tree.xpath(\"//span[contains(@class, 'view-count')]/text()\") vc = re.sub(\"[^0-9]\", \"\", vc[0]) if vc else None watch_part = urllib.parse.parse_qs(urllib.parse.urlparse(x[\"url\"]).query)[\"v\"] if watch_part: image = \"http://i3.ytimg.com/vi/{}/maxresdefault.jpg\".format(watch_part[0]) else: image = None channel = art.tree.xpath(\"//ytd-video-owner-renderer//a/text()\") if not channel: channel = art.tree.xpath(\"//ytd-channel-name//a/text()\") channel = \" \".join(channel) linked_data = { \"title\": title, \"type\": \"video\", \"source\": \"youtube\", \"image\": image, \"view_count\": vc, \"channel\": channel, } CACHE[path] = linked_data return linked_data Classes Videos class Videos ( data ) View Source class Videos ( NDF ) : vendor = \"web\" keywords = [ \"watch\" , \"viewers\" , \"listen\" , \"video\" , \"music video\" , \"youtube\" ] nlp_columns = [ \"title\" , \"channel\" ] selected_columns = [ \"time\" , \"title\" , \"view_count\" , \"url\" ] @classmethod def object_to_row ( cls , obj ) : url = str ( obj [ \"url\" ]) if \"youtub\" not in url or \"watch\" not in url or \"search_query\" in url : return None row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , fname = \"~/nostalgia_data/meta.jsonl\" , nrows = None ) : data = cls . load_object_per_newline ( fname , nrows ) return cls ( data ) @nlp ( \"end\" , \"how many views\" ) def sum ( self ) : return self . view_count . sum () Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( fname = '~/nostalgia_data/meta.jsonl' , nrows = None ) View Source @classmethod def load(cls, fname=\"~/nostalgia_data/meta.jsonl\", nrows=None): data = cls.load_object_per_newline(fname, nrows) return cls(data) object_to_row def object_to_row ( obj ) View Source @classmethod def object_to_row(cls, obj): url = str(obj[\"url\"]) if \"youtub\" not in url or \"watch\" not in url or \"search_query\" in url: return None row = get_linked_data(obj) if row is not None: row[\"time\"] = datetime.fromtimestamp(float(obj[\"time\"]), tz=tz) row[\"url\"] = obj[\"url\"] row[\"path\"] = obj[\"path\"] row[\"keywords\"] = \"\" return row Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) sum def sum ( self ) View Source @nlp(\"end\", \"how many views\") def sum(self): return self.view_count.sum() tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Linked Videos"},{"location":"reference/nostalgia/sources/web/linked_videos/#module-nostalgiasourcesweblinked_videos","text":"View Source import re import just import urllib.parse import pandas as pd from auto_extract import parse_article from nostalgia.cache import get_cache from datetime import datetime from nostalgia.times import tz from nostalgia.ndf import NDF from nostalgia.nlp import nlp CACHE = get_cache ( \"linked_data_videos\" ) def get_linked_data ( x ): path = x [ \"path\" ] if path in CACHE : return CACHE [ path ] try : html = just . read ( path ) except EOFError : CACHE [ path ] = None return None if not html . strip (): CACHE [ path ] = None return None art = parse_article ( html , x [ \"url\" ]) if \"youtube\" not in art . domain : return None title = re . sub ( \" - YouTube$\" , \"\" , art . tree . xpath ( \"//title/text()\" )[ 0 ]) if title == \"YouTube\" : CACHE [ path ] = None return None if not title : return None vc = art . tree . xpath ( \"//span[contains(@class, 'view-count')]/text()\" ) vc = re . sub ( \"[^0-9]\" , \"\" , vc [ 0 ]) if vc else None watch_part = urllib . parse . parse_qs ( urllib . parse . urlparse ( x [ \"url\" ]) . query )[ \"v\" ] if watch_part : image = \"http://i3.ytimg.com/vi/{}/maxresdefault.jpg\" . format ( watch_part [ 0 ]) else : image = None channel = art . tree . xpath ( \"//ytd-video-owner-renderer//a/text()\" ) if not channel : channel = art . tree . xpath ( \"//ytd-channel-name//a/text()\" ) channel = \" \" . join ( channel ) linked_data = { \"title\" : title , \"type\" : \"video\" , \"source\" : \"youtube\" , \"image\" : image , \"view_count\" : vc , \"channel\" : channel , } CACHE [ path ] = linked_data return linked_data class Videos ( NDF ): vendor = \"web\" keywords = [ \"watch\" , \"viewers\" , \"listen\" , \"video\" , \"music video\" , \"youtube\" ] nlp_columns = [ \"title\" , \"channel\" ] selected_columns = [ \"time\" , \"title\" , \"view_count\" , \"url\" ] @classmethod def object_to_row ( cls , obj ): url = str ( obj [ \"url\" ]) if \"youtub\" not in url or \"watch\" not in url or \"search_query\" in url : return None row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , fname = \"~/nostalgia_data/meta.jsonl\" , nrows = None ): data = cls . load_object_per_newline ( fname , nrows ) return cls ( data ) @nlp ( \"end\" , \"how many views\" ) def sum ( self ): return self . view_count . sum ()","title":"Module nostalgia.sources.web.linked_videos"},{"location":"reference/nostalgia/sources/web/linked_videos/#variables","text":"CACHE","title":"Variables"},{"location":"reference/nostalgia/sources/web/linked_videos/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/web/linked_videos/#get_linked_data","text":"def get_linked_data ( x ) View Source def get_linked_data(x): path = x[\"path\"] if path in CACHE: return CACHE[path] try: html = just.read(path) except EOFError: CACHE[path] = None return None if not html.strip(): CACHE[path] = None return None art = parse_article(html, x[\"url\"]) if \"youtube\" not in art.domain: return None title = re.sub(\" - YouTube$\", \"\", art.tree.xpath(\"//title/text()\")[0]) if title == \"YouTube\": CACHE[path] = None return None if not title: return None vc = art.tree.xpath(\"//span[contains(@class, 'view-count')]/text()\") vc = re.sub(\"[^0-9]\", \"\", vc[0]) if vc else None watch_part = urllib.parse.parse_qs(urllib.parse.urlparse(x[\"url\"]).query)[\"v\"] if watch_part: image = \"http://i3.ytimg.com/vi/{}/maxresdefault.jpg\".format(watch_part[0]) else: image = None channel = art.tree.xpath(\"//ytd-video-owner-renderer//a/text()\") if not channel: channel = art.tree.xpath(\"//ytd-channel-name//a/text()\") channel = \" \".join(channel) linked_data = { \"title\": title, \"type\": \"video\", \"source\": \"youtube\", \"image\": image, \"view_count\": vc, \"channel\": channel, } CACHE[path] = linked_data return linked_data","title":"get_linked_data"},{"location":"reference/nostalgia/sources/web/linked_videos/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/web/linked_videos/#videos","text":"class Videos ( data ) View Source class Videos ( NDF ) : vendor = \"web\" keywords = [ \"watch\" , \"viewers\" , \"listen\" , \"video\" , \"music video\" , \"youtube\" ] nlp_columns = [ \"title\" , \"channel\" ] selected_columns = [ \"time\" , \"title\" , \"view_count\" , \"url\" ] @classmethod def object_to_row ( cls , obj ) : url = str ( obj [ \"url\" ]) if \"youtub\" not in url or \"watch\" not in url or \"search_query\" in url : return None row = get_linked_data ( obj ) if row is not None : row [ \"time\" ] = datetime . fromtimestamp ( float ( obj [ \"time\" ]), tz = tz ) row [ \"url\" ] = obj [ \"url\" ] row [ \"path\" ] = obj [ \"path\" ] row [ \"keywords\" ] = \"\" return row @classmethod def load ( cls , fname = \"~/nostalgia_data/meta.jsonl\" , nrows = None ) : data = cls . load_object_per_newline ( fname , nrows ) return cls ( data ) @nlp ( \"end\" , \"how many views\" ) def sum ( self ) : return self . view_count . sum ()","title":"Videos"},{"location":"reference/nostalgia/sources/web/linked_videos/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/web/linked_videos/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/web/linked_videos/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/web/linked_videos/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/web/linked_videos/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/web/linked_videos/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/web/linked_videos/#load","text":"def load ( fname = '~/nostalgia_data/meta.jsonl' , nrows = None ) View Source @classmethod def load(cls, fname=\"~/nostalgia_data/meta.jsonl\", nrows=None): data = cls.load_object_per_newline(fname, nrows) return cls(data)","title":"load"},{"location":"reference/nostalgia/sources/web/linked_videos/#object_to_row","text":"def object_to_row ( obj ) View Source @classmethod def object_to_row(cls, obj): url = str(obj[\"url\"]) if \"youtub\" not in url or \"watch\" not in url or \"search_query\" in url: return None row = get_linked_data(obj) if row is not None: row[\"time\"] = datetime.fromtimestamp(float(obj[\"time\"]), tz=tz) row[\"url\"] = obj[\"url\"] row[\"path\"] = obj[\"path\"] row[\"keywords\"] = \"\" return row","title":"object_to_row"},{"location":"reference/nostalgia/sources/web/linked_videos/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/web/linked_videos/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/web/linked_videos/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/web/linked_videos/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/web/linked_videos/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/web/linked_videos/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/web/linked_videos/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/web/linked_videos/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/web/linked_videos/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/web/linked_videos/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/web/linked_videos/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/web/linked_videos/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/web/linked_videos/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/web/linked_videos/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/web/linked_videos/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/web/linked_videos/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/web/linked_videos/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/web/linked_videos/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/web/linked_videos/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/web/linked_videos/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/web/linked_videos/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/web/linked_videos/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/web/linked_videos/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/web/linked_videos/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/web/linked_videos/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/web/linked_videos/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/web/linked_videos/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/web/linked_videos/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/web/linked_videos/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/web/linked_videos/#sum","text":"def sum ( self ) View Source @nlp(\"end\", \"how many views\") def sum(self): return self.view_count.sum()","title":"sum"},{"location":"reference/nostalgia/sources/web/linked_videos/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/web/linked_videos/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/web/linked_videos/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/web/linked_videos/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/web/linked_videos/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/web/linked_videos/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/web/linked_videos/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/web/linked_videos/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/web/videos_watched/","text":"Module nostalgia.sources.web.videos_watched View Source from nostalgia.ndf import NDF from nostalgia.times import datetime_from_timestamp class VideosWatched ( NDF ): vendor = \"web\" @classmethod def handle_dataframe_per_file ( cls , data , fname ): data [ \"playingSince\" ] = [ datetime_from_timestamp ( x , \"utc\" ) for x in data [ \"playingSince\" ]] data [ \"playingUntil\" ] = [ datetime_from_timestamp ( x , \"utc\" ) for x in data [ \"playingUntil\" ]] return data @classmethod def load ( cls , nrows = None ): data = cls . load_data_file_modified_time ( \"~/nostalgia_data/videos_watched.jsonl\" , nrows = nrows ) return cls ( data . reset_index ()) Classes VideosWatched class VideosWatched ( data ) View Source class VideosWatched ( NDF ) : vendor = \"web\" @classmethod def handle_dataframe_per_file ( cls , data , fname ) : data [ \"playingSince\" ] = [ datetime_from_timestamp ( x , \"utc\" ) for x in data [ \"playingSince\" ]] data [ \"playingUntil\" ] = [ datetime_from_timestamp ( x , \"utc\" ) for x in data [ \"playingUntil\" ]] return data @classmethod def load ( cls , nrows = None ) : data = cls . load_data_file_modified_time ( \"~/nostalgia_data/videos_watched.jsonl\" , nrows = nrows ) return cls ( data . reset_index ()) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} handle_dataframe_per_file def handle_dataframe_per_file ( data , fname ) View Source @classmethod def handle_dataframe_per_file(cls, data, fname): data[\"playingSince\"] = [datetime_from_timestamp(x, \"utc\") for x in data[\"playingSince\"]] data[\"playingUntil\"] = [datetime_from_timestamp(x, \"utc\") for x in data[\"playingUntil\"]] return data ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): data = cls.load_data_file_modified_time( \"~/nostalgia_data/videos_watched.jsonl\", nrows=nrows ) return cls(data.reset_index()) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Videos Watched"},{"location":"reference/nostalgia/sources/web/videos_watched/#module-nostalgiasourceswebvideos_watched","text":"View Source from nostalgia.ndf import NDF from nostalgia.times import datetime_from_timestamp class VideosWatched ( NDF ): vendor = \"web\" @classmethod def handle_dataframe_per_file ( cls , data , fname ): data [ \"playingSince\" ] = [ datetime_from_timestamp ( x , \"utc\" ) for x in data [ \"playingSince\" ]] data [ \"playingUntil\" ] = [ datetime_from_timestamp ( x , \"utc\" ) for x in data [ \"playingUntil\" ]] return data @classmethod def load ( cls , nrows = None ): data = cls . load_data_file_modified_time ( \"~/nostalgia_data/videos_watched.jsonl\" , nrows = nrows ) return cls ( data . reset_index ())","title":"Module nostalgia.sources.web.videos_watched"},{"location":"reference/nostalgia/sources/web/videos_watched/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/web/videos_watched/#videoswatched","text":"class VideosWatched ( data ) View Source class VideosWatched ( NDF ) : vendor = \"web\" @classmethod def handle_dataframe_per_file ( cls , data , fname ) : data [ \"playingSince\" ] = [ datetime_from_timestamp ( x , \"utc\" ) for x in data [ \"playingSince\" ]] data [ \"playingUntil\" ] = [ datetime_from_timestamp ( x , \"utc\" ) for x in data [ \"playingUntil\" ]] return data @classmethod def load ( cls , nrows = None ) : data = cls . load_data_file_modified_time ( \"~/nostalgia_data/videos_watched.jsonl\" , nrows = nrows ) return cls ( data . reset_index ())","title":"VideosWatched"},{"location":"reference/nostalgia/sources/web/videos_watched/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/web/videos_watched/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/web/videos_watched/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/web/videos_watched/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/web/videos_watched/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/web/videos_watched/#handle_dataframe_per_file","text":"def handle_dataframe_per_file ( data , fname ) View Source @classmethod def handle_dataframe_per_file(cls, data, fname): data[\"playingSince\"] = [datetime_from_timestamp(x, \"utc\") for x in data[\"playingSince\"]] data[\"playingUntil\"] = [datetime_from_timestamp(x, \"utc\") for x in data[\"playingUntil\"]] return data","title":"handle_dataframe_per_file"},{"location":"reference/nostalgia/sources/web/videos_watched/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/web/videos_watched/#load","text":"def load ( nrows = None ) View Source @classmethod def load(cls, nrows=None): data = cls.load_data_file_modified_time( \"~/nostalgia_data/videos_watched.jsonl\", nrows=nrows ) return cls(data.reset_index())","title":"load"},{"location":"reference/nostalgia/sources/web/videos_watched/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/web/videos_watched/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/web/videos_watched/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/web/videos_watched/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/web/videos_watched/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/web/videos_watched/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/web/videos_watched/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/web/videos_watched/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/web/videos_watched/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/web/videos_watched/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/web/videos_watched/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/web/videos_watched/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/web/videos_watched/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/web/videos_watched/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/web/videos_watched/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/web/videos_watched/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/web/videos_watched/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/web/videos_watched/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/web/videos_watched/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/web/videos_watched/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/web/videos_watched/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/web/videos_watched/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/web/videos_watched/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/web/videos_watched/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/web/videos_watched/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/web/videos_watched/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/web/videos_watched/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/web/videos_watched/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/web/videos_watched/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/web/videos_watched/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/web/videos_watched/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/web/videos_watched/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/web/videos_watched/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/web/videos_watched/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/web/videos_watched/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/web/videos_watched/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/web/videos_watched/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/whereami/","text":"Module nostalgia.sources.whereami View Source import os import pandas as pd from datetime import datetime from nostalgia.ndf import NDF from nostalgia.times import tz def get_time ( x ): y , z = x . split () try : d = datetime . fromtimestamp ( float ( y ), tz ) except Exception as e : print ( e ) return return d , z class Whereami ( NDF ): nlp_columns = [ \"name\" ] @classmethod def load ( cls , file_path = \"~/nostalgia_data/input/whereami/history.tsv\" , nrows = None , ** kwargs ): file_path = os . path . expanduser ( file_path ) ndata = [] nrows = nrows or float ( \"inf\" ) it = 0 with open ( file_path ) as f : for x in f . read () . split ( \" \\n \" )[ 1 :]: if not x . strip (): continue x = get_time ( x ) if x is None : continue if it & gt ; nrows : break ndata . append ( x ) it += 1 whereami = pd . DataFrame ( ndata , columns = [ \"start\" , \"name\" ]) . sort_values ( \"start\" ) whereami = whereami [ whereami . name != \"unknown\" ] whereami [ \"end\" ] = whereami . start . shift ( - 1 ) whereami [ \"in_range\" ] = ( whereami . end - whereami . start ) & lt ; pd . Timedelta ( minutes = 20 ) whereami [ \"new\" ] = ( whereami . name != whereami . name . shift ( 1 )) . cumsum () groups = [] group_id = None start = None name = None for s , n , e , ir , new in zip ( whereami . start , whereami . name , whereami . end , whereami . in_range , whereami . new ): if group_id is None : group_id = new start = s name = n elif group_id != new or not ir : groups . append ({ \"start\" : start , \"end\" : s , \"name\" : name }) group_id = new start = s name = n return cls ( pd . DataFrame ( groups )) Sub-modules nostalgia.sources.whereami.whereami_scheduler Functions get_time def get_time ( x ) View Source def get_time(x): y, z = x.split() try: d = datetime.fromtimestamp(float(y), tz) except Exception as e: print(e) return return d, z Classes Whereami class Whereami ( data ) View Source class Whereami ( NDF ) : nlp_columns = [ \"name\" ] @classmethod def load ( cls , file_path = \"~/nostalgia_data/input/whereami/history.tsv\" , nrows = None , ** kwargs ) : file_path = os . path . expanduser ( file_path ) ndata = [] nrows = nrows or float ( \"inf\" ) it = 0 with open ( file_path ) as f : for x in f . read (). split ( \"\\n\" )[ 1 : ] : if not x . strip () : continue x = get_time ( x ) if x is None : continue if it & gt ; nrows : break ndata . append ( x ) it += 1 whereami = pd . DataFrame ( ndata , columns = [ \"start\" , \"name\" ]). sort_values ( \"start\" ) whereami = whereami [ whereami . name != \"unknown\" ] whereami [ \"end\" ] = whereami . start . shift ( - 1 ) whereami [ \"in_range\" ] = ( whereami . end - whereami . start ) & lt ; pd . Timedelta ( minutes = 20 ) whereami [ \"new\" ] = ( whereami . name != whereami . name . shift ( 1 )). cumsum () groups = [] group_id = None start = None name = None for s , n , e , ir , new in zip ( whereami . start , whereami . name , whereami . end , whereami . in_range , whereami . new ) : if group_id is None : group_id = new start = s name = n elif group_id != new or not ir : groups.append ({ \"start\" : start , \"end\" : s , \"name\" : name }) group_id = new start = s name = n return cls ( pd . DataFrame ( groups )) Ancestors (in MRO) nostalgia.ndf.NDF Class variables keywords nlp_columns nlp_when selected_columns vendor Static methods df_label def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title() get_schema def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)} ingest def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings) load def load ( file_path = '~/nostalgia_data/input/whereami/history.tsv' , nrows = None , ** kwargs ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/input/whereami/history.tsv\", nrows=None, **kwargs): file_path = os.path.expanduser(file_path) ndata = [] nrows = nrows or float(\"inf\") it = 0 with open(file_path) as f: for x in f.read().split(\"\\n\")[1:]: if not x.strip(): continue x = get_time(x) if x is None: continue if it &gt; nrows: break ndata.append(x) it += 1 whereami = pd.DataFrame(ndata, columns=[\"start\", \"name\"]).sort_values(\"start\") whereami = whereami[whereami.name != \"unknown\"] whereami[\"end\"] = whereami.start.shift(-1) whereami[\"in_range\"] = (whereami.end - whereami.start) &lt; pd.Timedelta(minutes=20) whereami[\"new\"] = (whereami.name != whereami.name.shift(1)).cumsum() groups = [] group_id = None start = None name = None for s, n, e, ir, new in zip( whereami.start, whereami.name, whereami.end, whereami.in_range, whereami.new ): if group_id is None: group_id = new start = s name = n elif group_id != new or not ir: groups.append({\"start\": start, \"end\": s, \"name\": name}) group_id = new start = s name = n return cls(pd.DataFrame(groups)) Instance variables at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday Methods add_heartrate def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\") as_simple def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\") at def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\") at_day def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)] at_night def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )] at_time def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res) browsing def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) by_me def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self col_contains def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)] containing def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array]) count def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0] duration_longer_than def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)] duration_shorter_than def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)] get_type_from_registry def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value head def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs)) heartrate_above def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value) heartrate_below def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value) heartrate_range def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high] in_a def in_a ( self , s ) View Source def in_a(self, s): return self.near(s) infer_time def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times)) last def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1]) near def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection) not_at_day def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)] query def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr)) read def read ( self , index ) View Source def read(self, index): return just.read(self.path[index]) show_me def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False)) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) ) tail def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs)) take_from def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name] time_level def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0 to_html def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html() to_place def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results) view def view ( self , index ) View Source def view(self, index): view(self.path[index]) when def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs)) when_at def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"Index"},{"location":"reference/nostalgia/sources/whereami/#module-nostalgiasourceswhereami","text":"View Source import os import pandas as pd from datetime import datetime from nostalgia.ndf import NDF from nostalgia.times import tz def get_time ( x ): y , z = x . split () try : d = datetime . fromtimestamp ( float ( y ), tz ) except Exception as e : print ( e ) return return d , z class Whereami ( NDF ): nlp_columns = [ \"name\" ] @classmethod def load ( cls , file_path = \"~/nostalgia_data/input/whereami/history.tsv\" , nrows = None , ** kwargs ): file_path = os . path . expanduser ( file_path ) ndata = [] nrows = nrows or float ( \"inf\" ) it = 0 with open ( file_path ) as f : for x in f . read () . split ( \" \\n \" )[ 1 :]: if not x . strip (): continue x = get_time ( x ) if x is None : continue if it & gt ; nrows : break ndata . append ( x ) it += 1 whereami = pd . DataFrame ( ndata , columns = [ \"start\" , \"name\" ]) . sort_values ( \"start\" ) whereami = whereami [ whereami . name != \"unknown\" ] whereami [ \"end\" ] = whereami . start . shift ( - 1 ) whereami [ \"in_range\" ] = ( whereami . end - whereami . start ) & lt ; pd . Timedelta ( minutes = 20 ) whereami [ \"new\" ] = ( whereami . name != whereami . name . shift ( 1 )) . cumsum () groups = [] group_id = None start = None name = None for s , n , e , ir , new in zip ( whereami . start , whereami . name , whereami . end , whereami . in_range , whereami . new ): if group_id is None : group_id = new start = s name = n elif group_id != new or not ir : groups . append ({ \"start\" : start , \"end\" : s , \"name\" : name }) group_id = new start = s name = n return cls ( pd . DataFrame ( groups ))","title":"Module nostalgia.sources.whereami"},{"location":"reference/nostalgia/sources/whereami/#sub-modules","text":"nostalgia.sources.whereami.whereami_scheduler","title":"Sub-modules"},{"location":"reference/nostalgia/sources/whereami/#functions","text":"","title":"Functions"},{"location":"reference/nostalgia/sources/whereami/#get_time","text":"def get_time ( x ) View Source def get_time(x): y, z = x.split() try: d = datetime.fromtimestamp(float(y), tz) except Exception as e: print(e) return return d, z","title":"get_time"},{"location":"reference/nostalgia/sources/whereami/#classes","text":"","title":"Classes"},{"location":"reference/nostalgia/sources/whereami/#whereami","text":"class Whereami ( data ) View Source class Whereami ( NDF ) : nlp_columns = [ \"name\" ] @classmethod def load ( cls , file_path = \"~/nostalgia_data/input/whereami/history.tsv\" , nrows = None , ** kwargs ) : file_path = os . path . expanduser ( file_path ) ndata = [] nrows = nrows or float ( \"inf\" ) it = 0 with open ( file_path ) as f : for x in f . read (). split ( \"\\n\" )[ 1 : ] : if not x . strip () : continue x = get_time ( x ) if x is None : continue if it & gt ; nrows : break ndata . append ( x ) it += 1 whereami = pd . DataFrame ( ndata , columns = [ \"start\" , \"name\" ]). sort_values ( \"start\" ) whereami = whereami [ whereami . name != \"unknown\" ] whereami [ \"end\" ] = whereami . start . shift ( - 1 ) whereami [ \"in_range\" ] = ( whereami . end - whereami . start ) & lt ; pd . Timedelta ( minutes = 20 ) whereami [ \"new\" ] = ( whereami . name != whereami . name . shift ( 1 )). cumsum () groups = [] group_id = None start = None name = None for s , n , e , ir , new in zip ( whereami . start , whereami . name , whereami . end , whereami . in_range , whereami . new ) : if group_id is None : group_id = new start = s name = n elif group_id != new or not ir : groups.append ({ \"start\" : start , \"end\" : s , \"name\" : name }) group_id = new start = s name = n return cls ( pd . DataFrame ( groups ))","title":"Whereami"},{"location":"reference/nostalgia/sources/whereami/#ancestors-in-mro","text":"nostalgia.ndf.NDF","title":"Ancestors (in MRO)"},{"location":"reference/nostalgia/sources/whereami/#class-variables","text":"keywords nlp_columns nlp_when selected_columns vendor","title":"Class variables"},{"location":"reference/nostalgia/sources/whereami/#static-methods","text":"","title":"Static methods"},{"location":"reference/nostalgia/sources/whereami/#df_label","text":"def df_label ( ) View Source @classmethod def df_label(cls): return normalize_name(cls.__name__).replace(\"_\", \" \").title()","title":"df_label"},{"location":"reference/nostalgia/sources/whereami/#get_schema","text":"def get_schema ( * args , ** kwargs ) View Source @classmethod def get_schema(cls, *args, **kwargs): sample = cls.load(*args, nrows=5, **kwargs) return {k: v for k, v in zip(sample.columns, sample.dtypes)}","title":"get_schema"},{"location":"reference/nostalgia/sources/whereami/#ingest","text":"def ingest ( ) View Source @classmethod def ingest(cls): load_from_download(vendor=cls.vendor, **cls.ingest_settings)","title":"ingest"},{"location":"reference/nostalgia/sources/whereami/#load","text":"def load ( file_path = '~/nostalgia_data/input/whereami/history.tsv' , nrows = None , ** kwargs ) View Source @classmethod def load(cls, file_path=\"~/nostalgia_data/input/whereami/history.tsv\", nrows=None, **kwargs): file_path = os.path.expanduser(file_path) ndata = [] nrows = nrows or float(\"inf\") it = 0 with open(file_path) as f: for x in f.read().split(\"\\n\")[1:]: if not x.strip(): continue x = get_time(x) if x is None: continue if it &gt; nrows: break ndata.append(x) it += 1 whereami = pd.DataFrame(ndata, columns=[\"start\", \"name\"]).sort_values(\"start\") whereami = whereami[whereami.name != \"unknown\"] whereami[\"end\"] = whereami.start.shift(-1) whereami[\"in_range\"] = (whereami.end - whereami.start) &lt; pd.Timedelta(minutes=20) whereami[\"new\"] = (whereami.name != whereami.name.shift(1)).cumsum() groups = [] group_id = None start = None name = None for s, n, e, ir, new in zip( whereami.start, whereami.name, whereami.end, whereami.in_range, whereami.new ): if group_id is None: group_id = new start = s name = n elif group_id != new or not ir: groups.append({\"start\": start, \"end\": s, \"name\": name}) group_id = new start = s name = n return cls(pd.DataFrame(groups))","title":"load"},{"location":"reference/nostalgia/sources/whereami/#instance-variables","text":"at_home at_work df_name duration during_office_hours end in_office_days in_office_hours last_day last_month last_week last_year outside_office_hours start text_cols time when_asleep yesterday","title":"Instance variables"},{"location":"reference/nostalgia/sources/whereami/#methods","text":"","title":"Methods"},{"location":"reference/nostalgia/sources/whereami/#add_heartrate","text":"def add_heartrate ( self ) View Source def add_heartrate(self): return self.take_from(\"heartrate\", \"value\")","title":"add_heartrate"},{"location":"reference/nostalgia/sources/whereami/#as_simple","text":"def as_simple ( self , max_n = None ) View Source def as_simple(self, max_n=None): data = { \"title\": self.df_name, # default, to be overwritten \"url\": None, \"start\": None, \"end\": None, # \"body\": None, \"type\": self.df_name, \"interval\": True, \"sender\": None, \"value\": getattr(self, \"value\", None), \"index_loc\": self.index, } for x in [\"title\", \"name\", \"naam\", \"subject\", \"url\", \"content\", \"text\", \"value\"]: res = getattr(self, x, None) if res is not None: data[\"title\"] = res break res = getattr(self, \"sender\", None) if res is not None: data[\"sender\"] = res for x in [\"url\", \"path\", \"file\"]: res = getattr(self, x, None) if res is not None: data[\"url\"] = res break for x in [\"start\", \"time\", \"timestamp\"]: res = getattr(self, x, None) if res is not None: data[\"start\"] = res break for x in [\"end\"]: res = getattr(self, x, None) if res is not None: data[\"end\"] = res - pd.Timedelta(microseconds=1) break if data[\"end\"] is None: data[\"end\"] = data[\"start\"] + pd.Timedelta(minutes=5) data[\"interval\"] = False try: data = pd.DataFrame(data).sort_values(\"start\") if max_n is not None: data = data.iloc[-max_n:] return data except ValueError: raise ValueError(f\"No fields are mapped for {self.__class__.__name__}\")","title":"as_simple"},{"location":"reference/nostalgia/sources/whereami/#at","text":"def at ( self , time_or_place ) View Source def at(self, time_or_place): if isinstance(time_or_place, NDF) and time_or_place.df_name.endswith(\"places\"): return self.when_at(time_or_place) if isinstance(time_or_place, str): mp = parse_date_tz(time_or_place) if mp: start = mp.start_date end = mp.end_date return self.at_time(start, end) else: return self.when_at(get_type_from_registry(\"places\").containing(time_or_place)) raise ValueError(\"neither time nor place was passed\")","title":"at"},{"location":"reference/nostalgia/sources/whereami/#at_day","text":"def at_day ( self , day_or_class ) View Source def at_day(self, day_or_class): return self[self._select_at_day(day_or_class)]","title":"at_day"},{"location":"reference/nostalgia/sources/whereami/#at_night","text":"def at_night ( self , start = 22 , end = 8 ) View Source def at_night ( self , start = 22 , end = 8 ): if self . _start_col is not None: return self [( self . start . dt . hour &gt ; start ) | ( self . end . dt . hour &lt ; end )] return self [( self . time . dt . hour &gt ; start ) | ( self . time . dt . hour &lt ; end )]","title":"at_night"},{"location":"reference/nostalgia/sources/whereami/#at_time","text":"def at_time ( self , start , end = None , sort_diff = True , ** window_kwargs ) View Source def at_time(self, start, end=None, sort_diff=True, **window_kwargs): if is_mp(start): start = start.start_date end = start.end_date elif isinstance(start, str) and end is None: mp = parse_date_tz(start) start = mp.start_date end = mp.end_date elif isinstance(start, str) and isinstance(end, str): mp = parse_date_tz(start) start = mp.start_date mp = parse_date_tz(end) end = mp.start_date elif end is None and window_kwargs: end = start elif end is None: raise ValueError( \"Either a metaperiod, a date string, 2 times, or time + window_kwargs.\" ) self.infer_time() if window_kwargs: start = start - pd.Timedelta(**window_kwargs) end = end + pd.Timedelta(**window_kwargs) if self._start_col is None: res = self[ab_overlap_c(start, end, self[self._time_col])] else: res = self[ab_overlap_cd(self[self._start_col], self[self._end_col], start, end)] if not res.empty and sort_diff: # avg_time = start + (end - start) / 2 # res[\"sort_score\"] = -abs(res[self._time_col] - avg_time) # res = res.sort_values('sort_score').drop('sort_score', axis=1) res[\"sort_score\"] = res[self._time_col] res = res.sort_values('sort_score').drop('sort_score', axis=1) return self.__class__(res)","title":"at_time"},{"location":"reference/nostalgia/sources/whereami/#browsing","text":"def browsing ( self , other , ** window_kwargs ) View Source def browsing(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"browser\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"browsing"},{"location":"reference/nostalgia/sources/whereami/#by_me","text":"def by_me ( self ) View Source @nlp(\"filter\", \"by me\", \"i\", \"my\") def by_me(self): return self","title":"by_me"},{"location":"reference/nostalgia/sources/whereami/#col_contains","text":"def col_contains ( self , string , col_name , case = False , regex = False , na = False ) View Source def col_contains(self, string, col_name, case=False, regex=False, na=False): return self[self[col_name].str.contains(string, case=case, regex=regex, na=na)]","title":"col_contains"},{"location":"reference/nostalgia/sources/whereami/#containing","text":"def containing ( self , string , col_name = None , case = False , regex = True , na = False , bound = True ) Filters using string in all string columns when col_name is None, otherwise in just that one View Source def containing(self, string, col_name=None, case=False, regex=True, na=False, bound=True): \"\"\" Filters using string in all string columns when col_name is None, otherwise in just that one \"\"\" if regex and bound: string = r\"\\b\" + string + r\"\\b\" if col_name is not None: return self.col_contains(string, col_name, case, regex, na) bool_cols = [ self[x].str.contains(string, case=case, regex=regex, na=na) for x in self.text_cols ] bool_array = bool_cols[0] for b in bool_cols[1:]: bool_array = np.logical_or(bool_array, b) return self.__class__(self[bool_array])","title":"containing"},{"location":"reference/nostalgia/sources/whereami/#count","text":"def count ( self ) View Source @nlp(\"end\", \"how many\", \"how many times\", \"how often\") def count(self): return self.shape[0]","title":"count"},{"location":"reference/nostalgia/sources/whereami/#duration_longer_than","text":"def duration_longer_than ( self , ** timedelta_kwargs ) View Source def duration_longer_than(self, **timedelta_kwargs): return self[(self.end - self.time) &gt;= timedelta(**timedelta_kwargs)]","title":"duration_longer_than"},{"location":"reference/nostalgia/sources/whereami/#duration_shorter_than","text":"def duration_shorter_than ( self , ** timedelta_kwargs ) View Source def duration_shorter_than(self, **timedelta_kwargs): return self[(self.end - self.time) &lt;= timedelta(**timedelta_kwargs)]","title":"duration_shorter_than"},{"location":"reference/nostalgia/sources/whereami/#get_type_from_registry","text":"def get_type_from_registry ( self , tp ) View Source def get_type_from_registry(self, tp): for key, value in rergistry.items(): if key.endswith(tp): return value","title":"get_type_from_registry"},{"location":"reference/nostalgia/sources/whereami/#head","text":"def head ( self , * args , ** kwargs ) View Source def head(self, *args, **kwargs): return self.__class__(super().head(*args, **kwargs))","title":"head"},{"location":"reference/nostalgia/sources/whereami/#heartrate_above","text":"def heartrate_above ( self , value ) View Source def heartrate_above(self, value): return self.heartrate_range(value)","title":"heartrate_above"},{"location":"reference/nostalgia/sources/whereami/#heartrate_below","text":"def heartrate_below ( self , value ) View Source def heartrate_below(self, value): return self.heartrate_range(None, value)","title":"heartrate_below"},{"location":"reference/nostalgia/sources/whereami/#heartrate_range","text":"def heartrate_range ( self , low , high = None ) View Source def heartrate_range(self, low, high=None): if \"heartrate_value\" not in self.columns: self.add_heartrate() if high is not None and low is not None: return self[(self[\"heartrate_value\"] &gt;= low) &amp; self[\"heartrate_value\"] &lt; high] if low is not None: return self[self[\"heartrate_value\"] &gt;= low] if high is not None: return self[self[\"heartrate_value\"] &lt; high]","title":"heartrate_range"},{"location":"reference/nostalgia/sources/whereami/#in_a","text":"def in_a ( self , s ) View Source def in_a(self, s): return self.near(s)","title":"in_a"},{"location":"reference/nostalgia/sources/whereami/#infer_time","text":"def infer_time ( self ) View Source def infer_time(self): if self.__class__.__name__ == \"Results\": self._start_col, self._time_col, self._end_col = \"start\", \"start\", \"end\" return times = [x for x, y in zip(self.columns, self.dtypes) if \"datetime\" in str(y)] levels = [self.time_level(self[x]) for x in times] if not levels: raise ValueError(f\"No datetime found in {self.__class__.__name__}\") max_level = max(levels) # workaround # start: 10:00:00 # end: 10:00:59 times = [t for t, l in zip(times, levels) if l == max_level or (l == 2 and max_level == 3)] num_times = len(times) self.num_times = num_times if num_times == 0: self._start_col, self._time_col, self._end_col = None, None, None elif num_times == 1: self._start_col, self._time_col, self._end_col = None, times[0], None elif num_times == 2: col1, col2 = times sub = self[self[col1].notnull() &amp; self[col2].notnull()] a, b = sub[col1], sub[col2] if (a &gt; b).all(): col1, col2 = col2, col1 elif not (a &lt;= b).all(): raise ValueError( \"Not strictly one col higher than other with dates, can't determine\" ) if col1 == \"end\" and col2 == \"start\": col2, col1 = col1, col2 self._start_col, self._time_col, self._end_col = col1, col1, col2 else: msg = 'infer time failed: there can only be 1 or 2 datetime columns at the same granularity.' raise Exception(msg + \" Found: \" + str(times))","title":"infer_time"},{"location":"reference/nostalgia/sources/whereami/#last","text":"def last ( self ) View Source @nlp(\"filter\", \"last\", \"last time\", \"most recently\") def last(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False).iloc[:1])","title":"last"},{"location":"reference/nostalgia/sources/whereami/#near","text":"def near ( self , s ) View Source def near(self, s): if isinstance(s, NDF) and s.df_name.endswith(\"places\"): selection = s else: selection = get_type_from_registry(\"places\").containing(s) return self.when_at(selection)","title":"near"},{"location":"reference/nostalgia/sources/whereami/#not_at_day","text":"def not_at_day ( self , day_or_class ) View Source def not_at_day(self, day_or_class): return self[~self._select_at_day(day_or_class)]","title":"not_at_day"},{"location":"reference/nostalgia/sources/whereami/#query","text":"def query ( self , expr ) View Source def query(self, expr): return self.__class__(super().query(expr))","title":"query"},{"location":"reference/nostalgia/sources/whereami/#read","text":"def read ( self , index ) View Source def read(self, index): return just.read(self.path[index])","title":"read"},{"location":"reference/nostalgia/sources/whereami/#show_me","text":"def show_me ( self ) View Source @nlp(\"end\", \"show\", \"show me\", \"show me the\", \"show the\", \"what\") def show_me(self): _ = self.time # to get inferred time if not set col = self._time_col or self._start_col return self.__class__(self.sort_values(col, na_position=\"last\", ascending=False))","title":"show_me"},{"location":"reference/nostalgia/sources/whereami/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' ) View Source def sort_values( self, by, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last' ): return self.__class__( pd.DataFrame.sort_values(self, by, axis, ascending, inplace, kind, na_position) )","title":"sort_values"},{"location":"reference/nostalgia/sources/whereami/#tail","text":"def tail ( self , * args , ** kwargs ) View Source def tail(self, *args, **kwargs): return self.__class__(super().tail(*args, **kwargs))","title":"tail"},{"location":"reference/nostalgia/sources/whereami/#take_from","text":"def take_from ( self , registry_ending , col_name ) View Source def take_from(self, registry_ending, col_name): for registry_type in registry: if not registry_type.endswith(registry_ending): continue # TODO: loop over columns, so we only do index lookup once # TODO: do not only try self.time but also self.end new_name = registry_ending + \"_\" + col_name if new_name in self.columns: return self[new_name] tp = get_type_from_registry(registry_type) results = [] if not self.inferred_time: self.infer_time() for x in self[self._time_col]: try: res = tp.loc[x] if not isinstance(res, pd.Series): res = res.iloc[0] res = res[col_name] except (KeyError, TypeError): res = np.nan results.append(res) self[new_name] = results return self[new_name]","title":"take_from"},{"location":"reference/nostalgia/sources/whereami/#time_level","text":"def time_level ( self , col ) View Source def time_level(self, col): if (col.dt.microsecond != 0).any(): return 4 if (col.dt.second != 0).any(): return 3 if (col.dt.minute != 0).any(): return 2 if (col.dt.hour != 0).any(): return 1 return 0","title":"time_level"},{"location":"reference/nostalgia/sources/whereami/#to_html","text":"def to_html ( self ) View Source def to_html(self): if self.selected_columns: data = pd.DataFrame({x: getattr(self, x) for x in self.selected_columns}) return data.to_html() return super().to_html()","title":"to_html"},{"location":"reference/nostalgia/sources/whereami/#to_place","text":"def to_place ( self ) View Source def to_place(self): results = [] places = get_type_from_registry(\"places\") for time in self.time: try: results.append(places.iloc[places.index.get_loc(time)].iloc[0]) except (TypeError, KeyError): pass return places.__class__(results)","title":"to_place"},{"location":"reference/nostalgia/sources/whereami/#view","text":"def view ( self , index ) View Source def view(self, index): view(self.path[index])","title":"view"},{"location":"reference/nostalgia/sources/whereami/#when","text":"def when ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when"},{"location":"reference/nostalgia/sources/whereami/#when_at","text":"def when_at ( self , other , ** window_kwargs ) View Source def when_at(self, other, **window_kwargs): if isinstance(other, str): other = get_type_from_registry(\"places\").containing(other) return self.__class__(join_time(other, self, **window_kwargs))","title":"when_at"},{"location":"reference/nostalgia/sources/whereami/whereami_scheduler/","text":"Module nostalgia.sources.whereami.whereami_scheduler Can be scheduled per 2 minutes with: sysdm create \"python -m nostalgia.sources.whereami.scheduler\" --extensions \"\" --timer \"*:0/2\" View Source \"\"\" Can be scheduled per 2 minutes with: sysdm create \"python -m nostalgia.sources.whereami.scheduler\" --extensions \"\" --timer \"*:0/2\" \"\"\" import time import whereami import os if __name__ == \"__main__\" : with open ( os . path . expanduser ( \"~/nostalgia_data/input/whereami/history.tsv\" ), \"a\" ) as f : res = whereami . predict () print ( res ) f . write ( \"{} \\t {} \\n \" . format ( time . time (), res ))","title":"Whereami Scheduler"},{"location":"reference/nostalgia/sources/whereami/whereami_scheduler/#module-nostalgiasourceswhereamiwhereami_scheduler","text":"Can be scheduled per 2 minutes with: sysdm create \"python -m nostalgia.sources.whereami.scheduler\" --extensions \"\" --timer \"*:0/2\" View Source \"\"\" Can be scheduled per 2 minutes with: sysdm create \"python -m nostalgia.sources.whereami.scheduler\" --extensions \"\" --timer \"*:0/2\" \"\"\" import time import whereami import os if __name__ == \"__main__\" : with open ( os . path . expanduser ( \"~/nostalgia_data/input/whereami/history.tsv\" ), \"a\" ) as f : res = whereami . predict () print ( res ) f . write ( \"{} \\t {} \\n \" . format ( time . time (), res ))","title":"Module nostalgia.sources.whereami.whereami_scheduler"}]}